# papers-dl Documentation

**Type:** PYPI
**Generated:** 2025-08-27 17:43:00 UTC

## Table of Contents

1. [Package Information](#package-information)
2. [Installation](#installation)
3. [Dependencies](#dependencies)
4. [API Documentation](#api-documentation)
5. [Usage Examples](#usage-examples)
6. [Complete Source Code](#complete-source-code)

---

## Package Information

- **Name**: papers-dl
- **Version**: 0.0.23
- **Author**: Unknown
- **Author Email**: Ben Muthalaly <benmuthalaly@gmail.com>
- **Home Page**: Not specified

### Project URLs

- **Homepage**: https://github.com/benmuth/papers-dl
- **Issues**: https://github.com/benmuth/papers-dl/issues

## Installation

Install this package using pip:

```bash
pip install papers-dl
```

## Dependencies

This package depends on the following packages:

- aiohttp==3.9.5
- beautifulsoup4==4.12.3
- bs4==0.0.2
- certifi==2024.2.2
- cffi==1.16.0
- charset-normalizer==3.3.2
- cryptography==42.0.5
- easygui==0.98.3
- feedparser==6.0.11
- google==3.0.0
- idna==3.6
- loguru==0.7.2
- pdf2doi==1.5.1
- pdfminer.six==20221105
- pdftitle==0.11
- pycparser==2.21
- PyMuPDF==1.23.26
- PyMuPDFb==1.23.22
- PyPDF2==2.0.0
- pyperclip==1.8.2
- requests==2.31.0
- retrying==1.3.4
- sgmllib3k==1.0.0
- six==1.16.0
- soupsieve==2.5
- urllib3==2.2.1
- w3lib==2.1.2

## Description

### Overview
A command line application for downloading scientific papers

### Overview
`papers-dl` is a command line application for downloading scientific papers.

### Usage
```shell
# parse DOI identifiers from a file:
papers-dl parse -m doi --path pages/my-paper.html

# parse ISBN identifiers from a file, output matches as CSV:
papers-dl parse -m isbn --path pages/my-paper.html -f csv

# fetch paper with given identifier from any known provider:
papers-dl fetch "10.1016/j.cub.2019.11.030"

# fetch paper from any known Sci-Hub URL with verbose logging on, and store in "papers" directory:
papers-dl -v fetch -p "scihub" -o "papers" "10.1107/s0907444905036693"

# fetch paper from specific Sci-Hub URL:
papers-dl fetch -p "sci-hub.ee" "10.1107/s0907444905036693"

# fetch paper from SciDB (Anna's Archive):
papers-dl fetch -p "scidb" "10.1107/s0907444905036693"
```

### About

`papers-dl` attempts to be a comprehensive tool for gathering research papers from popular open libraries. There are other solutions for this (see "Other tools" below), but `papers-dl` is trying to fill its own niche:

- comprehensive: other tools usually work with a single library, while `papers-dl` is trying to support a collection of popular libraries.
- performant: `papers-dl` tries to improve search and retrieval times by making use of concurrency where possible.

That said, `papers-dl` may not be the best choice for your specific use case right now. For example, if you require features supported by a specific library, one of the more mature and specialized tools listed below may be a better option.

`papers-dl` was initially created to serve as an extractor for [ArchiveBox](https://archivebox.io), a powerful solution for self-hosted web archiving.

This project started as a fork of [scihub.py](https://github.com/zaytoun/scihub.py).

### Other tools

- [Scidownl](https://pypi.org/project/scidownl/)
- [arxiv-dl](https://pypi.org/project/arxiv-dl/)
- [Anna's Archive API](https://github.com/dheison0/annas-archive-api)

### Roadmap

`papers-dl`'s CLI is not yet stable.

Short-term roadmap:

**parsing**
- add support for parsing more identifier types like PMID and ISSN

**fetching**
- add support for downloading formats other than PDFs, like HTML or epub

**searching**
- add a CLI command for searching libraries for papers and metadata



## API Documentation


### File: `papers_dl-0.0.23/src/__init__.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/src/fetch/__init__.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/src/fetch/fetch.py`

#### Function: `save(data, path)`

Save a file give data and a path.

#### Function: `generate_name(content)`

#### Function: `rename(out_dir, path, name=None) -> str`

Renames a PDF to either the given name or its appropriate title, if
possible. Adds the PDF extension. Returns the new path if renaming was
successful, or the original path if not.


### File: `papers_dl-0.0.23/src/papers_dl.py`

#### Function: `parse_ids(args) -> str`

#### Function: `main()`


### File: `papers_dl-0.0.23/src/parse/__init__.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/src/parse/parse.py`

#### Function: `valid_isbn(subject)`

#### Function: `find_pdf_url(html_content) -> str | None`

#### Function: `parse_file(path, id_types`

Find all matches for the given id types in a file. If id_types isn't given,
defaults to the types in id_patterns.

#### Function: `format_output(output`

Formats a list of dicts of ids and id types into a string according to the
given format type. 'raw' formats ids by line, ignoring type. 'jsonl' and
'csv' formats ids and types.


### File: `papers_dl-0.0.23/src/providers/__init__.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/src/providers/arxiv.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/src/providers/scidb.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/src/providers/scihub.py`

#### Class: `IdentifierNotFoundError`

##### Method: `classify(identifier) -> IDClass`

Classify the type of identifier:
url-direct - openly accessible paper
url-non-direct - pay-walled paper
pmid - PubMed ID
doi - digital object identifier


### File: `papers_dl-0.0.23/src/version.py`

No API documentation found in this file.

### File: `papers_dl-0.0.23/tests/test_cli.py`

#### Class: `TestCLI`

##### Method: `test_parse_command_doi_csv(self)`

##### Method: `test_parse_command_doi_jsonl(self)`

##### Method: `test_parse_command_isbn_raw(self)`

##### Method: `test_parse_command_cli(self)`


### File: `papers_dl-0.0.23/tests/test_fetch.py`

#### Class: `TestSciHub`


### File: `papers_dl-0.0.23/tests/test_parse.py`

#### Class: `TestParser`

##### Method: `setUpClass(cls)`

##### Method: `test_parse_text_ids(self)`

##### Method: `test_parse_text_pdfs(self)`


## Usage Examples

No example files found.

## Complete Source Code

### Package Structure

```
papers_dl-0.0.23/LICENSE
papers_dl-0.0.23/PKG-INFO
papers_dl-0.0.23/README.md
papers_dl-0.0.23/pyproject.toml
papers_dl-0.0.23/setup.cfg
papers_dl-0.0.23/src/__init__.py
papers_dl-0.0.23/src/fetch/__init__.py
papers_dl-0.0.23/src/fetch/fetch.py
papers_dl-0.0.23/src/papers_dl.egg-info/PKG-INFO
papers_dl-0.0.23/src/papers_dl.egg-info/SOURCES.txt
papers_dl-0.0.23/src/papers_dl.egg-info/dependency_links.txt
papers_dl-0.0.23/src/papers_dl.egg-info/entry_points.txt
papers_dl-0.0.23/src/papers_dl.egg-info/requires.txt
papers_dl-0.0.23/src/papers_dl.egg-info/top_level.txt
papers_dl-0.0.23/src/papers_dl.py
papers_dl-0.0.23/src/parse/__init__.py
papers_dl-0.0.23/src/parse/parse.py
papers_dl-0.0.23/src/providers/__init__.py
papers_dl-0.0.23/src/providers/arxiv.py
papers_dl-0.0.23/src/providers/scidb.py
papers_dl-0.0.23/src/providers/scihub.py
papers_dl-0.0.23/src/version.py
papers_dl-0.0.23/tests/test_cli.py
papers_dl-0.0.23/tests/test_fetch.py
papers_dl-0.0.23/tests/test_parse.py
```

### Complete File Contents

#### File: `papers_dl-0.0.23/LICENSE`

```text
MIT License

Copyright (c) 2024 Ben Muthalaly

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

#### File: `papers_dl-0.0.23/PKG-INFO`

```text
Metadata-Version: 2.1
Name: papers-dl
Version: 0.0.23
Summary: A command line application for downloading scientific papers
Author-email: Ben Muthalaly <benmuthalaly@gmail.com>
Project-URL: Homepage, https://github.com/benmuth/papers-dl
Project-URL: Issues, https://github.com/benmuth/papers-dl/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiohttp==3.9.5
Requires-Dist: beautifulsoup4==4.12.3
Requires-Dist: bs4==0.0.2
Requires-Dist: certifi==2024.2.2
Requires-Dist: cffi==1.16.0
Requires-Dist: charset-normalizer==3.3.2
Requires-Dist: cryptography==42.0.5
Requires-Dist: easygui==0.98.3
Requires-Dist: feedparser==6.0.11
Requires-Dist: google==3.0.0
Requires-Dist: idna==3.6
Requires-Dist: loguru==0.7.2
Requires-Dist: pdf2doi==1.5.1
Requires-Dist: pdfminer.six==20221105
Requires-Dist: pdftitle==0.11
Requires-Dist: pycparser==2.21
Requires-Dist: PyMuPDF==1.23.26
Requires-Dist: PyMuPDFb==1.23.22
Requires-Dist: PyPDF2==2.0.0
Requires-Dist: pyperclip==1.8.2
Requires-Dist: requests==2.31.0
Requires-Dist: retrying==1.3.4
Requires-Dist: sgmllib3k==1.0.0
Requires-Dist: six==1.16.0
Requires-Dist: soupsieve==2.5
Requires-Dist: urllib3==2.2.1
Requires-Dist: w3lib==2.1.2

### Overview
`papers-dl` is a command line application for downloading scientific papers.

### Usage
```shell
# parse DOI identifiers from a file:
papers-dl parse -m doi --path pages/my-paper.html

# parse ISBN identifiers from a file, output matches as CSV:
papers-dl parse -m isbn --path pages/my-paper.html -f csv

# fetch paper with given identifier from any known provider:
papers-dl fetch "10.1016/j.cub.2019.11.030"

# fetch paper from any known Sci-Hub URL with verbose logging on, and store in "papers" directory:
papers-dl -v fetch -p "scihub" -o "papers" "10.1107/s0907444905036693"

# fetch paper from specific Sci-Hub URL:
papers-dl fetch -p "sci-hub.ee" "10.1107/s0907444905036693"

# fetch paper from SciDB (Anna's Archive):
papers-dl fetch -p "scidb" "10.1107/s0907444905036693"
```

### About

`papers-dl` attempts to be a comprehensive tool for gathering research papers from popular open libraries. There are other solutions for this (see "Other tools" below), but `papers-dl` is trying to fill its own niche:

- comprehensive: other tools usually work with a single library, while `papers-dl` is trying to support a collection of popular libraries.
- performant: `papers-dl` tries to improve search and retrieval times by making use of concurrency where possible.

That said, `papers-dl` may not be the best choice for your specific use case right now. For example, if you require features supported by a specific library, one of the more mature and specialized tools listed below may be a better option.

`papers-dl` was initially created to serve as an extractor for [ArchiveBox](https://archivebox.io), a powerful solution for self-hosted web archiving.

This project started as a fork of [scihub.py](https://github.com/zaytoun/scihub.py).

### Other tools

- [Scidownl](https://pypi.org/project/scidownl/)
- [arxiv-dl](https://pypi.org/project/arxiv-dl/)
- [Anna's Archive API](https://github.com/dheison0/annas-archive-api)

### Roadmap

`papers-dl`'s CLI is not yet stable.

Short-term roadmap:

**parsing**
- add support for parsing more identifier types like PMID and ISSN

**fetching**
- add support for downloading formats other than PDFs, like HTML or epub

**searching**
- add a CLI command for searching libraries for papers and metadata


```

#### File: `papers_dl-0.0.23/README.md`

```markdown
### Overview
`papers-dl` is a command line application for downloading scientific papers.

### Usage
```shell
# parse DOI identifiers from a file:
papers-dl parse -m doi --path pages/my-paper.html

# parse ISBN identifiers from a file, output matches as CSV:
papers-dl parse -m isbn --path pages/my-paper.html -f csv

# fetch paper with given identifier from any known provider:
papers-dl fetch "10.1016/j.cub.2019.11.030"

# fetch paper from any known Sci-Hub URL with verbose logging on, and store in "papers" directory:
papers-dl -v fetch -p "scihub" -o "papers" "10.1107/s0907444905036693"

# fetch paper from specific Sci-Hub URL:
papers-dl fetch -p "sci-hub.ee" "10.1107/s0907444905036693"

# fetch paper from SciDB (Anna's Archive):
papers-dl fetch -p "scidb" "10.1107/s0907444905036693"
```

### About

`papers-dl` attempts to be a comprehensive tool for gathering research papers from popular open libraries. There are other solutions for this (see "Other tools" below), but `papers-dl` is trying to fill its own niche:

- comprehensive: other tools usually work with a single library, while `papers-dl` is trying to support a collection of popular libraries.
- performant: `papers-dl` tries to improve search and retrieval times by making use of concurrency where possible.

That said, `papers-dl` may not be the best choice for your specific use case right now. For example, if you require features supported by a specific library, one of the more mature and specialized tools listed below may be a better option.

`papers-dl` was initially created to serve as an extractor for [ArchiveBox](https://archivebox.io), a powerful solution for self-hosted web archiving.

This project started as a fork of [scihub.py](https://github.com/zaytoun/scihub.py).

### Other tools

- [Scidownl](https://pypi.org/project/scidownl/)
- [arxiv-dl](https://pypi.org/project/arxiv-dl/)
- [Anna's Archive API](https://github.com/dheison0/annas-archive-api)

### Roadmap

`papers-dl`'s CLI is not yet stable.

Short-term roadmap:

**parsing**
- add support for parsing more identifier types like PMID and ISSN

**fetching**
- add support for downloading formats other than PDFs, like HTML or epub

**searching**
- add a CLI command for searching libraries for papers and metadata


```

#### File: `papers_dl-0.0.23/pyproject.toml`

```toml
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "papers-dl"
authors = [
  { name="Ben Muthalaly", email="benmuthalaly@gmail.com" },
]
description = "A command line application for downloading scientific papers"
readme = "README.md"
requires-python = ">=3.8"
dynamic=["version"]
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = [
  "aiohttp==3.9.5",
  "beautifulsoup4==4.12.3",
  "bs4==0.0.2",
  "certifi==2024.2.2",
  "cffi==1.16.0",
  "charset-normalizer==3.3.2",
  "cryptography==42.0.5",
  "easygui==0.98.3",
  "feedparser==6.0.11",
  "google==3.0.0",
  "idna==3.6",
  "loguru==0.7.2",
  "pdf2doi==1.5.1",
  "pdfminer.six==20221105",
  "pdftitle==0.11",
  "pycparser==2.21",
  "PyMuPDF==1.23.26",
  "PyMuPDFb==1.23.22",
  "PyPDF2==2.0.0",
  "pyperclip==1.8.2",
  "requests==2.31.0",
  "retrying==1.3.4",
  "sgmllib3k==1.0.0",
  "six==1.16.0",
  "soupsieve==2.5",
  "urllib3==2.2.1",
  "w3lib==2.1.2",
]

[project.scripts]
papers-dl = "papers_dl:main"

[project.urls]
Homepage = "https://github.com/benmuth/papers-dl"
Issues = "https://github.com/benmuth/papers-dl/issues"

[tool.setuptools.dynamic]
version = {attr = "version.__version__"}

```

#### File: `papers_dl-0.0.23/setup.cfg`

```ini
[egg_info]
tag_build = 
tag_date = 0


```

#### File: `papers_dl-0.0.23/src/__init__.py`

```python


```

#### File: `papers_dl-0.0.23/src/fetch/__init__.py`

```python


```

#### File: `papers_dl-0.0.23/src/fetch/fetch.py`

```python
import asyncio
import hashlib
import json
import os
from typing import Iterable

import aiohttp
import pdf2doi
import providers.scidb as scidb
import providers.scihub as scihub
import providers.arxiv as arxiv
from loguru import logger

all_providers = ["scihub", "scidb", "arxiv"]


def match_available_providers(
    providers, available_providers: Iterable[str] | None = None
) -> list[str]:
    "Find the providers that are included in available_providers"
    if not available_providers:
        available_providers = all_providers
    matching_providers = []
    for provider in providers:
        for available_provider in available_providers:
            # a user-supplied provider might be a substring of a supported
            # provider (e.g. sci-hub.ee instead of https://sci-hub.ee)
            if provider in available_provider:
                matching_providers.append(available_provider)
    return matching_providers


async def get_urls(session, identifier, providers):
    urls = []
    if providers == "all":
        urls.append(await scidb.get_url(session, identifier))
        urls.extend(await scihub.get_direct_urls(session, identifier))
        urls.append(await arxiv.get_url(identifier))
        return urls

    providers = [provider.strip() for provider in providers.split(",")]
    logger.info(f"given providers: {providers}")

    matching_providers = match_available_providers(providers)
    logger.info(f"matching providers: {matching_providers}")
    for mp in matching_providers:
        if mp == "scihub":
            urls.extend(await scihub.get_direct_urls(session, identifier))
        if mp == "scidb":
            urls.append(await scidb.get_url(session, identifier))
        if mp == "arxiv":
            urls.append(await arxiv.get_url(identifier))

    # if the catch-all "scihub" provider isn't given, we look for
    # specific Sci-Hub urls. if we find specific Sci-Hub URLs in the
    # user input, we only use those
    if "scihub" not in providers:
        matching_scihub_urls = match_available_providers(
            providers, await scihub.get_available_scihub_urls()
        )
        logger.info(f"matching scihub urls: {matching_scihub_urls}")
        if len(matching_scihub_urls) > 0:
            urls.extend(
                await scihub.get_direct_urls(
                    session, identifier, base_urls=matching_scihub_urls
                )
            )

    return urls


async def fetch(session, identifier, providers) -> tuple | None:
    # catch exceptions so that they don't cancel the task group
    async def get_wrapper(url):
        try:
            return await session.get(url)
        except Exception as e:
            logger.error("error: {}", e)
            return None

    urls = await get_urls(session, identifier, providers)

    urls = [url for url in urls if url is not None]

    if len(urls) > 0:
        logger.info("PDF urls: {}", "\n".join(urls))
    tasks = [get_wrapper(url) for url in urls if url]
    for item in zip(asyncio.as_completed(tasks), urls):
        res = await item[0]
        if res is None or res.content_type != "application/pdf":
            logger.info("couldn't find url at {}", item[1])
            continue
        return (await res.read(), item[1])
    return None


def save(data, path):
    """
    Save a file give data and a path.
    """
    try:
        logger.info(f"Saving file to {path}")

        with open(path, "wb") as f:
            f.write(data)
    except Exception as e:
        logger.error(f"Failed to write to {path} {e}")
        raise e


def generate_name(content):
    "Generate unique filename for paper"

    pdf_hash = hashlib.md5(content).hexdigest()
    return f"{pdf_hash}" + ".pdf"


def rename(out_dir, path, name=None) -> str:
    """
    Renames a PDF to either the given name or its appropriate title, if
    possible. Adds the PDF extension. Returns the new path if renaming was
    successful, or the original path if not.
    """

    logger.info("Finding paper title")
    pdf2doi.config.set("verbose", False)

    try:
        if name is None:
            result_info = pdf2doi.pdf2doi(path)
            if not result_info:
                return path
            raw_validation_info = result_info["validation_info"]
            if isinstance(raw_validation_info, (str, bytes, bytearray)):
                validation_info = json.loads(raw_validation_info)
            else:
                validation_info = raw_validation_info
            name = validation_info.get("title")

        if name:
            name += ".pdf"
            new_path = os.path.join(out_dir, name)
            os.rename(path, new_path)
            logger.info(f"File renamed to {new_path}")
            return new_path
        else:
            return path
    except Exception as e:
        logger.error(f"Couldn't get paper title from PDF at {path}: {e}")
        return path

```

#### File: `papers_dl-0.0.23/src/papers_dl.egg-info/PKG-INFO`

```text
Metadata-Version: 2.1
Name: papers-dl
Version: 0.0.23
Summary: A command line application for downloading scientific papers
Author-email: Ben Muthalaly <benmuthalaly@gmail.com>
Project-URL: Homepage, https://github.com/benmuth/papers-dl
Project-URL: Issues, https://github.com/benmuth/papers-dl/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiohttp==3.9.5
Requires-Dist: beautifulsoup4==4.12.3
Requires-Dist: bs4==0.0.2
Requires-Dist: certifi==2024.2.2
Requires-Dist: cffi==1.16.0
Requires-Dist: charset-normalizer==3.3.2
Requires-Dist: cryptography==42.0.5
Requires-Dist: easygui==0.98.3
Requires-Dist: feedparser==6.0.11
Requires-Dist: google==3.0.0
Requires-Dist: idna==3.6
Requires-Dist: loguru==0.7.2
Requires-Dist: pdf2doi==1.5.1
Requires-Dist: pdfminer.six==20221105
Requires-Dist: pdftitle==0.11
Requires-Dist: pycparser==2.21
Requires-Dist: PyMuPDF==1.23.26
Requires-Dist: PyMuPDFb==1.23.22
Requires-Dist: PyPDF2==2.0.0
Requires-Dist: pyperclip==1.8.2
Requires-Dist: requests==2.31.0
Requires-Dist: retrying==1.3.4
Requires-Dist: sgmllib3k==1.0.0
Requires-Dist: six==1.16.0
Requires-Dist: soupsieve==2.5
Requires-Dist: urllib3==2.2.1
Requires-Dist: w3lib==2.1.2

### Overview
`papers-dl` is a command line application for downloading scientific papers.

### Usage
```shell
# parse DOI identifiers from a file:
papers-dl parse -m doi --path pages/my-paper.html

# parse ISBN identifiers from a file, output matches as CSV:
papers-dl parse -m isbn --path pages/my-paper.html -f csv

# fetch paper with given identifier from any known provider:
papers-dl fetch "10.1016/j.cub.2019.11.030"

# fetch paper from any known Sci-Hub URL with verbose logging on, and store in "papers" directory:
papers-dl -v fetch -p "scihub" -o "papers" "10.1107/s0907444905036693"

# fetch paper from specific Sci-Hub URL:
papers-dl fetch -p "sci-hub.ee" "10.1107/s0907444905036693"

# fetch paper from SciDB (Anna's Archive):
papers-dl fetch -p "scidb" "10.1107/s0907444905036693"
```

### About

`papers-dl` attempts to be a comprehensive tool for gathering research papers from popular open libraries. There are other solutions for this (see "Other tools" below), but `papers-dl` is trying to fill its own niche:

- comprehensive: other tools usually work with a single library, while `papers-dl` is trying to support a collection of popular libraries.
- performant: `papers-dl` tries to improve search and retrieval times by making use of concurrency where possible.

That said, `papers-dl` may not be the best choice for your specific use case right now. For example, if you require features supported by a specific library, one of the more mature and specialized tools listed below may be a better option.

`papers-dl` was initially created to serve as an extractor for [ArchiveBox](https://archivebox.io), a powerful solution for self-hosted web archiving.

This project started as a fork of [scihub.py](https://github.com/zaytoun/scihub.py).

### Other tools

- [Scidownl](https://pypi.org/project/scidownl/)
- [arxiv-dl](https://pypi.org/project/arxiv-dl/)
- [Anna's Archive API](https://github.com/dheison0/annas-archive-api)

### Roadmap

`papers-dl`'s CLI is not yet stable.

Short-term roadmap:

**parsing**
- add support for parsing more identifier types like PMID and ISSN

**fetching**
- add support for downloading formats other than PDFs, like HTML or epub

**searching**
- add a CLI command for searching libraries for papers and metadata


```

#### File: `papers_dl-0.0.23/src/papers_dl.egg-info/SOURCES.txt`

```text
LICENSE
README.md
pyproject.toml
src/__init__.py
src/papers_dl.py
src/version.py
src/fetch/__init__.py
src/fetch/fetch.py
src/papers_dl.egg-info/PKG-INFO
src/papers_dl.egg-info/SOURCES.txt
src/papers_dl.egg-info/dependency_links.txt
src/papers_dl.egg-info/entry_points.txt
src/papers_dl.egg-info/requires.txt
src/papers_dl.egg-info/top_level.txt
src/parse/__init__.py
src/parse/parse.py
src/providers/__init__.py
src/providers/arxiv.py
src/providers/scidb.py
src/providers/scihub.py
tests/test_cli.py
tests/test_fetch.py
tests/test_parse.py
```

#### File: `papers_dl-0.0.23/src/papers_dl.egg-info/dependency_links.txt`

```text


```

#### File: `papers_dl-0.0.23/src/papers_dl.egg-info/entry_points.txt`

```text
[console_scripts]
papers-dl = papers_dl:main

```

#### File: `papers_dl-0.0.23/src/papers_dl.egg-info/requires.txt`

```text
aiohttp==3.9.5
beautifulsoup4==4.12.3
bs4==0.0.2
certifi==2024.2.2
cffi==1.16.0
charset-normalizer==3.3.2
cryptography==42.0.5
easygui==0.98.3
feedparser==6.0.11
google==3.0.0
idna==3.6
loguru==0.7.2
pdf2doi==1.5.1
pdfminer.six==20221105
pdftitle==0.11
pycparser==2.21
PyMuPDF==1.23.26
PyMuPDFb==1.23.22
PyPDF2==2.0.0
pyperclip==1.8.2
requests==2.31.0
retrying==1.3.4
sgmllib3k==1.0.0
six==1.16.0
soupsieve==2.5
urllib3==2.2.1
w3lib==2.1.2

```

#### File: `papers_dl-0.0.23/src/papers_dl.egg-info/top_level.txt`

```text
__init__
fetch
papers_dl
parse
providers
search
version

```

#### File: `papers_dl-0.0.23/src/papers_dl.py`

```python
import argparse
import asyncio
import os
import sys

import aiohttp
from loguru import logger
from fetch import fetch
from parse.parse import format_output, id_patterns, parse_file, parse_ids_from_text


async def fetch_paper(args) -> str:
    providers = args.providers
    id = args.query
    out = args.output

    headers = None
    if args.user_agent is not None:
        headers = {
            "User-Agent": args.user_agent,
        }

    async with aiohttp.ClientSession(headers=headers) as sess:
        result = await fetch.fetch(sess, id, providers)

    if result is None:
        return None

    pdf_content, url = result

    path = os.path.join(out, fetch.generate_name(pdf_content))
    fetch.save(pdf_content, path)
    new_path = fetch.rename(out, path)
    return f"Successfully downloaded paper from {url}.\n Saved to {new_path}"


def parse_ids(args) -> str:
    output = None
    if hasattr(args, "path") and args.path:
        output = parse_file(args.path, args.match)
    else:
        # if a path isn't passed or is empty, read from stdin
        output = parse_ids_from_text(sys.stdin.read(), args.match)
    return format_output(output, args.format)


async def run():
    name = "papers-dl"
    parser = argparse.ArgumentParser(
        prog=name,
        description="Download scientific papers from the command line",
    )

    from version import __version__

    parser.add_argument(
        "--version", "-V", action="version", version=f"{name} {__version__}"
    )

    parser.add_argument(
        "--verbose", "-v", action="store_true", help="increase verbosity"
    )

    subparsers = parser.add_subparsers()

    # FETCH
    parser_fetch = subparsers.add_parser(
        "fetch", help="try to download a paper with the given identifier"
    )

    parser_fetch.add_argument(
        "query",
        metavar="(DOI|PMID|URL)",
        type=str,
        help="the identifier to try to download",
    )

    parser_fetch.add_argument(
        "-o",
        "--output",
        metavar="path",
        help="optional output directory for downloaded papers",
        default=".",
        type=str,
    )

    parser_fetch.add_argument(
        "-p",
        "--providers",
        help="comma separated list of providers to try fetching from",
        default="all",
        type=str,
    )

    parser_fetch.add_argument(
        "-A",
        "--user-agent",
        help="",
        default=None,
        type=str,
    )

    # PARSE
    parser_parse = subparsers.add_parser(
        "parse", help="parse identifiers from a file or stdin"
    )
    parser_parse.add_argument(
        "-m",
        "--match",
        metavar="type",
        help="the type of identifier to search for",
        type=str,
        choices=id_patterns.keys(),
        action="append",
    )
    parser_parse.add_argument(
        "-p",
        "--path",
        help="the path of the file to parse",
        type=str,
    )
    parser_parse.add_argument(
        "-f",
        "--format",
        help="the output format for printing",
        metavar="fmt",
        default="raw",
        choices=["raw", "jsonl", "csv"],
        nargs="?",
    )

    parser_fetch.set_defaults(func=fetch_paper)
    parser_parse.set_defaults(func=parse_ids)

    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="INFO", enqueue=True, format="{message}")
    else:
        logger.add(sys.stderr, level="ERROR", enqueue=True, format="{message}")

    if hasattr(args, "func"):
        if asyncio.iscoroutinefunction(args.func):
            result = await args.func(args)
        else:
            result = args.func(args)

        if result:
            print(result)
        else:
            # TODO: change this to be more general
            print("No papers found")
    else:
        parser.print_help()


def main():
    asyncio.run(run())


if __name__ == "__main__":
    asyncio.run(run())

```

#### File: `papers_dl-0.0.23/src/parse/__init__.py`

```python


```

#### File: `papers_dl-0.0.23/src/parse/parse.py`

```python
import json
import re

from bs4 import BeautifulSoup
from loguru import logger


# from https://isbn-checker.netlify.app
def valid_isbn(subject):
    "Check if the subject is a valid ISBN"

    isbn_regex = re.compile(
        r"^(?:ISBN(?:-1[03])?:? )?(?=[0-9X]{10}$|(?=(?:[0-9]+[- ]){3})[- 0-9X]{13}$|97[89][0-9]{10}$|(?=(?:[0-9]+[- ]){4})[- 0-9]{17}$)(?:97[89][- ]?)?[0-9]{1,5}[- ]?[0-9]+[- ]?[0-9]+[- ]?[0-9X]$"
    )

    # Check if the subject matches the ISBN pattern
    if isbn_regex.match(subject):
        chars = re.sub(r"[- ]|^ISBN(?:-1[03])?:?", "", subject)
        chars = list(chars)
        last = chars.pop()
        sum = 0
        check = 0

        if len(chars) == 9:
            chars.reverse()
            for i in range(len(chars)):
                sum += (i + 2) * int(chars[i])
            check = 11 - (sum % 11)
            if check == 10:
                check = "X"
            elif check == 11:
                check = "0"
        else:
            for i in range(len(chars)):
                sum += (i % 2 * 2 + 1) * int(chars[i])
            check = 10 - (sum % 10)
            if check == 10:
                check = "0"

        if str(check) == last:
            return True
        else:
            return False
    else:
        return False


# these are the currently supported identifier types that we can parse, along
# with their regex patterns
id_patterns = {
    # These come from https://gist.github.com/oscarmorrison/3744fa216dcfdb3d0bcb
    "isbn": [
        r"(?:ISBN(?:-10)?:?\ )?(?=[0-9X]{10}|(?=(?:[0-9]+[-\ ]){3})[-\ 0-9X]{13})[0-9]{1,5}[-\ ]?[0-9]+[-\ ]?[0-9]+[-\ ]?[0-9X]",
        r"(?:ISBN(?:-13)?:?\ )?(?=[0-9]{13}|(?=(?:[0-9]+[-\ ]){4})[-\ 0-9]{17})97[89][-\ ]?[0-9]{1,5}[-\ ]?[0-9]+[-\ ]?[0-9]+[-\ ]?[0-9]",
    ],
    # doi regexes taken from https://www.crossref.org/blog/dois-and-matching-regular-expressions/
    # listed in decreasing order of goodness. Not fully tested yet.
    "doi": [
        r"10.\d{4,9}\/[-._;()\/:A-Z0-9]+",
        r"10.1002\/[^\s]+",
        r"10.\d{4}\/\d+-\d+X?(\d+)\d+<[\d\w]+:[\d\w]*>\d+.\d+.\w+;\d",
        r"10.1021\/\w\w\d++",
        r"10.1207/[\w\d]+\&\d+_\d+",
    ],
    # arXiv ids: https://info.arxiv.org/help/arxiv_identifier.html
    "arxiv": [
        # identifiers since March 2007
        r"arXiv:\d{4}\.\d{4,5}(v\d+)?",
        # identifiers before March 2007
        r"arXiv:[A-Za-z-]{3,10}(\.[A-Z]{2})?\/\d{4,8}",
    ],
}

# these can eliminate false positives
# TODO: remove duplication of validation logic and parsing logic
id_validators = {
    "isbn": valid_isbn,
}


def find_pdf_url(html_content) -> str | None:
    "Given HTML content, find an embedded link to a PDF."

    s = BeautifulSoup(html_content, "html.parser")

    # look for a dynamically loaded PDF
    script_element = s.find("script", string=re.compile("PDFObject.embed"))

    if script_element:
        match = re.search(r'PDFObject\.embed\("([^"]+)"', script_element.string)
        if match:
            return match.group(1)

    # look for the "<embed>" element (scihub)
    embed_element = s.find("embed", {"id": "pdf", "type": "application/pdf"})

    if embed_element:
        direct_url = embed_element["src"]
        if isinstance(direct_url, list):
            direct_url = direct_url[0]
        if direct_url:
            return direct_url

    # look for an iframe
    iframe = s.find("iframe", {"type": "application/pdf"})

    if iframe:
        logger.info(f"found iframe: {iframe}")
        direct_url = iframe.get("src")
        if isinstance(direct_url, list):
            direct_url = direct_url[0]
        if direct_url:
            return direct_url

    return None


def parse_ids_from_text(
    s: str, id_types: list[str] | None = None
) -> list[dict[str, str]]:
    """
    Find all matches for the given id types in a string. If id_types isn't
    given, it will parse all the types in id_patterns by default.
    """

    # we look for all ID patterns by default
    if id_types is None:
        id_types = list(id_patterns)

    seen = set()
    matches = []
    for id_type in id_types:
        validator = id_validators.get(id_type)
        for regex in id_patterns[id_type]:
            for match in re.finditer(regex, s, re.IGNORECASE):
                mg = match.group()
                valid_id = validator(mg) if validator else True
                if mg not in seen and valid_id:
                    matches.append({"id": mg, "type": id_type})
                seen.add(mg)
    return matches


def parse_file(path, id_types: list[str] | None = None):
    """
    Find all matches for the given id types in a file. If id_types isn't given,
    defaults to the types in id_patterns.
    """

    matches = []
    try:
        with open(path) as f:
            content = f.read()
        matches = parse_ids_from_text(content, id_types)
    except Exception as e:
        print(f"Error: {e}")

    return matches


def format_output(output: list[dict[str, str]], format: str = "raw") -> str:
    """
    Formats a list of dicts of ids and id types into a string according to the
    given format type. 'raw' formats ids by line, ignoring type. 'jsonl' and
    'csv' formats ids and types.
    """

    lines: list[str] = []
    if format == "raw":
        lines = [line["id"] for line in output]
    elif format == "jsonl":
        lines = [json.dumps(line) for line in output]
    elif format == "csv":
        lines = [f"{line['id']},{line['type']}" for line in output]
    else:
        raise Exception(f"invalid format {format}")
    return "\n".join(lines)

```

#### File: `papers_dl-0.0.23/src/providers/__init__.py`

```python
import sys
import os

sys.path.insert(
    0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "fetch"))
)

```

#### File: `papers_dl-0.0.23/src/providers/arxiv.py`

```python
# from urllib.parse import urljoin

# from loguru import logger
from parse.parse import parse_ids_from_text


async def get_url(identifier):
    is_arxiv = parse_ids_from_text(identifier, ["arxiv"])
    if is_arxiv:
        pdf_url = f"https://arxiv.org/pdf/{identifier}.pdf"
        return pdf_url

    return None

```

#### File: `papers_dl-0.0.23/src/providers/scidb.py`

```python
from urllib.parse import urljoin

from loguru import logger
from parse.parse import find_pdf_url, parse_ids_from_text


async def get_url(session, identifier):
    base_url = "https://annas-archive.org/scidb/"
    # TODO: add support for .se and .li base_urls

    is_doi = parse_ids_from_text(identifier, ["doi"])
    if is_doi:
        url = urljoin(base_url, identifier)
        logger.info("searching SciDB: {}", url)
        try:
            res = await session.get(url)
        except Exception as e:
            logger.error("Couldn't connect to SciDB: {}", e)
            return None
        pdf_url = find_pdf_url(await res.read())
        if pdf_url is None:
            logger.info("No direct link to PDF found from SciDB")
        return pdf_url

    return None

```

#### File: `papers_dl-0.0.23/src/providers/scihub.py`

```python
import asyncio
import enum
import re
from urllib.parse import urljoin

import aiohttp
from bs4 import BeautifulSoup
from loguru import logger
from parse.parse import find_pdf_url

# URL-DIRECT - openly accessible paper
# URL-NON-DIRECT - pay-walled paper
# PMID - PubMed ID
# DOI - digital object identifier
IDClass = enum.Enum("identifier", ["URL-DIRECT", "URL-NON-DIRECT", "PMD", "DOI"])

DEFAULT_USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15"


class IdentifierNotFoundError(Exception):
    pass


async def get_available_scihub_urls() -> list[str]:
    """
    Finds available Sci-Hub urls via https://sci-hub.now.sh/
    """

    # NOTE: This misses some valid URLs. Alternatively, we could parse
    # the HTML more finely by navigating the parsed DOM, instead of relying
    # on filtering. That might be more brittle in case the HTML changes.
    # Generally, we don't need to get all URLs.
    scihub_domain = re.compile(r"^http[s]*://sci.hub", flags=re.IGNORECASE)
    urls = []

    try:
        async with aiohttp.request("GET", "https://sci-hub.now.sh/") as res:
            s = BeautifulSoup(await res.text(), "html.parser")
    except Exception as e:
        logger.info("Couldn't find Sci-Hub URLs: {}", e)
        return []

    text_matches = s.find_all(
        "a",
        href=True,
        string=re.compile(scihub_domain),
    )

    href_matches = s.find_all(
        "a",
        re.compile(scihub_domain),
        href=True,
    )

    full_match_set = set(text_matches) | set(href_matches)
    for a in full_match_set:
        if "sci" in a or "sci" in a["href"]:
            urls.append(a["href"])

    return urls


async def get_direct_urls(
    session,
    identifier: str,
    base_urls: list[str] | None = None,
) -> list[str]:
    """
    Finds the direct source url for a given identifier.
    """

    if base_urls is None:
        base_urls = await get_available_scihub_urls()

    logger.info("searching Sci-Hub urls: {}", base_urls)

    # catch exceptions so that they don't cancel the task group
    async def get_wrapper(url):
        try:
            return await session.get(url)
        except Exception as e:
            logger.info("Couldn't connect to {}: {}", url, e)
            return None

    if classify(identifier) == IDClass["URL-DIRECT"]:
        return [identifier]

    async with asyncio.TaskGroup() as tg:
        tasks = [
            tg.create_task(get_wrapper(urljoin(base_url, identifier)))
            for base_url in base_urls
        ]

    direct_urls = []
    try:
        for task in tasks:
            res = await task
            if res is None:
                continue
            path = find_pdf_url(await res.text())
            if isinstance(path, list):
                path = path[0]
            if isinstance(path, str) and path.startswith("//"):
                direct_urls.append("https:" + path)
            elif isinstance(path, str) and path.startswith("/"):
                direct_urls.append(urljoin(res.url.human_repr(), path))

    except Exception as err:
        logger.error("Error while looking for PDF urls: {}", err)

    if not direct_urls:
        logger.info("No direct link to PDF found from Sci-Hub")

    return list(set(direct_urls))


def classify(identifier) -> IDClass:
    """
    Classify the type of identifier:
    url-direct - openly accessible paper
    url-non-direct - pay-walled paper
    pmid - PubMed ID
    doi - digital object identifier
    """
    if identifier.startswith("http") or identifier.startswith("https"):
        if identifier.endswith("pdf"):
            return IDClass["URL-DIRECT"]
        else:
            return IDClass["URL-NON-DIRECT"]
    elif identifier.isdigit():
        return IDClass["PMID"]
    else:
        return IDClass["DOI"]

```

#### File: `papers_dl-0.0.23/src/version.py`

```python
__version__ = "0.0.23"

```

#### File: `papers_dl-0.0.23/tests/test_cli.py`

```python
import unittest
import subprocess
import sys

test_paper_id = "10.1016/j.cub.2019.11.030"
test_paper_title = "Parrots Voluntarily Help Each Other to Obtain Food Rewards"


class TestCLI(unittest.TestCase):
    def test_parse_command_doi_csv(self):
        result = subprocess.run(
            [
                sys.executable,
                "src/papers_dl.py",
                "parse",
                "-m",
                "doi",
                "-p",
                "tests/documents/bsp-tree.html",
                "-f",
                "csv",
            ],
            capture_output=True,
            text=True,
        )
        self.assertIn("10.1109/83.544569,doi", result.stdout)

    def test_parse_command_doi_jsonl(self):
        result = subprocess.run(
            [
                sys.executable,
                "src/papers_dl.py",
                "parse",
                "-m",
                "doi",
                "-f",
                "jsonl",
                "-p",
                "tests/documents/bsp-tree.html",
            ],
            capture_output=True,
            text=True,
        )
        self.assertIn('{"id": "10.1109/83.544569", "type": "doi"}', result.stdout)

    def test_parse_command_isbn_raw(self):
        result = subprocess.run(
            [
                sys.executable,
                "src/papers_dl.py",
                "parse",
                "-m",
                "isbn",
                "-f",
                "raw",
                "-p",
                "tests/documents/b-tree-techniques.html",
            ],
            capture_output=True,
            text=True,
        )
        self.assertIn("978-1-60198-482-1", result.stdout)
        self.assertIn("978-1-60198-483-8", result.stdout)

    def test_parse_command_cli(self):
        args = [
            sys.executable,
            "src/papers_dl.py",
            "parse",
            "-f",
            "jsonl",
            "-m",
            "isbn",
        ]

        input_data = "978-1-60198-482-1 978-1-60198-483-8"

        result = subprocess.run(args, input=input_data, capture_output=True, text=True)
        self.assertIn('{"id": "978-1-60198-482-1", "type": "isbn"}', result.stdout)
        self.assertIn('{"id": "978-1-60198-483-8", "type": "isbn"}', result.stdout)

```

#### File: `papers_dl-0.0.23/tests/test_fetch.py`

```python
import unittest

import aiohttp
import asyncio

from src.providers.scihub import get_available_scihub_urls


class TestSciHub(unittest.IsolatedAsyncioTestCase):
    async def test_scihub_up(self):
        """
        Test to verify that `scihub.now.sh` is available
        """
        urls = await get_available_scihub_urls()
        self.assertIsNotNone(urls, "Failed to find Sci-Hub domains")

```

#### File: `papers_dl-0.0.23/tests/test_parse.py`

```python
import os
import unittest

from parse import parse

target_ids = ("doi", "pmid", "isbn", "issn", "url", "arxiv")


class TestParser(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.test_material_dir = "tests/documents"
        cls.valid_id_types = parse.id_patterns.keys()
        for id_type in target_ids:
            if id_type not in cls.valid_id_types:
                print(f"Skipping testing for {id_type} parsing")

    def test_parse_text_ids(self):
        "Test to parse identifiers from a set of files."

        # NOTE: this test does not fail on false positive matches
        # for file in test_document_ids:
        for file in test_document_ids:
            print(f"testing {file}")
            with open(os.path.join(TestParser.test_material_dir, file)) as f:
                file_content = f.read()

            parsed_results = parse.parse_ids_from_text(file_content)

            # just include the matching id, not the type
            parsed_results = [result["id"] for result in parsed_results]

            expected_ids = []
            for type in test_document_ids[file]:
                if type in parse.id_patterns:
                    for id in test_document_ids[file][type]:
                        expected_ids.append(id)

            if not expected_ids:
                print("No expected IDs for this file")
                continue

            for expected_id in expected_ids:
                self.assertIn(
                    expected_id,
                    parsed_results,
                    f"ID {expected_id} not found in {file}",
                )

    def test_parse_text_pdfs(self):
        for file, expected_url in test_document_links:
            with open(os.path.join(TestParser.test_material_dir, file), "rt") as f:
                html_content = f.read()
                pdf_url = parse.find_pdf_url(html_content)
            self.assertEqual(pdf_url, expected_url)


test_document_ids = {
    "ids.txt": {
        "url": [
            "https://www.cell.com/current-biology/fulltext/S0960-9822(19)31469-1",
        ],
        "doi": [
            "10.1016/j.cub.2019.11.030",
            "10.1107/s0907444905036693",
        ],
    },
    "bsp-tree.html": {
        "doi": [
            "10.1109/83.544569",
        ],
        "issn": [
            "1057-7149",
            "1941-0042",
        ],
    },
    "reyes-rendering.html": {
        "doi": [
            "10.1145/37402.37414",
        ],
    },
    "superscalar-cisc.html": {
        "doi": [
            "10.1109/HPCA.2006.1598111",
        ],
        "issn": [
            "1530-0897",
            "2378-203X",
        ],
    },
    "b-tree-techniques.html": {
        "doi": [
            "10.1561/1900000028",
        ],
        "url": [
            "http://dx.doi.org/10.1561/1900000028",
        ],
        "isbn": [
            "978-1-60198-482-1",
            "978-1-60198-483-8",
        ],
    },
    "real-time-rendering.html": {
        "url": [
            "https://doi.org/10.1201/9781315365459",
        ],
        "isbn": [
            "9781315365459",
        ],
    },
    "arxiv.html": {
        "url": [
            "https://arxiv.org/abs/1605.04938",
        ],
        "arxiv": [
            # identifiers after March 2007
            "arXiv:2407.13619",
            "arXiv:1608.00878",
            "arXiv:1605.04938",
            # identifiers before March 2007
            "arXiv:q-bio/0512009",
            "arXiv:math/0601009",
            "arXiv:hep-th/0512302",
            "arXiv:cond-mat/0512295",
            "arXiv:quant-ph/0511150",
        ],
    },
}

test_document_links = [
    (
        "scidb.html",
        "https://wbsg8v.xyz/d3/x/1719017408/134/i/scimag/80500000/80542000/10.1016/j.cub.2019.11.030.pdf~/Avtp6y0GwksOGlfLFy9d9Q/Parrots%20Voluntarily%20Help%20Each%20Other%20to%20Obtain%20Food%20Rewards%20--%20Brucks%2C%20D%C3%A9sir%C3%A9e%3B%20von%20Bayern%2C%20Auguste%20M_P_%20--%20Current%20Biology%2C%20%232%2C%2030%2C%20pages%20292-297_e5%2C%20--%2010_1016%2Fj_cub_2019_11_030%20--%20c28dc1242df6f931c29b9cd445a55597%20--%20Anna%E2%80%99s%20Archive.pdf",
    ),
    ("scihub.html", "https://sci.bban.top/pdf/10.1016/j.cub.2019.11.030.pdf"),
]

```


---

*Documentation generated by LlamaPackageService for papers-dl (pypi)*
