# jannisborn/paperscraper Documentation

**Type:** GITHUB
**Generated:** 2025-08-27 17:46:26 UTC

## Table of Contents

1. [Overview](#overview)
2. [Repository Information](#repository-information)
3. [Code Structure](#code-structure)
4. [Key Files](#key-files)
5. [Dependencies](#dependencies)
6. [Usage Examples](#usage-examples)

---

## Repository Information

- **Repository:** jannisborn/paperscraper
- **URL:** https://github.com/jannisborn/paperscraper
- **Processed:** 2025-08-27 17:46:26 UTC
- **Description:** Tools to scrape publications & their metadata from pubmed, arxiv, medrxiv, biorxiv and chemrxiv.

- **Default Branch:** main
- **Stars:** 406
- **Forks:** 47
- **Issues:** 1
- **Last Updated:** 2025-08-26T17:52:37Z
- **Primary Language:** Python
- **License:** MIT License

### Topics

- arxiv
- biorxiv
- chemrxiv
- medrxiv
- paperscraper
- pubmed

## Repository Structure

```
└── .github/
    └── workflows/
        └── codespell.yml
        └── deploy_pypi.yml
        └── docs.yml
        └── test_pypi.yml
        └── test_tip.yml
└── assets/
    └── ai_imaging.png
    └── ai_imaging_covid.png
    └── both.png
    └── molreps.png
└── docs/
    └── api/
        └── paperscraper/
            └── index.md
        └── arxiv.md
        └── citations.md
        └── get_dumps.md
        └── index.md
        └── pdf.md
        └── pubmed.md
        └── scholar.md
        └── xrxiv.md
    └── css/
        └── mkdocstrings-chips.css
        └── nav-code-style.css
    └── README.md
    └── api_reference.md
    └── favicon.png
└── paperscraper/
    └── arxiv/
        └── __init__.py
        └── arxiv.py
        └── utils.py
    └── citations/
        └── entity/
            └── __init__.py
            └── core.py
            └── paper.py
            └── researcher.py
        └── tests/
            └── __init__.py
            └── test_citations.py
            └── test_paper.py
            └── test_self_citations.py
            └── test_self_references.py
        └── __init__.py
        └── citations.py
        └── core.py
        └── orcid.py
        └── self_citations.py
        └── self_references.py
        └── utils.py
    └── get_dumps/
        └── utils/
            └── chemrxiv/
                └── __init__.py
                └── chemrxiv_api.py
                └── utils.py
            └── __init__.py
        └── __init__.py
        └── arxiv.py
        └── biorxiv.py
        └── chemrxiv.py
        └── medrxiv.py
    └── pdf/
        └── __init__.py
        └── fallbacks.py
        └── pdf.py
        └── utils.py
    └── pubmed/
        └── tests/
            └── __init__.py
            └── test_pubmed.py
        └── __init__.py
        └── pubmed.py
        └── utils.py
    └── scholar/
        └── tests/
            └── __init__.py
            └── test_scholar.py
        └── __init__.py
        └── core.py
        └── scholar.py
    └── server_dumps/
        └── __init__.py
    └── tests/
        └── __init__.py
        └── test_dump.jsonl
        └── test_dump.py
        └── test_impactor.py
        └── test_pdf.py
    └── xrxiv/
        └── tests/
            └── __init__.py
            └── test_xrxiv.py
        └── __init__.py
        └── xrxiv_api.py
        └── xrxiv_query.py
    └── __init__.py
    └── async_utils.py
    └── impact.py
    └── load_dumps.py
    └── plotting.py
    └── postprocessing.py
    └── utils.py
└── .codespellrc
└── .coveragerc
└── .gitignore
└── LICENSE
└── README.md
└── codecov.yml
└── dev_requirements.txt
└── mkdocs.yml
└── requirements.txt
└── setup.py
```

## README Content

../README.md

## Dependencies

### Package File: requirements.txt

```
arxiv>=1.4.7
pymed-paperscraper>=1.0.4
pandas>=1.0.4
requests==2.32.0
tqdm>=4.51.0
scholarly>=1.0.0
seaborn>=0.11.0
matplotlib>=3.3.2
matplotlib-venn>=0.11.5
bs4>=0.0.1
impact-factor>=1.1.1
thefuzz>=0.20.0
pytest
tldextract
semanticscholar>=0.8.4
pydantic
unidecode
dotenv
boto3
```

## Code Files

### Language Distribution

- **py**: 60 files (78.9%)
- **md**: 12 files (15.8%)
- **yml**: 2 files (2.6%)
- **css**: 2 files (2.6%)

## Code Size Analysis

| Language | Files | Lines of Code | Size (KB) | % of Codebase |
|----------|-------|---------------|-----------|---------------|
| css | 2 | 37 | 0.91 | 0.5% |
| md | 12 | 506 | 18.80 | 9.7% |
| py | 60 | 4971 | 170.17 | 88.0% |
| yml | 2 | 149 | 3.42 | 1.8% |
| **Total** | **76** | **5663** | **193.30** | **100%** |

### Key Source Files

#### Directory: `paperscraper-main`

##### File: `paperscraper-main/README.md`

```markdown
[![build](https://github.com/jannisborn/paperscraper/actions/workflows/test_tip.yml/badge.svg?branch=main)](https://github.com/jannisborn/paperscraper/actions/workflows/test_tip.yml?query=branch%3Amain)
[![build](https://github.com/jannisborn/paperscraper/actions/workflows/test_pypi.yml/badge.svg?branch=main)](https://github.com/jannisborn/paperscraper/actions/workflows/test_pypi.yml?query=branch%3Amain)
[![build](https://github.com/jannisborn/paperscraper/actions/workflows/docs.yml/badge.svg?branch=main)](https://jannisborn.github.io/paperscraper/)
[![License:
MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/paperscraper.svg)](https://badge.fury.io/py/paperscraper)
[![Downloads](https://static.pepy.tech/badge/paperscraper)](https://pepy.tech/project/paperscraper)
[![codecov](https://codecov.io/github/jannisborn/paperscraper/branch/main/graph/badge.svg?token=Clwi0pu61a)](https://codecov.io/github/jannisborn/paperscraper)
# paperscraper

`paperscraper` is a `python` package for scraping publication metadata or full text files (PDF or XML) from
**PubMed** or preprint servers such as **arXiv**, **medRxiv**, **bioRxiv** and **chemRxiv**.
It provides a streamlined interface to scrape metadata, allows to retrieve citation counts
from Google Scholar, impact factors from journals and comes with simple postprocessing functions
and plotting routines for meta-analysis.


## Table of Contents

1. [Getting Started](#getting-started)
   - [Download X-rxiv Dumps](#download-x-rxiv-dumps)
   - [Arxiv Local Dump](#arxiv-local-dump)
2. [Examples](#examples)
   - [Publication Keyword Search](#publication-keyword-search)
   - [Full-Text Retrieval (PDFs & XMLs)](#full-text-retrieval-pdfs--xmls)
   - [Citation Search](#citation-search)
   - [Journal Impact Factor](#journal-impact-factor)
3. [Plotting](#plotting)
   - [Barplots](#barplots)
   - [Venn Diagrams](#venn-diagrams)
4. [Citation](#citation)
5. [Contributions](#contributions)

## Getting started

```console
pip install paperscraper
```

This is enough to query PubMed, arXiv or Google Scholar.

#### Download X-rxiv Dumps

However, to scrape publication data from the preprint servers [biorxiv](https://www.biorxiv.org), [medrxiv](https://www.medrxiv.org) and [chemrxiv](https://www.chemrxiv.org), the setup is different. The entire dump is downloaded and stored in the `server_dumps` folder in a `.jsonl` format (one paper per line).

```py
from paperscraper.get_dumps import biorxiv, medrxiv, chemrxiv
medrxiv()  #  Takes ~30min and should result in ~35 MB file
biorxiv()  # Takes ~1h and should result in ~350 MB file
chemrxiv()  #  Takes ~45min and should result in ~20 MB file
```
*NOTE*: Once the dumps are stored, please make sure to restart the python interpreter so that the changes take effect. 
*NOTE*: If you experience API connection issues (`ConnectionError`), since v0.2.12 there are automatic retries which you can even control and raise from the default of 10, as in `biorxiv(max_retries=20)`.

Since v0.2.5 `paperscraper` also allows to scrape {med/bio/chem}rxiv for specific dates.
```py
medrxiv(start_date="2023-04-01", end_date="2023-04-08")
```
But watch out. The resulting `.jsonl` file will be labelled according to the current date and all your subsequent searches will be based on this file **only**. If you use this option you might want to keep an eye on the source files (`paperscraper/server_dumps/*jsonl`) to ensure they contain the paper metadata for all papers you're interested in.

#### Arxiv local dump
If you prefer local search rather than using the arxiv API:

```py
from paperscraper.get_dumps import arxiv
arxiv(start_date='2024-01-01', end_date=None) # scrapes all metadata from 2024 until today.
```

Afterwards you can search the local arxiv dump just like the other x-rxiv dumps.
The direct endpoint is `paperscraper.arxiv.get_arxiv_papers_local`. You can also specify the
backend directly in the `get_and_dump_arxiv_papers` function:
```py
from paperscraper.arxiv import get_and_dump_arxiv_papers
get_and_dump_arxiv_papers(..., backend='local')
```

## Examples

`paperscraper` is build on top of the packages [arxiv](https://pypi.org/project/arxiv/), [pymed](https://pypi.org/project/pymed-paperscraper/), and [scholarly](https://pypi.org/project/scholarly/). 

### Publication keyword search

Consider you want to perform a publication keyword search with the query:
`COVID-19` **AND** `Artificial Intelligence` **AND** `Medical Imaging`. 

* Scrape papers from PubMed:

```py
from paperscraper.pubmed import get_and_dump_pubmed_papers
covid19 = ['COVID-19', 'SARS-CoV-2']
ai = ['Artificial intelligence', 'Deep learning', 'Machine learning']
mi = ['Medical imaging']
query = [covid19, ai, mi]

get_and_dump_pubmed_papers(query, output_filepath='covid19_ai_imaging.jsonl')
```

* Scrape papers from arXiv:

```py
from paperscraper.arxiv import get_and_dump_arxiv_papers

get_and_dump_arxiv_papers(query, output_filepath='covid19_ai_imaging.jsonl')
```

* Scrape papers from bioRiv, medRxiv or chemRxiv:

```py
from paperscraper.xrxiv.xrxiv_query import XRXivQuery

querier = XRXivQuery('server_dumps/chemrxiv_2020-11-10.jsonl')
querier.search_keywords(query, output_filepath='covid19_ai_imaging.jsonl')
```

You can also use `dump_queries` to iterate over a bunch of queries for all available databases.

```py
from paperscraper import dump_queries

queries = [[covid19, ai, mi], [covid19, ai], [ai]]
dump_queries(queries, '.')
```

Or use the harmonized interface of `QUERY_FN_DICT` to query multiple databases of your choice:
```py
from paperscraper.load_dumps import QUERY_FN_DICT
print(QUERY_FN_DICT.keys())

QUERY_FN_DICT['biorxiv'](query, output_filepath='biorxiv_covid_ai_imaging.jsonl')
QUERY_FN_DICT['medrxiv'](query, output_filepath='medrxiv_covid_ai_imaging.jsonl')
```

* Scrape papers from Google Scholar:

Thanks to [scholarly](https://pypi.org/project/scholarly/), there is an endpoint for Google Scholar too.
It does not understand Boolean expressions like the others, but should be used just like
the [Google Scholar search fields](https://scholar.google.com).

```py
from paperscraper.scholar import get_and_dump_scholar_papers
topic = 'Machine Learning'
get_and_dump_scholar_papers(topic)
```
*NOTE*: The scholar endpoint does not require authentication but since it regularly prompts with captchas, it's difficult to apply large scale.

### Full-Text Retrieval (PDFs & XMLs)

`paperscraper` allows you to download full text of publications using DOIs. The basic functionality works reliably for preprint servers (arXiv, bioRxiv, medRxiv, chemRxiv), but retrieving papers from PubMed dumps is more challenging due to publisher restrictions and paywalls.

#### Standard Usage

The main download functions work for all paper types with automatic fallbacks:

```py
from paperscraper.pdf import save_pdf
paper_data = {'doi': "10.48550/arXiv.2207.03928"}
save_pdf(paper_data, filepath='gt4sd_paper.pdf')
```

To batch download full texts from your metadata search results:

```py
from paperscraper.pdf import save_pdf_from_dump

# Save PDFs/XMLs in current folder and name the files by their DOI
save_pdf_from_dump('medrxiv_covid_ai_imaging.jsonl', pdf_path='.', key_to_save='doi')
```

#### Automatic Fallback Mechanisms

When the standard text retrieval fails, `paperscraper` automatically tries these fallbacks:

- **BioC-PMC**: For biomedical papers in [PubMed Central](https://pmc.ncbi.nlm.nih.gov/) (open-access repository), it retrieves open-access full-text XML from the [BioC-PMC API](https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/).
- **eLife Papers**: For [eLife](https://elifesciences.org/) journal papers, it fetches XML files from eLife's open [GitHub repository](https://github.com/elifesciences/elife-article-xml).

These fallbacks are tried automatically without requiring any additional configuration.

#### Enhanced Retrieval with Publisher APIs

For more comprehensive access to papers from major publishers, you can provide API keys for:

- **Wiley TDM API**: Enables access to [Wiley](https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining) publications (2,000+ journals).
- **Elsevier TDM API**: Enables access to [Elsevier](https://www.elsevier.com/about/policies-and-standards/text-and-data-mining) publications (The Lancet, Cell, ...).
- **bioRxiv TDM API** Enable access to [bioRxiv](https://www.biorxiv.org/tdm) publications (since May 2025 bioRxiv is protected with Cloudflare)

To use publisher APIs:

1. Create a file with your API keys:
```
WILEY_TDM_API_TOKEN=your_wiley_token_here
ELSEVIER_TDM_API_KEY=your_elsevier_key_here
AWS_ACCESS_KEY_ID=your_aws_access_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
```
NOTE: The AWS keys can be created in your AWS/IAM account. When creating the key, make sure you tick the `AmazonS3ReadOnlyAccess` permission policy. 
NOTE: If you name the file `.env` it will be loaded automatically (if it is in the cwd or anywhere above the tree to home).

2. Pass the file path when calling retrieval functions:

```py
from paperscraper.pdf import save_pdf_from_dump

save_pdf_from_dump(
    'pubmed_query_results.jsonl',
    pdf_path='./papers',
    key_to_save='doi',
    api_keys='path/to/your/api_keys.txt'
)
```

For obtaining API keys:
- Wiley TDM API: Visit [Wiley Text and Data Mining](https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining) (free for academic users with institutional subscription)
- Elsevier TDM API: Visit [Elsevier's Text and Data Mining](https://www.elsevier.com/about/policies-and-standards/text-and-data-mining) (free for academic users with institutional subscription)

*NOTE*: While these fallback mechanisms improve retrieval success rates, they cannot guarantee access to all papers due to various access restrictions.


### Citation search

You can fetch the number of citations of a paper from its title or DOI

```py
from paperscraper.citations import get_citations_from_title, get_citations_by_doi
title = 'Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I.'
print(get_citations_from_title(title))

doi = '10.1021/acs.jcim.3c00132'
get_citations_by_doi(doi)
```

### Journal impact factor

You can also retrieve the impact factor for all journals:
```py
>>>from paperscraper.impact import Impactor
>>>i = Impactor()
>>>i.search("Nat Comms", threshold=85, sort_by='impact') 
[
    {'journal': 'Nature Communications', 'factor': 17.694, 'score': 94}, 
    {'journal': 'Natural Computing', 'factor': 1.504, 'score': 88}
]
```
This performs a fuzzy search with a threshold of 85. `threshold` defaults to 100 in which case an exact search
is performed. You can also search by journal abbreviation, [E-ISSN](https://portal.issn.org) or [NLM ID](https://portal.issn.org).
```py
i.search("Nat Rev Earth Environ") # [{'journal': 'Nature Reviews Earth & Environment', 'factor': 37.214, 'score': 100}]
i.search("101771060") # [{'journal': 'Nature Reviews Earth & Environment', 'factor': 37.214, 'score': 100}]
i.search('2662-138X') # [{'journal': 'Nature Reviews Earth & Environment', 'factor': 37.214, 'score': 100}]

# Filter results by impact factor
i.search("Neural network", threshold=85, min_impact=1.5, max_impact=20)
# [
#   {'journal': 'IEEE Transactions on Neural Networks and Learning Systems', 'factor': 14.255, 'score': 93}, 
#   {'journal': 'NEURAL NETWORKS', 'factor': 9.657, 'score': 91},
#   {'journal': 'WORK-A Journal of Prevention Assessment & Rehabilitation', 'factor': 1.803, 'score': 86}, 
#   {'journal': 'NETWORK-COMPUTATION IN NEURAL SYSTEMS', 'factor': 1.5, 'score': 92}
# ]

# Show all fields
i.search("quantum information", threshold=90, return_all=True)
# [
#   {'factor': 10.758, 'jcr': 'Q1', 'journal_abbr': 'npj Quantum Inf', 'eissn': '2056-6387', 'journal': 'npj Quantum Information', 'nlm_id': '101722857', 'issn': '', 'score': 92},
#   {'factor': 1.577, 'jcr': 'Q3', 'journal_abbr': 'Nation', 'eissn': '0027-8378', 'journal': 'NATION', 'nlm_id': '9877123', 'issn': '0027-8378', 'score': 91}
# ]
```


## Plotting

When multiple query searches are performed, two types of plots can be generated
automatically: Venn diagrams and bar plots.

### Barplots

Compare the temporal evolution of different queries across different servers.

```py
from paperscraper import QUERY_FN_DICT
from paperscraper.postprocessing import aggregate_paper
from paperscraper.utils import get_filename_from_query, load_jsonl

# Define search terms and their synonyms
ml = ['Deep learning', 'Neural Network', 'Machine learning']
mol = ['molecule', 'molecular', 'drug', 'ligand', 'compound']
gnn = ['gcn', 'gnn', 'graph neural', 'graph convolutional', 'molecular graph']
smiles = ['SMILES', 'Simplified molecular']
fp = ['fingerprint', 'molecular fingerprint', 'fingerprints']

# Define queries
queries = [[ml, mol, smiles], [ml, mol, fp], [ml, mol, gnn]]

root = '../keyword_dumps'

data_dict = dict()
for query in queries:
    filename = get_filename_from_query(query)
    data_dict[filename] = dict()
    for db,_ in QUERY_FN_DICT.items():
        # Assuming the keyword search has been performed already
        data = load_jsonl(os.path.join(root, db, filename))

        # Unstructured matches are aggregated into 6 bins, 1 per year
        # from 2015 to 2020. Sanity check is performed by having 
        # `filtering=True`, removing papers that don't contain all of
        # the keywords in query.
        data_dict[filename][db], filtered = aggregate_paper(
            data, 2015, bins_per_year=1, filtering=True,
            filter_keys=query, return_filtered=True
        )

# Plotting is now very simple
from paperscraper.plotting import plot_comparison

data_keys = [
    'deeplearning_molecule_fingerprint.jsonl',
    'deeplearning_molecule_smiles.jsonl', 
    'deeplearning_molecule_gcn.jsonl'
]
plot_comparison(
    data_dict,
    data_keys,
    title_text="'Deep Learning' AND 'Molecule' AND X",
    keyword_text=['Fingerprint', 'SMILES', 'Graph'],
    figpath='mol_representation'
)
```

![molreps](https://github.com/jannisborn/paperscraper/blob/main/assets/molreps.png?raw=true "MolReps")


### Venn Diagrams

```py
from paperscraper.plotting import (
    plot_venn_two, plot_venn_three, plot_multiple_venn
)

sizes_2020 = (30842, 14474, 2292, 35476, 1904, 1408, 376)
sizes_2019 = (55402, 11899, 2563)
labels_2020 = ('Medical\nImaging', 'Artificial\nIntelligence', 'COVID-19')
labels_2019 = ['Medical Imaging', 'Artificial\nIntelligence']

plot_venn_two(sizes_2019, labels_2019, title='2019', figname='ai_imaging')
```

![2019](https://github.com/jannisborn/paperscraper/blob/main/assets/ai_imaging.png?raw=true "2019")


```py
plot_venn_three(
    sizes_2020, labels_2020, title='2020', figname='ai_imaging_covid'
)
```

![2020](https://github.com/jannisborn/paperscraper/blob/main/assets/ai_imaging_covid.png?raw=true "2020")

Or plot both together:

```py
plot_multiple_venn(
    [sizes_2019, sizes_2020], [labels_2019, labels_2020], 
    titles=['2019', '2020'], suptitle='Keyword search comparison', 
    gridspec_kw={'width_ratios': [1, 2]}, figsize=(10, 6),
    figname='both'
)
```

![both](https://github.com/jannisborn/paperscraper/blob/main/assets/both.png?raw=true "Both")



## Citation
If you use `paperscraper`, please cite a paper that motivated our development of this tool.

```bibtex
@article{born2021trends,
  title={Trends in Deep Learning for Property-driven Drug Design},
  author={Born, Jannis and Manica, Matteo},
  journal={Current Medicinal Chemistry},
  volume={28},
  number={38},
  pages={7862--7886},
  year={2021},
  publisher={Bentham Science Publishers}
}
```

## Contributions
Thanks to the following contributors:
- [@mathinic](https://github.com/mathinic): Since `v0.3.0` improved PubMed full text retrieval with additional fallback mechanisms (BioC-PMC, eLife and optional Wiley/Elsevier APIs).

- [@memray](https://github.com/memray): Since `v0.2.12` there are automatic retries when downloading the {med/bio/chem}rxiv dumps.

- [@achouhan93](https://github.com/achouhan93): Since `v0.2.5` {med/bio/chem}rxiv can be scraped for specific dates!

- [@daenuprobst](https://github.com/daenuprobst): Since  `v0.2.4` PDF files can be scraped directly (`paperscraper.pdf.save_pdf`)

- [@oppih](https://github.com/oppih): Since `v0.2.3` chemRxiv API also provides DOI and URL if available

- [@lukasschwab](https://github.com/lukasschwab): Enabled support for `arxiv` >`1.4.2` in paperscraper `v0.1.0`.

- [@juliusbierk](https://github.com/juliusbierk): Bugfixes

```

##### File: `paperscraper-main/codecov.yml`

```yaml
coverage:
  status:
    patch: off
    project:
      default:
        target: 80%
        threshold: 2% # Up to 2% drop/fluctuation is OK

  ignore:
    - "paperscraper/tests/*"
    - "**/test_*.py"
```

##### File: `paperscraper-main/mkdocs.yml`

```yaml
site_name: Paperscraper
site_description: Documentation for the paperscraper python package
# site_url :
# Repository
repo_name: jannisborn/paperscraper
repo_url: https://github.com/jannisborn/paperscraper
edit_uri: edit/main/docs/

# Theme
theme:
  name: material
  font:
    text: IBM Plex Sans
    code: IBM Plex Mono
  logo: favicon.png
  favicon: favicon.png
  features:
    - content.code.annotate
    # - content.tabs.link
    - content.tooltips
    # - header.autohide
    # - navigation.expand
    - navigation.indexes
    # - navigation.instant
    - navigation.sections
    - navigation.tabs
    - navigation.tabs.sticky
    - navigation.top
    - navigation.tracking
    - search.highlight
    - search.share
    - search.suggest
    - toc.follow
    - toc.integrate
  palette:
    - scheme: default
      primary: black
      accent: blue
      toggle:
        icon: material/toggle-switch
        name: Switch to dark mode
    - scheme: slate
      primary: black
      accent: blue
      toggle:
        icon: material/toggle-switch-off-outline
        name: Switch to light mode
  language: en

nav:
  - Overview: README.md
  - API Documentation:
    - Overview: api/index.md
    - paperscraper: 
      - api/paperscraper/index.md
      - paperscraper.arxiv: api/arxiv.md
      - paperscraper.citations: api/citations.md
      - paperscraper.get_dumps: api/get_dumps.md
      - paperscraper.pdf: api/pdf.md
      - paperscraper.pubmed: api/pubmed.md
      - paperscraper.scholar: api/scholar.md
      - paperscraper.xrxiv: api/xrxiv.md

# Copyright
copyright: MIT License

# Customization
extra:
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/jannisborn/paperscraper
  generator: false

# Plugins
plugins:
  - search
  - mkdocstrings:
      default_handler: python
      handlers:
        python:
          options:
            show_submodules: true
            heading_level: 2
            group_by_category: true
            members_order: source
            show_symbol_type_heading: true
            show_symbol_type_toc: true
            render_typehint_in_signature: true
            show_root_heading: true
            show_signature_annotations: true
            show_source: true

markdown_extensions:
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.inlinehilite
  - md_in_html
  - abbr
  - admonition
  - attr_list
  - def_list
  - footnotes
  - meta
  - toc:
      permalink: true
      # toc_depth: 3
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.betterem:
      smart_enable: all
  - pymdownx.caret
  - pymdownx.critic
  - pymdownx.details
  - pymdownx.emoji:
      emoji_index: !!python/name:material.extensions.emoji.twemoji
      emoji_generator: !!python/name:material.extensions.emoji.to_svg
  - pymdownx.keys
  - pymdownx.mark
  - pymdownx.smartsymbols
  - pymdownx.snippets:
      check_paths: true
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.tasklist:
      custom_checkbox: true
  - pymdownx.tilde

extra_css:
  - css/nav-code-style.css
  - css/mkdocstrings-chips.css
extra_javascript:
  - js/termynal.js
  - js/custom.js

```

##### File: `paperscraper-main/setup.py`

```python
"""Install package."""

import io
import os
import re

from setuptools import find_packages, setup

__version__ = re.search(
    r'__version__\s*=\s*[\'"]([^\'"]*)[\'"]',
    io.open("paperscraper/__init__.py", encoding="utf_8_sig").read(),
).group(1)

LONG_DESCRIPTION = ""
if os.path.exists("README.md"):
    with open("README.md") as fp:
        LONG_DESCRIPTION = fp.read()

setup(
    name="paperscraper",
    version=__version__,
    description="paperscraper: Package to scrape papers.",
    long_description=LONG_DESCRIPTION,
    long_description_content_type="text/markdown",
    author="Jannis Born, Matteo Manica",
    author_email=("jannis.born@gmx.de, drugilsberg@gmail.com"),
    url="https://github.com/jannisborn/paperscraper",
    license="MIT",
    install_requires=[
        "arxiv>=1.4.2",
        "pymed-paperscraper>=1.0.4",
        "pandas",
        "requests",
        "tqdm",
        "scholarly>=1.0.0",
        "seaborn",
        "matplotlib",
        "matplotlib_venn",
        "bs4",
        "impact-factor>=1.1.1",
        "thefuzz",
        "pytest",
        "tldextract",
        "semanticscholar",
        "pydantic",
        "unidecode",
        "dotenv",
        "boto3",
    ],
    keywords=[
        "Academics",
        "Science",
        "Publication",
        "Search",
        "PubMed",
        "Arxiv",
        "Medrxiv",
        "Biorxiv",
        "Chemrxiv",
        "Google Scholar",
    ],
    packages=find_packages("."),
    package_data={"paperscraper.server_dumps": ["*"]},
    zip_safe=False,
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Topic :: Software Development :: Libraries :: Python Modules",
    ],
)

```

#### Directory: `paperscraper-main/docs`

##### File: `paperscraper-main/docs/README.md`

```markdown
../README.md
```

##### File: `paperscraper-main/docs/api_reference.md`

```markdown
# API Reference

::: paperscraper
    options:
        show_if_no_docstring: false
        show_submodules: true
        filters:
            - "!^_[^_]"

```

#### Directory: `paperscraper-main/docs/api`

##### File: `paperscraper-main/docs/api/arxiv.md`

```markdown
::: paperscraper.arxiv

```

##### File: `paperscraper-main/docs/api/citations.md`

```markdown
::: paperscraper.citations

```

##### File: `paperscraper-main/docs/api/get_dumps.md`

```markdown
::: paperscraper.get_dumps

```

##### File: `paperscraper-main/docs/api/index.md`

```markdown
# API Reference

This section documents the public API of **paperscraper**.

Below you’ll find links to the documentation for each module:

- [`paperscraper`](paperscraper/index.md) — Main package entry point.
- [`paperscraper.arxiv`](arxiv.md) — ArXiv scraping & keyword search
- [`paperscraper.citations`](citations.md) — Get (self-)citations & (self-)reference of papers and authors
- [`paperscraper.get_dumps`](get_dumps.md) — Utilities to download bioRxiv, medRxiv & chemRxiv metadata
- [`paperscraper.pdf`](pdf.md) — Download publications as pdfs
- [`paperscraper.pubmed`](pubmed.md) — Pubmed keyword search
- [`paperscraper.scholar`](scholar.md) — Google Scholar endpoints
- [`paperscraper.xrxiv`](xrxiv.md) — Shared utilities for {bio,med,chem}Rxiv 


## Citation
If you use `paperscraper`, please cite a paper that motivated our development of this tool.


<normal>
```bibtex
@article{born2021trends,
  title={Trends in Deep Learning for Property-driven Drug Design},
  author={Born, Jannis and Manica, Matteo},
  journal={Current Medicinal Chemistry},
  volume={28},
  number={38},
  pages={7862--7886},
  year={2021},
  publisher={Bentham Science Publishers}
}
```
</normal> 
---

## Top-level API

::: paperscraper
    options:
        show_if_no_docstring: false
        show_submodules: false
        filters:
            - "!^_[^_]"

```

##### File: `paperscraper-main/docs/api/pdf.md`

```markdown
::: paperscraper.pdf

```

##### File: `paperscraper-main/docs/api/pubmed.md`

```markdown
::: paperscraper.pubmed

```

##### File: `paperscraper-main/docs/api/scholar.md`

```markdown
::: paperscraper.scholar

```

##### File: `paperscraper-main/docs/api/xrxiv.md`

```markdown
::: paperscraper.xrxiv

```

#### Directory: `paperscraper-main/docs/api/paperscraper`

##### File: `paperscraper-main/docs/api/paperscraper/index.md`

```markdown
# paperscraper (top-level utilities)

Below are the modules that live directly under `paperscraper/` (not in subpackages):

## async_utils
::: paperscraper.async_utils
    options:
      group_by_category: true
      members_order: source

## impact
::: paperscraper.impact
    options:
      group_by_category: true
      members_order: source

## load_dumps
::: paperscraper.load_dumps
    options:
      group_by_category: true
      members_order: source

## plotting
::: paperscraper.plotting
    options:
      group_by_category: true
      members_order: source

## postprocessing
::: paperscraper.postprocessing
    options:
      group_by_category: true
      members_order: source

## server_dumps
::: paperscraper.server_dumps
    options:
      group_by_category: true
      members_order: source
## utils
::: paperscraper.utils
    options:
      group_by_category: true
      members_order: source

```

#### Directory: `paperscraper-main/docs/css`

##### File: `paperscraper-main/docs/css/mkdocstrings-chips.css`

```
/* Colorize mkdocstrings symbol-type badges */
.md-typeset .doc-symbol {
    border-radius: .25rem;
    font-size: .72em;
    padding: .12em .4em;
}

.md-typeset .doc-symbol--module {
    background: #e8f0ff;
    color: #1a4fff;
}

.md-typeset .doc-symbol--class {
    background: #e6f7ef;
    color: #0a7f4f;
}

.md-typeset .doc-symbol--method,
.md-typeset .doc-symbol--function {
    background: #fff3e0;
    color: #b95a00;
}

.md-typeset .doc-symbol--attribute,
.md-typeset .doc-symbol--variable {
    background: #f3e8ff;
    color: #6b21a8;
}
```

##### File: `paperscraper-main/docs/css/nav-code-style.css`

```
/* Style only the submodule entries under 'paperscraper' */
.md-sidebar--primary nav.md-nav[aria-labelledby="__nav_2_label"] li.md-nav__item ul.md-nav__list li.md-nav__item .md-ellipsis {
    font-family: var(--md-code-font-family);
    font-size: .9em;
    background: var(--md-code-bg-color);
    color: var(--md-code-fg-color);
    padding: 0 .3rem;
    border-radius: .2rem;
}
```

#### Directory: `paperscraper-main/paperscraper`

##### File: `paperscraper-main/paperscraper/__init__.py`

```python
"""Initialize the module."""

__name__ = "paperscraper"
__version__ = "0.3.2"

import logging
import os
import sys
from typing import List, Union

from .load_dumps import QUERY_FN_DICT
from .utils import get_filename_from_query

logging.basicConfig(stream=sys.stdout, level=logging.WARNING)
logger = logging.getLogger(__name__)

# Set urllib logging depth
url_logger = logging.getLogger("urllib3")
url_logger.setLevel(logging.WARNING)
# Set arxiv logging depth
arxiv_logger = logging.getLogger("arxiv")
arxiv_logger.setLevel(logging.WARNING)


def dump_queries(keywords: List[List[Union[str, List[str]]]], dump_root: str) -> None:
    """Performs keyword search on all available servers and dump the results.

    Args:
        keywords (List[List[Union[str, List[str]]]]): List of lists of keywords
            Each second-level list is considered a separate query. Within each
            query, each item (whether str or List[str]) are considered AND
            separated. If an item is again a list, strs are considered synonyms
            (OR separated).
        dump_root (str): Path to root for dumping.
    """

    for idx, keyword in enumerate(keywords):
        for db, f in QUERY_FN_DICT.items():
            logger.info(f" Keyword {idx + 1}/{len(keywords)}, DB: {db}")
            filename = get_filename_from_query(keyword)
            os.makedirs(os.path.join(dump_root, db), exist_ok=True)
            f(keyword, output_filepath=os.path.join(dump_root, db, filename))

```

##### File: `paperscraper-main/paperscraper/async_utils.py`

```python
import asyncio
import logging
import sys
import threading
from functools import wraps
from typing import Any, Awaitable, Callable, TypeVar, Union

import httpx

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)

T = TypeVar("T")
F = TypeVar("F", bound=Callable[..., Awaitable[Any]])


def _start_bg_loop(loop: asyncio.AbstractEventLoop):
    asyncio.set_event_loop(loop)
    loop.run_forever()


# Start one background loop in its own daemon thread
_background_loop = asyncio.new_event_loop()
threading.Thread(target=_start_bg_loop, args=(_background_loop,), daemon=True).start()


def optional_async(
    func: Callable[..., Awaitable[T]],
) -> Callable[..., Union[T, Awaitable[T]]]:
    """
    Allows an async function to be called from sync code (blocks until done)
    or from within an async context (returns a coroutine to await).
    """

    @wraps(func)
    def wrapper(*args, **kwargs) -> Union[T, Awaitable[T]]:
        coro = func(*args, **kwargs)
        try:
            # If we're already in an asyncio loop, hand back the coroutine:
            asyncio.get_running_loop()
            return coro  # caller must await it
        except RuntimeError:
            # Otherwise, schedule on the background loop and block
            future = asyncio.run_coroutine_threadsafe(coro, _background_loop)
            return future.result()

    return wrapper


def retry_with_exponential_backoff(
    *, max_retries: int = 5, base_delay: float = 1.0
) -> Callable[[F], F]:
    """
    Decorator factory that retries an `async def` on HTTP 429, with exponential backoff.

    Args:
        max_retries: how many times to retry before giving up.
        base_delay: initial delay in seconds; next delays will be duplication of previous.

    Usage:

        @retry_with_exponential_backoff(max_retries=3, base_delay=0.5)
        async def fetch_data(...):
            ...

    """

    def decorator(func: F) -> F:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            delay = base_delay
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except httpx.HTTPStatusError as e:
                    # only retry on 429
                    status = e.response.status_code if e.response is not None else None
                    if status != 429 or attempt == max_retries - 1:
                        raise
                # backoff
                await asyncio.sleep(delay)
                delay *= 2
            # in theory we never reach here

        return wrapper

    return decorator

```

##### File: `paperscraper-main/paperscraper/impact.py`

```python
import logging
from typing import Any, Dict, List, Optional

import pandas as pd
from impact_factor.core import Factor
from thefuzz import fuzz

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Disable sqlalchemy logging
logging.getLogger("sqlalchemy").propagate = False
logger.propagate = True


class Impactor:
    def __init__(self):
        """
        Initialize the Impactor class with an instance of the Factor class.
        This allows access to the database of journal impact factors.
        """
        self.fa = Factor()
        self.all_journals = self.fa.search("%")
        self.metadata = pd.DataFrame(self.all_journals, dtype=str)
        logger.info(f"Loaded metadata for {len(self.metadata)} journals")

    def search(
        self,
        query: str,
        threshold: int = 100,
        sort_by: Optional[str] = None,
        min_impact: float = 0.0,
        max_impact: float = float("inf"),
        return_all: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        Search for journals matching the given query with an optional fuzziness
            level and sorting.

        Args:
            query: The journal name or abbreviation to search for.
            threshold: The threshold for fuzzy matching. If set to 100, exact matching
                is performed. If set below 100, fuzzy matching is used. Defaults to 100.
            sort_by: Criterion for sorting results, one of 'impact', 'journal' and 'score'.
            min_impact: Minimum impact factor for journals to be considered, defaults to 0.
            max_impact: Maximum impact factor for journals to be considered, defaults to infinity.
            return_all: If True, returns all columns of the DataFrame for each match.

        Returns:
            List[dict]: A list of dictionaries containing the journal information.

        """
        # Validation of parameters
        if not isinstance(query, str) or not isinstance(threshold, int):
            raise TypeError(
                f"Query must be a str and threshold must be an int, not {type(query)} and {type(threshold)}"
            )
        if threshold < 0 or threshold > 100:
            raise ValueError(
                f"Fuzziness threshold must be between 0 and 100, not {threshold}"
            )

        if str.isdigit(query) and threshold >= 100:
            # When querying with NLM ID, exact matching does not work since impact_factor
            # strips off leading zeros, so we use fuzzy matching instead
            threshold = 99

        # Define a function to calculate fuzziness score
        def calculate_fuzziness_score(row):
            return max(fuzz.partial_ratio(query, str(value)) for value in row.values)

        # Search with or without fuzzy matching
        if threshold >= 100:
            matched_df = self.metadata[
                self.metadata.apply(
                    lambda x: query.lower() in x.astype(str).str.lower().values, axis=1
                )
            ].copy()
            # Exact matches get a default score of 100
            matched_df["score"] = 100
        else:
            matched_df = self.metadata[
                self.metadata.apply(
                    lambda x: calculate_fuzziness_score(x) >= threshold, axis=1
                )
            ].copy()
            matched_df["score"] = matched_df.apply(calculate_fuzziness_score, axis=1)

        # Sorting based on the specified criterion
        if sort_by == "score":
            matched_df = matched_df.sort_values(by="score", ascending=False)
        elif sort_by == "journal":
            matched_df = matched_df.sort_values(by="journal")
        elif sort_by == "impact":
            matched_df = matched_df.sort_values(by="factor", ascending=False)

        matched_df["factor"] = pd.to_numeric(matched_df["factor"])
        matched_df = matched_df[
            (matched_df["factor"] >= min_impact) & (matched_df["factor"] <= max_impact)
        ]

        # Prepare the final result
        results = [
            (
                row.to_dict()
                if return_all
                else {
                    "journal": row["journal"],
                    "factor": row["factor"],
                    "score": row["score"],
                }
            )
            for _, row in matched_df.iterrows()
        ]

        return results

```

##### File: `paperscraper-main/paperscraper/load_dumps.py`

```python
import glob
import logging
import os
import sys

import pkg_resources

from .arxiv import get_and_dump_arxiv_papers
from .pubmed import get_and_dump_pubmed_papers
from .xrxiv.xrxiv_query import XRXivQuery

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

# Set up the query dictionary
QUERY_FN_DICT = {
    "arxiv": get_and_dump_arxiv_papers,
    "pubmed": get_and_dump_pubmed_papers,
}
# For biorxiv, chemrxiv and medrxiv search for local dumps
dump_root = pkg_resources.resource_filename("paperscraper", "server_dumps")

for db in ["biorxiv", "chemrxiv", "medrxiv"]:
    dump_paths = glob.glob(os.path.join(dump_root, db + "*"))
    if not dump_paths:
        logger.warning(f" No dump found for {db}. Skipping entry.")
        continue
    elif len(dump_paths) > 1:
        logger.info(f" Multiple dumps found for {db}, taking most recent one")
    path = sorted(dump_paths, reverse=True)[0]

    # Handly empty dumped files (e.g. when API is down)
    if os.path.getsize(path) == 0:
        logger.warning(f"Empty dump for {db}. Skipping entry.")
        continue
    querier = XRXivQuery(path)
    if not querier.errored:
        QUERY_FN_DICT.update({db: querier.search_keywords})
        logger.info(f"Loaded {db} dump with {len(querier.df)} entries")

if len(QUERY_FN_DICT) == 2:
    logger.warning(
        " No dumps found for either biorxiv, medrxiv and chemrxiv."
        " Consider using paperscraper.get_dumps.* to fetch the dumps."
    )

```

##### File: `paperscraper-main/paperscraper/plotting.py`

```python
import logging
import math
from typing import Iterable, List, Optional

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib_venn import venn2, venn2_circles, venn3, venn3_circles

# Set matplotlib logging depth
mpl_logger = logging.getLogger("matplotlib")
mpl_logger.setLevel(logging.WARNING)


def plot_comparison(
    data_dict: dict,
    keys: List[str],
    x_ticks: List[str] = ["2015", "2016", "2017", "2018", "2019", "2020"],
    show_preprint: bool = False,
    title_text: str = "",
    keyword_text: Optional[List[str]] = None,
    figpath: str = "comparison_plot.pdf",
) -> None:
    """Plot temporal evolution of number of papers per keyword

    Args:
        data_dict: A dictionary with keywords as keys. Each value should be a
            dictionary itself, with keys for the different APIs. For example
            data_dict = {
                'covid_19.jsonl': {
                    'pubmed': [0, 0, 0, 12345],
                    'arxiv': [0, 0, 0, 1234],
                    ...
                }
                'coronavirus.jsonl':
                    'pubmed': [234, 345, 456, 12345],
                    'arxiv': [123, 234, 345, 1234],
                    ...
                }
            }
        keys: List of keys which should be plotted. This has to be a subset of data_dict.keys().
        x_ticks: List of strings to be used for the x-ticks. Should have same length as
            data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'],
            meaning that papers are aggregated per year.
        show_preprint: Whether preprint servers are aggregated or not.
            Defaults to False.
        title_text: Title for the produced figure. Defaults to ''.
        keyword_text: Figure caption per keyword. Defaults to None, i.e. empty strings will be used.
        figpath: Name under which figure is saved. Relative or absolute
            paths can be given. Defaults to 'comparison_plot.pdf'.

    Raises:
        KeyError: If a database is missing in data_dict.
    """

    sns.set_palette(sns.color_palette("colorblind", 10))
    plt.rcParams.update({"hatch.color": "w"})
    plt.rcParams["figure.facecolor"] = "white"
    plt.figure(figsize=(8, 5))

    arxiv, biorxiv, pubmed, medrxiv, chemrxiv, preprint = [], [], [], [], [], []

    for key in keys:
        try:
            arxiv.append(data_dict[key]["arxiv"])
            biorxiv.append(data_dict[key]["biorxiv"])
            medrxiv.append(data_dict[key]["medrxiv"])
            chemrxiv.append(data_dict[key]["chemrxiv"])
            pubmed.append(data_dict[key]["pubmed"])
        except KeyError:
            raise KeyError(
                f"Did not find all DBs for {key}, only found {data_dict[key].keys()}"
            )
        preprint.append(arxiv[-1] + biorxiv[-1] + medrxiv[-1] + chemrxiv[-1])

    ind = np.arange(len(arxiv[0]))  # the x locations for the groups
    width = [0.2] * len(ind)  # the width of the bars: can also be len(x) sequence
    if len(keys) == 2:
        pos = [-0.2, 0.2]
    elif len(keys) == 3:
        pos = [-0.3, 0.0, 0.3]

    plts = []
    legend_plts = []
    patterns = ("|||", "oo", "xx", "..", "**")
    if show_preprint:
        bars = [pubmed, preprint]
        legend_platform = ["PubMed", "Preprint"]
    else:
        bars = [pubmed, arxiv, biorxiv, chemrxiv, medrxiv]
        legend_platform = ["PubMed", "ArXiv", "BiorXiv", "ChemRxiv", "MedRxiv"]
    for idx in range(len(keys)):
        bottom = 0

        for bidx, b in enumerate(bars):
            if idx == 0:
                p = plt.bar(
                    ind + pos[idx],
                    b[idx],
                    width,
                    linewidth=1,
                    edgecolor="k",
                    bottom=bottom,
                )
            else:
                p = plt.bar(
                    ind + pos[idx],
                    b[idx],
                    width,
                    color=next(iter(plts[bidx])).get_facecolor(),
                    linewidth=1,
                    edgecolor="k",
                    bottom=bottom,
                )

            bottom += b[idx]
            plts.append(p)
        legend_plts.append(
            plt.bar(ind + pos[idx], np.zeros((len(ind),)), color="k", bottom=bottom)
        )

    plt.ylabel("Counts", size=15)
    plt.xlabel("Years", size=15)
    plt.title(f"Keywords: {title_text}", size=14)
    # Customize minor tick labels
    plt.xticks(ind, x_ticks, size=10)

    legend = plt.legend(
        legend_platform,
        prop={"size": 12},
        loc="upper left",
        title="Platform:",
        title_fontsize=13,
        ncol=1,
    )

    # Now set the hatches to not destroy legend

    for idx, stackbar in enumerate(plts):
        pidx = int(np.floor(idx / len(bars)))
        for bar in stackbar:
            bar.set_hatch(patterns[pidx])

    for idx, stackbar in enumerate(legend_plts):
        for bar in stackbar:
            bar.set_hatch(patterns[idx])

    if not keyword_text:
        keyword_text = [""] * len(keys)

    plt.legend(
        legend_plts,
        keyword_text,
        loc="upper center",
        prop={"size": 12},
        title="Keywords (X):",
        title_fontsize=13,
    )
    plt.gca().add_artist(legend)

    get_step_size = lambda x: round(x / 10, -math.floor(math.log10(x)) + 1)
    ymax = plt.gca().get_ylim()[1]
    step_size = np.clip(get_step_size(ymax), 5, 1000)
    y_steps = np.arange(0, ymax, step_size)

    for y_step in y_steps:
        plt.hlines(y_step, xmax=10, xmin=-1, color="black", linewidth=0.1)
    plt.xlim([-0.5, len(ind)])
    plt.ylim([0, ymax * 1.02])

    plt.tight_layout()
    plt.savefig(figpath)
    plt.show()


def plot_single(
    data_dict: dict,
    keys: str,
    x_ticks: List[str] = ["2015", "2016", "2017", "2018", "2019", "2020"],
    show_preprint: bool = False,
    title_text: str = "",
    figpath: str = "comparison_plot.pdf",
    logscale: bool = False,
) -> None:
    """Plot temporal evolution of number of papers per keyword

    Args:
        data_dict: A dictionary with keywords as keys. Each value should be a
            dictionary itself, with keys for the different APIs. For example
            data_dict = {
                'covid_19.jsonl': {
                    'pubmed': [0, 0, 0, 12345],
                    'arxiv': [0, 0, 0, 1234],
                    ...
                }
                'coronavirus.jsonl':
                    'pubmed': [234, 345, 456, 12345],
                    'arxiv': [123, 234, 345, 1234],
                    ...
                }
            }
        keys: A key which should be plotted. This has to be a subset of data_dict.keys().
        x_ticks (List[str]): List of strings to be used for the x-ticks. Should have
            same length as data_dict[key][database]. Defaults to ['2015', '2016',
            '2017', '2018', '2019', '2020'], meaning that papers are aggregated per
            year.
        show_preprint: Whether preprint servers are aggregated or not.
            Defaults to False.
        title_text: Title for the produced figure. Defaults to ''.
        figpath (str, optional): Name under which figure is saved. Relative or absolute
            paths can be given. Defaults to 'comparison_plot.pdf'.
        logscale: Whether y-axis is plotted on logscale. Defaults to False.

    Raises:
        KeyError: If a database is missing in data_dict.
    """

    sns.set_palette(sns.color_palette("colorblind", 10))
    plt.rcParams.update({"hatch.color": "w"})
    plt.rcParams["figure.facecolor"] = "white"
    plt.figure(figsize=(8, 5))

    arxiv, biorxiv, pubmed, medrxiv, chemrxiv, preprint = [], [], [], [], [], []

    for key in keys:
        try:
            arxiv.append(data_dict[key]["arxiv"])
            biorxiv.append(data_dict[key]["biorxiv"])
            medrxiv.append(data_dict[key]["medrxiv"])
            chemrxiv.append(data_dict[key]["chemrxiv"])
            pubmed.append(data_dict[key]["pubmed"])
        except KeyError:
            raise KeyError(
                f"Did not find all DBs for {key}, only found {data_dict[key].keys()}"
            )
        preprint.append(arxiv[-1] + biorxiv[-1] + medrxiv[-1] + chemrxiv[-1])

    ind = np.arange(len(arxiv[0]))  # the x locations for the groups
    width = [0.75] * len(ind)  # the width of the bars: can also be len(x) sequence
    fnc = np.log10 if logscale else np.copy

    plts = []
    legend_plts = []
    if show_preprint:
        bars = [pubmed, preprint]
        legend_platform = ["PubMed", "Preprint"]
        if logscale:
            sums = np.array(pubmed) + np.array(preprint)
            logsums = np.log10(sums)
            bars = [pubmed * logsums / sums, preprint * logsums / sums]

    else:
        bars = [pubmed, arxiv, biorxiv, chemrxiv, medrxiv]
        legend_platform = ["PubMed", "ArXiv", "BiorXiv", "ChemRxiv", "MedRxiv"]
        if logscale:
            sums = (
                np.array(pubmed)
                + np.array(arxiv)
                + np.array(biorxiv)
                + np.array(chemrxiv)
                + np.array(medrxiv)
            )
            logsums = np.log10s(sums)
            bars = [
                pubmed * logsums / sums,
                arxiv * logsums / sums,
                biorxiv * logsums / sums,
                chemrxiv * logsums / sums,
                medrxiv * logsums / sums,
            ]
    for idx in range(len(keys)):
        bottom = 0

        for bidx, b in enumerate(bars):
            if idx == 0:
                p = plt.bar(
                    ind,
                    b[idx],
                    width,
                    linewidth=1,
                    edgecolor="k",
                    bottom=bottom,
                )
            else:
                p = plt.bar(
                    ind,
                    b[idx],
                    width,
                    color=next(iter(plts[bidx])).get_facecolor(),
                    linewidth=1,
                    edgecolor="k",
                    bottom=bottom,
                )

            bottom += b[idx]
            plts.append(p)
        legend_plts.append(
            plt.bar(ind, np.zeros((len(ind),)), color="k", bottom=bottom)
        )

    (
        plt.ylabel("Counts", size=17)
        if not logscale
        else plt.ylabel("Counts (log scale)", size=17)
    )
    plt.xlabel("Years", size=17)
    plt.title(title_text, size=17)
    # Customize minor tick labels

    plt.xticks(ind, x_ticks, size=14)
    ymax = plt.gca().get_ylim()[1]
    if logscale:
        yticks = np.arange(1, ymax).astype(int)
        plt.yticks(yticks, np.power(10, yticks))

    plt.tick_params(axis="y", labelsize=17)

    plt.legend(
        legend_platform,
        prop={"size": 14},
        loc="upper left",
        title="Platform:",
        title_fontsize=17,
        ncol=1,
    )

    get_step_size = lambda x: round(x / 10, -math.floor(math.log10(x)) + 1)
    ymax = plt.gca().get_ylim()[1]

    for y_step in plt.yticks()[0]:
        plt.hlines(y_step, xmax=10, xmin=-1, color="black", linewidth=0.1)
    plt.xlim([-0.5, len(ind)])
    plt.ylim([0, ymax * 1.02])

    plt.tight_layout()
    plt.savefig(figpath)
    plt.show()


get_name = lambda n: " vs. ".join(list(map(lambda x: x.split(" ")[0], n)))


def plot_venn_two(
    sizes: List[int],
    labels: List[str],
    figpath: str = "venn_two.pdf",
    title: str = "",
    **kwargs,
) -> None:
    """Plot a single Venn Diagram with two terms.

    Args:
        sizes (List[int]): List of ints of length 3. First two elements correspond to
            the labels, third one to the intersection.
        labels ([type]): List of str of length 2, containing names of circles.
        figpath (str): Name under which figure is saved. Defaults to 'venn_two.pdf', i.e. it is
            inferred from labels.
        title (str): Title of the plot. Defaults to '', i.e. it is inferred from
            labels.
        **kwargs: Additional keyword arguments for venn2.
    """
    assert len(sizes) == 3, "Incorrect type/length of sizes"
    assert len(labels) == 2, "Incorrect type/length of labels"

    title = get_name(labels) if title == "" else title
    figname = title.lower().replace(" vs. ", "_") if figpath == "" else figpath
    venn2(subsets=sizes, set_labels=labels, alpha=0.6, **kwargs)
    venn2_circles(
        subsets=sizes, linestyle="solid", linewidth=0.6, color="grey", **kwargs
    )
    if kwargs.get("ax", False):
        print(kwargs, type(kwargs))
        print(kwargs["ax"])
        kwargs["ax"].set_title(title, fontdict={"fontweight": "bold"}, size=15)
    else:
        plt.title(title, fontdict={"fontweight": "bold"}, size=15)
        plt.savefig(f"{figname}.pdf")


def plot_venn_three(
    sizes: List[int], labels: List[str], figpath: str = "", title: str = "", **kwargs
) -> None:
    """Plot a single Venn Diagram with two terms.

    Args:
        sizes (List[int]): List of ints of length 3. First two elements correspond to
            the labels, third one to the intersection.
        labels (List[str]): List of str of length 2, containing names of circles.
        figpath (str): Name under which figure is saved. Defaults to '', i.e. it is
            inferred from labels.
        title (str): Title of the plot. Defaults to '', i.e. it is inferred from
            labels.
        **kwargs: Additional keyword arguments for venn3.
    """
    assert len(sizes) == 7, "Incorrect type/length of sizes"
    assert len(labels) == 3, "Incorrect type/length of labels"

    title = get_name(labels) if title == "" else title
    figname = title.lower().replace(" vs. ", "_") if figpath == "" else figpath

    venn3(subsets=sizes, set_labels=labels, alpha=0.6, **kwargs)
    venn3_circles(
        subsets=sizes, linestyle="solid", linewidth=0.6, color="grey", **kwargs
    )

    if kwargs.get("ax", False):
        kwargs["ax"].set_title(title, fontdict={"fontweight": "bold"}, size=15)
    else:
        plt.title(title, fontdict={"fontweight": "bold"}, size=15)
        plt.savefig(f"{figname}.pdf")


def plot_multiple_venn(
    sizes: List[List[int]],
    labels: List[List[str]],
    figname: str,
    titles: List[str],
    suptitle: str = "",
    gridspec_kw: dict = {},
    figsize: Iterable = (8, 4.5),
    **kwargs,
) -> None:
    """Plots multiple Venn Diagrams next to each other

    Args:
        sizes (List[List[int]]): List of lists with sizes, one per Venn Diagram.
            Lengths of lists should be either 3 (plot_venn_two) or 7
            (plot_venn_two).
        labels (List[List[str]]): List of Lists of str containing names of circles.
            Lengths of lists should be either 2 or 3.
        figname (str): Name under which figure is saved. Defaults to '', i.e. it is
            inferred from labels.
        titles (List[str]): Titles of subplots. Should have same length like labels
            and sizes.
        suptitle (str): Title of entire plot. Defaults to '', i.e. no title.
        gridspec_kw (dict): Additional keyword args for plt.subplots. Useful to
            adjust width of plots. E.g.
                gridspec_kw={'width_ratios': [1, 2]}
            will make the second Venn Diagram double as wide as first one.
        **kwargs: Additional keyword arguments for venn3.
    """

    assert len(sizes) == len(labels), "Length of labels & sizes dont match."
    assert len(sizes) == len(titles), "Length of titles & sizes dont match."
    assert len(sizes) > 1, "At least 2 items should be provided."
    assert all(list(map(lambda x: len(x) in [2, 3], labels))), "Wrong label sizes."
    assert all(list(map(lambda x: len(x) in [3, 7], sizes))), "Wrong label sizes."

    fig, axes = plt.subplots(1, len(sizes), gridspec_kw=gridspec_kw, figsize=figsize)
    plt.suptitle(suptitle, size=18, fontweight="bold")

    figname = titles[0].lower().replace(" vs. ", "_") if figname == "" else figname

    for idx, (size, label, title) in enumerate(zip(sizes, labels, titles)):
        if len(label) == 2:
            plot_venn_two(size, label, title=title, ax=axes[idx])
        elif len(label) == 3:
            plot_venn_three(size, label, title=title, ax=axes[idx])

    plt.savefig(f"{figname}.pdf")

```

##### File: `paperscraper-main/paperscraper/postprocessing.py`

```python
import logging
import sys
from typing import Dict, List

import numpy as np
import pandas as pd

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)


def aggregate_paper(
    data: List[Dict[str, str]],
    start_year: int = 2016,
    bins_per_year: int = 4,
    filtering: bool = False,
    filter_keys: List = list(),
    unwanted_keys: List = list(),
    return_filtered: bool = False,
    filter_abstract: bool = True,
    last_year: int = 2021,
):
    """Consumes a list of unstructured keyword results from a .jsonl and
    aggregates papers into several bins per year.

    Args:
        data (List[Dict[str,str]]): Content of a .jsonl file, i.e., a list of
            dictionaries, one per paper.
        start_year (int, optional): First year of interest. Defaults to 2016.
        bins_per_year (int, optional): Defaults to 4 (quarterly aggregation).
        filtering (bool, optional): Whether or not all papers in .jsonl are
            perceived as matches or whether an additional sanity checking for
            the keywords is performed in abstract/title. Defaults to False.
        filter_keys (list, optional): List of str used for filtering. Only
            applies if filtering is True. Defaults to empty list.
        unwanted_keys (list, optional): List of str that must not occur in either
            title or abstract. Only applies if filtering is True.
        return_filtered (bool, optional): Whether the filtered matches are also
            returned. Only applies if filtering is True. Defaults to False.
        filter_abstract (bool, optional): Whether the keyword is searched in the abstract
            or not. Defaults to True.
        last_year (int, optional): Most recent year for the aggregation. Defaults
            to current year. All newer entries are discarded.

    Returns:
        bins (np.array): Vector of length number of years (2020 - start_year) x
            bins_per_year.
    """

    if not isinstance(data, list):
        raise ValueError(f"Expected list, received {type(data)}")
    if not isinstance(bins_per_year, int):
        raise ValueError(f"Expected int, received {type(bins_per_year)}")
    if 12 % bins_per_year != 0:
        raise ValueError(f"Can't split year into {bins_per_year} bins")

    num_years = last_year - start_year + 1
    bins = np.zeros((num_years * bins_per_year))

    if len(data) == 0:
        return bins if not return_filtered else (bins, [])

    # Remove duplicate entries (keep only the first one)
    df = pd.DataFrame(data).sort_values(by="date", ascending=True)
    data = df.drop_duplicates(subset="title", keep="first").to_dict("records")

    dates = [dd["date"] for dd in data]

    filtered = []
    for paper, date in zip(data, dates):
        year = int(date.split("-")[0])
        if year < start_year or year > last_year:
            continue

        # At least one synonym per keyword needs to be in either title or
        # abstract.
        if filtering and filter_keys != list():
            # Filter out papers which undesired terms
            unwanted = False
            for unwanted_key in unwanted_keys:
                if unwanted_key.lower() in paper["title"].lower():
                    unwanted = True
                if (
                    filter_abstract
                    and paper["abstract"] is not None
                    and unwanted_key.lower() in paper["abstract"].lower()
                ):
                    unwanted = True
            if unwanted:
                continue

            got_keys = []
            for key_term in filter_keys:
                got_key = False
                if not isinstance(key_term, list):
                    key_term = [key_term]
                for key in key_term:
                    if key.lower() in paper["title"].lower():
                        got_key = True
                    if (
                        filter_abstract
                        and paper["abstract"] is not None
                        and key.lower() in paper["abstract"].lower()
                    ):
                        got_key = True
                got_keys.append(got_key)

            if len(got_keys) != sum(got_keys):
                continue

        filtered.append(paper)

        if len(date.split("-")) < 2:
            logger.warning(
                f"Paper without month {date}, randomly assigned month.{paper['title']}"
            )
            month = np.random.choice(12)
        else:
            month = int(date.split("-")[1])

        year_bin = year - start_year
        month_bin = int(np.floor((month - 1) / (12 / bins_per_year)))
        bins[year_bin * bins_per_year + month_bin] += 1

    if return_filtered:
        return bins, filtered
    else:
        return bins

```

##### File: `paperscraper-main/paperscraper/utils.py`

```python
import json
import logging
import sys
from typing import Dict, List

import pandas as pd

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)


def dump_papers(papers: pd.DataFrame, filepath: str) -> None:
    """
    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl
    file with one paper per line.

    Args:
        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.
        filepath (str): Path to dump the papers, has to end with `.jsonl`.
    """
    if not isinstance(filepath, str):
        raise TypeError(f"filepath must be a string, not {type(filepath)}")
    if not filepath.endswith(".jsonl"):
        raise ValueError("Please provide a filepath with .jsonl extension")

    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):
        papers = pd.DataFrame(papers)
        logger.warning(
            "Preferably pass a pd.DataFrame, not a list of dictionaries. "
            "Passing a list is a legacy functionality that might become deprecated."
        )

    if not isinstance(papers, pd.DataFrame):
        raise TypeError(f"papers must be a pd.DataFrame, not {type(papers)}")

    paper_list = list(papers.T.to_dict().values())

    with open(filepath, "w") as f:
        for paper in paper_list:
            f.write(json.dumps(paper) + "\n")


def get_filename_from_query(query: List[str]) -> str:
    """Convert a keyword query into filenames to dump the paper.

    Args:
        query (list): List of string with keywords.

    Returns:
        str: Filename.
    """
    filename = "_".join([k if isinstance(k, str) else k[0] for k in query]) + ".jsonl"
    filename = filename.replace(" ", "").lower()
    return filename


def load_jsonl(filepath: str) -> List[Dict[str, str]]:
    """
    Load data from a `.jsonl` file, i.e., a file with one dictionary per line.

    Args:
        filepath (str): Path to `.jsonl` file.

    Returns:
        List[Dict[str, str]]: A list of dictionaries, one per paper.
    """

    with open(filepath, "r") as f:
        data = [json.loads(line) for line in f.readlines()]
    return data

```

#### Directory: `paperscraper-main/paperscraper/arxiv`

##### File: `paperscraper-main/paperscraper/arxiv/__init__.py`

```python
from .arxiv import *  # noqa

```

##### File: `paperscraper-main/paperscraper/arxiv/arxiv.py`

```python
import glob
import logging
import os
import sys
from typing import Dict, List, Literal, Union

import arxiv
import pandas as pd
import pkg_resources
from tqdm import tqdm

from ..utils import dump_papers
from ..xrxiv.xrxiv_query import XRXivQuery
from .utils import get_query_from_keywords, infer_backend

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

dump_root = pkg_resources.resource_filename("paperscraper", "server_dumps")

global ARXIV_QUERIER
ARXIV_QUERIER = None


def search_local_arxiv():
    global ARXIV_QUERIER
    if ARXIV_QUERIER is not None:
        return
    dump_paths = glob.glob(os.path.join(dump_root, "arxiv*"))

    if len(dump_paths) > 0:
        path = sorted(dump_paths, reverse=True)[0]
        querier = XRXivQuery(path)
        if not querier.errored:
            ARXIV_QUERIER = querier.search_keywords
            logger.info(f"Loaded arxiv dump with {len(querier.df)} entries")


arxiv_field_mapper = {
    "published": "date",
    "journal_ref": "journal",
    "summary": "abstract",
    "entry_id": "doi",
}

# Authors, date, and journal fields need specific processing
process_fields = {
    "authors": lambda authors: ", ".join([a.name for a in authors]),
    "date": lambda date: date.strftime("%Y-%m-%d"),
    "journal": lambda j: j if j is not None else "",
    "doi": lambda entry_id: f"10.48550/arXiv.{entry_id.split('/')[-1].split('v')[0]}",
}


def get_arxiv_papers_local(
    keywords: List[Union[str, List[str]]],
    fields: List[str] = None,
    output_filepath: str = None,
) -> pd.DataFrame:
    """
    Search for papers in the dump using keywords.

    Args:
        keywords: Items will be AND separated. If items
            are lists themselves, they will be OR separated.
        fields: fields to be used in the query search.
            Defaults to None, a.k.a. search in all fields excluding date.
        output_filepath: optional output filepath where to store the hits in JSONL format.
            Defaults to None, a.k.a., no export to a file.

    Returns:
        pd.DataFrame: A dataframe with one paper per row.
    """
    search_local_arxiv()
    if ARXIV_QUERIER is None:
        raise ValueError(
            "Could not find local arxiv dump. Use `backend=api` or download dump via `paperscraper.get_dumps.arxiv"
        )
    return ARXIV_QUERIER(
        keywords=keywords, fields=fields, output_filepath=output_filepath
    )


def get_arxiv_papers_api(
    query: str,
    fields: List = ["title", "authors", "date", "abstract", "journal", "doi"],
    max_results: int = 99999,
    client_options: Dict = {"num_retries": 10},
    search_options: Dict = dict(),
    verbose: bool = True,
) -> pd.DataFrame:
    """
    Performs arxiv API request of a given query and returns list of papers with
    fields as desired.

    Args:
        query: Query to arxiv API. Needs to match the arxiv API notation.
        fields: List of strings with fields to keep in output.
        max_results: Maximal number of results, defaults to 99999.
        client_options: Optional arguments for `arxiv.Client`. E.g.:
            page_size (int), delay_seconds (int), num_retries (int).
            NOTE: Decreasing 'num_retries' will speed up processing but might
            result in more frequent 'UnexpectedEmptyPageErrors'.
        search_options: Optional arguments for `arxiv.Search`. E.g.:
            id_list (List), sort_by, or sort_order.

    Returns:
        pd.DataFrame: One row per paper.

    """
    client = arxiv.Client(**client_options)
    search = arxiv.Search(query=query, max_results=max_results, **search_options)
    results = client.results(search)

    processed = pd.DataFrame(
        [
            {
                arxiv_field_mapper.get(key, key): process_fields.get(
                    arxiv_field_mapper.get(key, key), lambda x: x
                )(value)
                for key, value in vars(paper).items()
                if arxiv_field_mapper.get(key, key) in fields and key != "doi"
            }
            for paper in tqdm(results, desc=f"Processing {query}", disable=not verbose)
        ]
    )
    return processed


def get_and_dump_arxiv_papers(
    keywords: List[Union[str, List[str]]],
    output_filepath: str,
    fields: List = ["title", "authors", "date", "abstract", "journal", "doi"],
    start_date: str = "None",
    end_date: str = "None",
    backend: Literal["api", "local", "infer"] = "api",
    *args,
    **kwargs,
):
    """
    Combines get_arxiv_papers and dump_papers.

    Args:
        keywords: List of keywords for arxiv search.
            The outer list level will be considered as AND separated keys, the
            inner level as OR separated.
        output_filepath: Path where the dump will be saved.
        fields: List of strings with fields to keep in output.
            Defaults to ['title', 'authors', 'date', 'abstract',
            'journal', 'doi'].
        start_date: Start date for the search. Needs to be in format:
            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific
            dates are used.
        end_date: End date for the search. Same notation as start_date.
        backend: If `api`, the arXiv API is queried. If `local` the local arXiv dump
            is queried (has to be downloaded before). If `infer` the local dump will
            be used if exists, otherwise API will be queried. Defaults to `api`
            since it is faster.
        *args, **kwargs are additional arguments for `get_arxiv_papers`.
    """
    # Translate keywords into query.
    query = get_query_from_keywords(keywords, start_date=start_date, end_date=end_date)

    if backend not in {"api", "local", "infer"}:
        raise ValueError(
            f"Invalid backend: {backend}. Must be one of ['api', 'local', 'infer']"
        )
    elif backend == "infer":
        backend = infer_backend()

    if backend == "api":
        papers = get_arxiv_papers_api(query, fields, *args, **kwargs)
    elif backend == "local":
        papers = get_arxiv_papers_local(keywords, fields, *args, **kwargs)
    dump_papers(papers, output_filepath)

```

##### File: `paperscraper-main/paperscraper/arxiv/utils.py`

```python
import glob
import os
from datetime import datetime
from typing import List, Union

import pkg_resources

finalize_disjunction = lambda x: "(" + x[:-4] + ") AND "
finalize_conjunction = lambda x: x[:-5]

EARLIEST_START = "1970-01-01"


def format_date(date_str: str) -> str:
    """Converts a date in YYYY-MM-DD format to arXiv's YYYYMMDDTTTT format."""
    date_obj = datetime.strptime(date_str, "%Y-%m-%d")
    return date_obj.strftime("%Y%m%d0000")


def get_query_from_keywords(
    keywords: List[Union[str, List[str]]],
    start_date: str = "None",
    end_date: str = "None",
) -> str:
    """Receives a list of keywords and returns the query for the arxiv API.

    Args:
        keywords (List[str, List[str]]): Items will be AND separated. If items
            are lists themselves, they will be OR separated.
        start_date (str): Start date for the search. Needs to be in format:
            YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific
            dates are used.
        end_date (str): End date for the search. Same notation as start_date.

    Returns:
        str: query to enter to arxiv API.
    """

    query = ""
    for i, key in enumerate(keywords):
        if isinstance(key, str):
            query += f"all:{key} AND "
        elif isinstance(key, list):
            inter = "".join([f"all:{syn} OR " for syn in key])
            query += finalize_disjunction(inter)

    query = finalize_conjunction(query)
    if start_date == "None" and end_date == "None":
        return query
    elif start_date == "None":
        start_date = EARLIEST_START
    elif end_date == "None":
        end_date = datetime.now().strftime("%Y-%m-%d")

    start = format_date(start_date)
    end = format_date(end_date)
    date_filter = f" AND submittedDate:[{start} TO {end}]"
    return query + date_filter


def infer_backend():
    dump_root = pkg_resources.resource_filename("paperscraper", "server_dumps")
    dump_paths = glob.glob(os.path.join(dump_root, "arxiv" + "*"))
    return "api" if not dump_paths else "local"

```

#### Directory: `paperscraper-main/paperscraper/citations`

##### File: `paperscraper-main/paperscraper/citations/__init__.py`

```python
from .citations import get_citations_by_doi, get_citations_from_title  # noqa
from .core import SelfLinkClient  # noqa
from .self_citations import self_citations_paper  # noqa
from .self_references import self_references_paper  # noqa

```

##### File: `paperscraper-main/paperscraper/citations/citations.py`

```python
import logging
import sys
from time import sleep

from scholarly import scholarly
from semanticscholar import SemanticScholar, SemanticScholarException

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)
sch = SemanticScholar()


def get_citations_by_doi(doi: str) -> int:
    """
    Get the number of citations of a paper according to semantic scholar.

    Args:
        doi: the DOI of the paper.

    Returns:
        The number of citations
    """

    try:
        paper = sch.get_paper(doi)
        citations = len(paper["citations"])
    except SemanticScholarException.ObjectNotFoundException:
        logger.warning(f"Could not find paper {doi}, assuming 0 citation.")
        citations = 0
    except ConnectionRefusedError as e:
        logger.warning(f"Waiting for 10 sec since {doi} gave: {e}")
        sleep(10)
        citations = len(sch.get_paper(doi)["citations"])
    finally:
        return citations


def get_citations_from_title(title: str) -> int:
    """
    Args:
        title (str): Title of paper to be searched on Scholar.

    Raises:
        TypeError: If sth else than str is passed.

    Returns:
        int: Number of citations of paper.
    """

    if not isinstance(title, str):
        raise TypeError(f"Pass str not {type(title)}")

    # Search for exact match
    title = '"' + title.strip() + '"'

    matches = scholarly.search_pubs(title)
    counts = list(map(lambda p: int(p["num_citations"]), matches))
    if len(counts) == 0:
        logger.warning(f"Found no match for {title}.")
        return 0
    if len(counts) > 1:
        logger.warning(f"Found {len(counts)} matches for {title}, returning first one.")
    return counts[0]

```

##### File: `paperscraper-main/paperscraper/citations/core.py`

```python
import logging
import sys
from typing import Literal

from .entity import Paper, Researcher

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)

ModeType = Literal[tuple(MODES := ("paper", "author"))]


class SelfLinkClient:
    def __init__(self, entity: str, mode: ModeType = "paper") -> None:
        self.mode = mode.lower()
        if self.mode not in MODES:
            raise ValueError(f"Unknown mode `{self.mode}`, chose from {MODES}")
        if self.mode == "paper":
            self.object = Paper(entity)

        elif mode == "author":
            self.object = Researcher(entity)

    def extract_self_citations(self):
        self.object.self_citations()

    def extract_self_references(self):
        self.object.self_references()

    def extract(self):
        self.extract_self_citations()
        self.extract_self_references()

    def get_result(self):
        return self.object.get_result()

```

##### File: `paperscraper-main/paperscraper/citations/orcid.py`

```python
import logging
import sys
from typing import Optional

import requests

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

BASE_URL = "https://pub.orcid.org/v3.0/"


def orcid_to_author_name(orcid_id: str) -> Optional[str]:
    """
    Given an ORCID ID (as a string, e.g. '0000-0002-1825-0097'),
    returns the full name of the author from the ORCID public API.
    """

    headers = {"Accept": "application/json"}
    response = requests.get(f"{BASE_URL}{orcid_id}/person", headers=headers)
    if response.status_code == 200:
        data = response.json()
        given = data.get("name", {}).get("given-names", {}).get("value", "")
        family = data.get("name", {}).get("family-name", {}).get("value", "")
        full_name = f"{given} {family}".strip()
        return full_name
    logger.error(
        f"Error fetching ORCID data ({orcid_id}): {response.status_code} {response.text}"
    )

```

##### File: `paperscraper-main/paperscraper/citations/self_citations.py`

```python
import asyncio
import logging
import re
import sys
from typing import Any, Dict, List, Union

import httpx
import numpy as np
from pydantic import BaseModel

from ..async_utils import optional_async, retry_with_exponential_backoff
from .utils import DOI_PATTERN, find_matching

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)


class CitationResult(BaseModel):
    ssid: str  # semantic scholar paper id
    num_citations: int
    self_citations: Dict[str, float] = {}
    citation_score: float


async def _fetch_citation_data(
    client: httpx.AsyncClient, suffix: str
) -> Dict[str, Any]:
    """
    Fetch raw paper data from Semantic Scholar by DOI or SSID suffix.

    Args:
        client: An active httpx.AsyncClient.
        suffix: Prefixed identifier (e.g., "DOI:10.1000/xyz123" or SSID).

    Returns:
        The JSON-decoded response as a dictionary.
    """
    response = await client.get(
        f"https://api.semanticscholar.org/graph/v1/paper/{suffix}",
        params={"fields": "title,authors,citations.authors"},
    )
    response.raise_for_status()
    return response.json()


async def _process_single(client: httpx.AsyncClient, identifier: str) -> CitationResult:
    """
    Compute self-citation stats for a single paper.

    Args:
        client: An active httpx.AsyncClient.
        identifier: A DOI or Semantic Scholar ID.

    Returns:
        A CitationResult containing counts and percentages of self-citations.
    """
    # Determine prefix for Semantic Scholar API
    if len(identifier) > 15 and identifier.isalnum() and identifier.islower():
        prefix = ""
    elif len(re.findall(DOI_PATTERN, identifier, re.IGNORECASE)) == 1:
        prefix = "DOI:"
    else:
        prefix = ""

    suffix = f"{prefix}{identifier}"
    paper = await _fetch_citation_data(client, suffix)

    # Initialize counters
    author_counts: Dict[str, int] = {a["name"]: 0 for a in paper.get("authors", [])}
    citations = paper.get("citations", [])
    total_cites = len(citations)

    # Tally self-citations
    for cite in citations:
        matched = find_matching(paper.get("authors", []), cite.get("authors", []))
        for name in matched:
            author_counts[name] += 1

    # Compute percentages
    ratios: Dict[str, float] = {
        name: round((count / total_cites * 100), 2) if total_cites > 0 else 0.0
        for name, count in author_counts.items()
    }

    avg_score = round(float(np.mean(list(ratios.values()))) if ratios else 0.0, 3)

    return CitationResult(
        ssid=identifier,
        num_citations=total_cites,
        self_citations=ratios,
        citation_score=avg_score,
    )


@optional_async
@retry_with_exponential_backoff(max_retries=4, base_delay=1.0)
async def self_citations_paper(
    inputs: Union[str, List[str]], verbose: bool = False
) -> Union[CitationResult, List[CitationResult]]:
    """
    Analyze self-citations for one or more papers by DOI or Semantic Scholar ID.

    Args:
        inputs: A single DOI/SSID string or a list of them.
        verbose: If True, logs detailed information for each paper.

    Returns:
        A single CitationResult if a string was passed, else a list of CitationResults.
    """
    single_input = isinstance(inputs, str)
    identifiers = [inputs] if single_input else list(inputs)

    async with httpx.AsyncClient(timeout=httpx.Timeout(20)) as client:
        tasks = [_process_single(client, ident) for ident in identifiers]
        results = await asyncio.gather(*tasks)

    if verbose:
        for res in results:
            logger.info(
                f'Self-citations in "{res.ssid}": N={res.num_citations}, Score={res.citation_score}%'
            )
            for author, pct in res.self_citations.items():
                logger.info(f"  {author}: {pct}%")

    return results[0] if single_input else results

```

##### File: `paperscraper-main/paperscraper/citations/self_references.py`

```python
import asyncio
import logging
import re
import sys
from typing import Any, Dict, List, Literal, Union

import httpx
import numpy as np
from pydantic import BaseModel

from ..async_utils import optional_async, retry_with_exponential_backoff
from .utils import DOI_PATTERN, find_matching

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)
ModeType = Literal[tuple(MODES := ("doi", "infer", "ssid"))]


class ReferenceResult(BaseModel):
    ssid: str  # semantic scholar paper id
    num_references: int
    self_references: Dict[str, float] = {}
    reference_score: float


async def _fetch_reference_data(
    client: httpx.AsyncClient, suffix: str
) -> Dict[str, Any]:
    """
    Fetch raw paper data from Semantic Scholar by DOI or SSID suffix.

    Args:
        client: An active httpx.AsyncClient.
        suffix: Prefixed identifier (e.g., "DOI:10.1000/xyz123" or SSID).

    Returns:
        The JSON-decoded response as a dictionary.
    """
    response = await client.get(
        f"https://api.semanticscholar.org/graph/v1/paper/{suffix}",
        params={"fields": "title,authors,references.authors"},
    )
    response.raise_for_status()
    return response.json()


async def _process_single_reference(
    client: httpx.AsyncClient, identifier: str
) -> ReferenceResult:
    """
    Compute self-reference statistics for a single paper.

    Args:
        client: An active httpx.AsyncClient.
        identifier: A DOI or Semantic Scholar ID.

    Returns:
        A ReferenceResult containing counts and percentages of self-references.
    """
    # Determine prefix for API
    if len(identifier) > 15 and identifier.isalnum() and identifier.islower():
        prefix = ""
    elif len(re.findall(DOI_PATTERN, identifier, re.IGNORECASE)) == 1:
        prefix = "DOI:"
    else:
        prefix = ""

    suffix = f"{prefix}{identifier}"
    paper = await _fetch_reference_data(client, suffix)

    # Initialize counters
    author_counts: Dict[str, int] = {a["name"]: 0 for a in paper.get("authors", [])}
    references = paper.get("references", [])
    total_refs = len(references)

    # Tally self-references
    for ref in references:
        matched = find_matching(paper.get("authors", []), ref.get("authors", []))
        for name in matched:
            author_counts[name] += 1

    # Compute percentages per author
    ratios: Dict[str, float] = {
        name: round((count / total_refs * 100), 2) if total_refs > 0 else 0.0
        for name, count in author_counts.items()
    }

    # Compute average score
    avg_score = round(float(np.mean(list(ratios.values()))) if ratios else 0.0, 3)

    return ReferenceResult(
        ssid=identifier,
        num_references=total_refs,
        self_references=ratios,
        reference_score=avg_score,
    )


@optional_async
@retry_with_exponential_backoff(max_retries=4, base_delay=1.0)
async def self_references_paper(
    inputs: Union[str, List[str]], verbose: bool = False
) -> Union[ReferenceResult, List[ReferenceResult]]:
    """
    Analyze self-references for one or more papers by DOI or Semantic Scholar ID.

    Args:
        inputs: A single DOI/SSID string or a list of them.
        verbose: If True, logs detailed information for each paper.

    Returns:
        A single ReferenceResult if a string was passed, else a list of ReferenceResults.

    Raises:
        ValueError: If no references are found for a given identifier.
    """
    single_input = isinstance(inputs, str)
    identifiers = [inputs] if single_input else list(inputs)

    async with httpx.AsyncClient(timeout=httpx.Timeout(20)) as client:
        tasks = [_process_single_reference(client, ident) for ident in identifiers]
        results = await asyncio.gather(*tasks)

    if verbose:
        for res in results:
            logger.info(
                f'Self-references in "{res.ssid}": N={res.num_references}, '
                f"Score={res.reference_score}%"
            )
            for author, pct in res.self_references.items():
                logger.info(f"  {author}: {pct}% self-reference")

    return results[0] if single_input else results

```

##### File: `paperscraper-main/paperscraper/citations/utils.py`

```python
import logging
import re
import sys
from time import sleep
from typing import Any, Dict, List, Literal, Optional

import httpx
import requests
from tqdm import tqdm
from unidecode import unidecode

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)

DOI_PATTERN = r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b"
PAPER_URL: str = "https://api.semanticscholar.org/graph/v1/paper/"
AUTHOR_URL: str = "https://api.semanticscholar.org/graph/v1/author/search"


def get_doi_from_title(title: str) -> Optional[str]:
    """
    Searches the DOI of a paper based on the paper title

    Args:
        title: Paper title

    Returns:
        DOI according to semantic scholar API
    """
    response = requests.get(
        PAPER_URL + "search",
        params={"query": title, "fields": "externalIds", "limit": 1},
    )
    data = response.json()

    if data.get("data"):
        paper = data["data"][0]
        doi = paper.get("externalIds", {}).get("DOI")
        if doi:
            return doi
    logger.warning(f"Did not find DOI for title={title}")


def get_doi_from_ssid(ssid: str, max_retries: int = 10) -> Optional[str]:
    """
    Given a Semantic Scholar paper ID, returns the corresponding DOI if available.

    Parameters:
      ssid (str): The paper ID on Semantic Scholar.

    Returns:
      str or None: The DOI of the paper, or None if not found or in case of an error.
    """
    logger.warning(
        "Semantic Scholar API is easily overloaded when passing SS IDs, provide DOIs to improve throughput."
    )
    attempts = 0
    for attempt in tqdm(
        range(1, max_retries + 1), desc=f"Fetching DOI for {ssid}", unit="attempt"
    ):
        # Make the GET request to Semantic Scholar.
        response = requests.get(
            f"{PAPER_URL}{ssid}", params={"fields": "externalIds", "limit": 1}
        )

        # If successful, try to extract and return the DOI.
        if response.status_code == 200:
            data = response.json()
            doi = data.get("externalIds", {}).get("DOI")
            return doi
        attempts += 1
        sleep(10)
    logger.warning(
        f"Did not find DOI for paper ID {ssid}. Code={response.status_code}, text={response.text}"
    )


def get_title_and_id_from_doi(doi: str) -> Dict[str, Any]:
    """
    Given a DOI, retrieves the paper's title and semantic scholar paper ID.

    Parameters:
        doi (str): The DOI of the paper (e.g., "10.18653/v1/N18-3011").

    Returns:
        dict or None: A dictionary with keys 'title' and 'ssid'.
    """

    # Send the GET request to Semantic Scholar
    response = requests.get(f"{PAPER_URL}DOI:{doi}")
    if response.status_code == 200:
        data = response.json()
        return {"title": data.get("title"), "ssid": data.get("paperId")}
    logger.warning(
        f"Could not get authors & semantic scholar ID for DOI={doi}, {response.status_code}: {response.text}"
    )


def author_name_to_ssaid(author_name: str) -> str:
    """
    Given an author name, returns the Semantic Scholar author ID.

    Parameters:
        author_name (str): The full name of the author.

    Returns:
        str or None: The Semantic Scholar author ID or None if no author is found.
    """

    response = requests.get(
        AUTHOR_URL, params={"query": author_name, "fields": "name", "limit": 1}
    )
    if response.status_code == 200:
        data = response.json()
        authors = data.get("data", [])
        if authors:
            # Return the Semantic Scholar author ID from the first result.
            return authors[0].get("authorId")

    logger.error(
        f"Error in retrieving name from SS Author ID: {response.status_code} - {response.text}"
    )


def determine_paper_input_type(input: str) -> Literal["ssid", "doi", "title"]:
    """
    Determines the intended input type by the user if not explicitly given (`infer`).

    Args:
        input: Either a DOI or a semantic scholar paper ID or an author name.

    Returns:
        The input type
    """
    if len(input) > 15 and " " not in input and (input.isalnum() and input.islower()):
        mode = "ssid"
    elif len(re.findall(DOI_PATTERN, input, re.IGNORECASE)) == 1:
        mode = "doi"
    else:
        logger.info(
            f"Assuming `{input}` is a paper title, since it seems neither a DOI nor a paper ID"
        )
        mode = "title"
    return mode


async def get_papers_for_author(ss_author_id: str) -> List[str]:
    """
    Given a Semantic Scholar author ID, returns a list of all Semantic Scholar paper IDs for that author.

    Args:
        ss_author_id (str): The Semantic Scholar author ID (e.g., "1741101").

    Returns:
        A list of paper IDs (as strings) authored by the given author.
    """
    papers = []
    offset = 0
    limit = 100

    async with httpx.AsyncClient() as client:
        while True:
            response = await client.get(
                f"https://api.semanticscholar.org/graph/v1/author/{ss_author_id}/papers",
                params={"fields": "paperId", "offset": offset, "limit": limit},
            )
            response.raise_for_status()
            data = response.json()
            page = data.get("data", [])

            # Extract paper IDs from the current page.
            for paper in page:
                if "paperId" in paper:
                    papers.append(paper["paperId"])

            # If fewer papers were returned than the limit, we've reached the end.
            if len(page) < limit:
                break

            offset += limit

    return papers


def find_matching(
    first: List[Dict[str, str]], second: List[Dict[str, str]]
) -> List[str]:
    """
    Ingests two sets of authors and returns a list of those that match (either based on name
        or on author ID).

    Args:
        first: First set of authors given as list of dict with two keys (`authorID` and `name`).
        second: Second set of authors given as list of dict with two same keys.

    Returns:
        List of names of authors in first list where a match was found.
    """
    # Check which author IDs overlap
    second_names = set(map(lambda x: x["authorId"], second))
    overlap_ids = {f["name"] for f in first if f["authorId"] in second_names}

    overlap_names = {
        f["name"]
        for f in first
        if f["authorId"] not in overlap_ids
        and any([check_overlap(f["name"], s["name"]) for s in second])
    }
    return list(overlap_ids | overlap_names)


def check_overlap(n1: str, n2: str) -> bool:
    """
    Check whether two author names are identical.
    TODO: This can be made more robust

    Args:
        n1: first name
        n2: second name

    Returns:
        bool: Whether names are identical.
    """
    # remove initials and check for name intersection
    s1 = {w for w in clean_name(n1).split()}
    s2 = {w for w in clean_name(n2).split()}
    return len(s2) > 0 and len(s1 | s2) == len(s1)


def clean_name(s: str) -> str:
    """
    Clean up a str by removing special characters.

    Args:
        s: Input possibly containing special symbols

    Returns:
        Homogenized string.
    """
    return "".join(ch for ch in unidecode(s) if ch.isalpha() or ch.isspace()).lower()

```

#### Directory: `paperscraper-main/paperscraper/citations/entity`

##### File: `paperscraper-main/paperscraper/citations/entity/__init__.py`

```python
from .paper import Paper, PaperResult  # noqa
from .researcher import Researcher, ResearcherResult  # noqa

```

##### File: `paperscraper-main/paperscraper/citations/entity/core.py`

```python
from abc import abstractmethod
from typing import Dict

from pydantic import BaseModel


class EntityResult(BaseModel):
    num_citations: int
    num_references: int
    # keys are authors or papers and values are absolute self links
    self_citations: Dict[str, int] = {}
    self_references: Dict[str, int] = {}
    # aggregated results
    self_citation_ratio: float = 0
    self_reference_ratio: float = 0


class Entity:
    """
    An abstract entity class with a set of utilities shared by the objects that perform
    self-linking analyses, such as Paper and Researcher.
    """

    @abstractmethod
    def self_references(self):
        """
        Has to be implemented by the child class. Performs a self-referencing analyses
        for the object.
        """
        ...

    @abstractmethod
    def self_citations(self):
        """
        Has to be implemented by the child class. Performs a self-citation analyses
        for the object.
        """
        ...

    @abstractmethod
    def get_result(self):
        """
        Has to be implemented by the child class. Provides the result of the analysis.
        """
        ...

```

##### File: `paperscraper-main/paperscraper/citations/entity/paper.py`

```python
import logging
import sys
from typing import List, Literal, Optional

from ..self_citations import CitationResult, self_citations_paper
from ..self_references import ReferenceResult, self_references_paper
from ..utils import (
    determine_paper_input_type,
    get_doi_from_ssid,
    get_doi_from_title,
    get_title_and_id_from_doi,
)
from .core import Entity

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)


class PaperResult(ReferenceResult, CitationResult):
    title: str


ModeType = Literal[tuple(MODES := ("doi", "title", "ss_id", "infer"))]

BASE_URL: str = "https://api.semanticscholar.org/graph/v1/paper/search"


class Paper(Entity):
    title: str = ""
    doi: str = ""
    authors: List[str] = []

    def __init__(self, input: str, mode: ModeType = "infer"):
        """
        Set up a Paper object for analysis.

        Args:
            input: Paper identifier. This can be the title, DOI or semantic scholar ID
                of the paper.
            mode: The format in which the ID was provided. Defaults to "infer".

        Raises:
            ValueError: If unknown mode is given.
        """
        if mode not in MODES:
            raise ValueError(f"Unknown mode {mode} chose from {MODES}.")

        input = input.strip()
        self.input = input
        if mode == "infer":
            mode = determine_paper_input_type(input)

        if mode == "doi":
            self.doi = input
        elif mode == "title":
            self.doi = get_doi_from_title(input)
        elif mode == "ssid":
            self.doi = get_doi_from_ssid(input)

        if self.doi is not None:
            out = get_title_and_id_from_doi(self.doi)
            if out is not None:
                self.title = out["title"]
                self.ssid = out["ssid"]

    def self_references(self):
        """
        Extracts the self references of a paper, for each author.
        """
        if isinstance(self.doi, str):
            self.ref_result: ReferenceResult = self_references_paper(self.doi)

    def self_citations(self):
        """
        Extracts the self citations of a paper, for each author.
        """
        if isinstance(self.doi, str):
            self.citation_result: CitationResult = self_citations_paper(self.doi)

    def get_result(self) -> Optional[PaperResult]:
        """
        Provides the result of the analysis.

        Returns: PaperResult if available.
        """
        if not hasattr(self, "ref_result"):
            logger.warning(
                f"Can't get result since no referencing result for {self.input} exists. Run `.self_references` first."
            )
            return
        elif not hasattr(self, "citation_result"):
            logger.warning(
                f"Can't get result since no citation result for {self.input} exists. Run `.self_citations` first."
            )
            return
        ref_result = self.ref_result.model_dump()
        ref_result.pop("ssid", None)
        return PaperResult(
            title=self.title, **ref_result, **self.citation_result.model_dump()
        )

```

##### File: `paperscraper-main/paperscraper/citations/entity/researcher.py`

```python
from typing import List, Literal, Optional

from semanticscholar import SemanticScholar
from tqdm import tqdm

from ..orcid import orcid_to_author_name
from ..self_references import ReferenceResult
from ..utils import author_name_to_ssaid, get_papers_for_author
from .core import Entity, EntityResult


class ResearcherResult(EntityResult):
    name: str
    ssid: int
    orcid: Optional[str] = None
    # TODO: the ratios will be averaged across all papers for that author


ModeType = Literal[tuple(MODES := ("name", "orcid", "ssaid", "infer"))]

sch = SemanticScholar()


class Researcher(Entity):
    name: str
    ssid: int
    orcid: Optional[str] = None

    def __init__(self, input: str, mode: ModeType = "infer"):
        """
        Construct researcher object for self citation/reference analysis.

        Args:
            input: A researcher to search for.
            mode: This can be a `name` `orcid` (ORCID iD) or `ssaid` (Semantic Scholar Author ID).
                Defaults to "infer".

        Raises:
            ValueError: Unknown mode
        """
        if mode not in MODES:
            raise ValueError(f"Unknown mode {mode} chose from {MODES}.")

        input = input.strip()
        if mode == "infer":
            if input.isdigit():
                mode = "ssaid"
            elif (
                input.count("-") == 3
                and len(input) == 19
                and all([x.isdigit() for x in input.split("-")])
            ):
                mode = "orcid"
            else:
                mode = "author"

        if mode == "ssaid":
            self.author = sch.get_author(input)
            self.ssid = input
        elif mode == "orcid":
            self.author = orcid_to_author_name(input)
            self.orcid = input
            self.ssid = author_name_to_ssaid(input)
        elif mode == "author":
            self.author = input
            self.ssid = author_name_to_ssaid(input)

        # TODO: Skip over erratum / corrigendum
        self.ssids = get_papers_for_author(self.ssid)

    def self_references(self):
        """
        Sifts through all papers of a researcher and extracts the self references.
        """
        # TODO: Asynchronous call to self_references
        print("Going through SSIDs", self.ssids)

        # TODO: Aggregate results

    def self_citations(self):
        """
        Sifts through all papers of a researcher and finds how often they are self-cited.
        """
        ...

    def get_result(self) -> ResearcherResult:
        """
        Provides the result of the analysis.
        """
        ...

```

#### Directory: `paperscraper-main/paperscraper/citations/tests`

##### File: `paperscraper-main/paperscraper/citations/tests/__init__.py`

```python

```

##### File: `paperscraper-main/paperscraper/citations/tests/test_citations.py`

```python
import logging

from paperscraper.citations import get_citations_by_doi

logging.disable(logging.INFO)


class TestCitations:
    def test_citations(self):
        num = get_citations_by_doi("10.1038/s42256-023-00639-z")
        assert isinstance(num, int) and num > 50

        # Try invalid DOI
        num = get_citations_by_doi("10.1035348/s42256-023-00639-z")
        assert isinstance(num, int) and num == 0

        num = get_citations_by_doi("10.1035348/s42256-023-00639-z")
        assert isinstance(num, int) and num == 0

```

##### File: `paperscraper-main/paperscraper/citations/tests/test_paper.py`

```python
import logging

import pytest

from paperscraper.citations import SelfLinkClient
from paperscraper.citations.entity import PaperResult

logging.disable(logging.INFO)


class TestPaper:
    @pytest.fixture
    def ssids(self):
        return [
            "a732443cae8cd2d6a76f4f3cf785a562baf41137",  # semantic scholar ID
        ]

    @pytest.fixture
    def dois(self):
        return ["10.1038/s41586-023-06600-9", "10.1016/j.neunet.2014.09.003"]

    def test_paper_doi(self, dois):
        for doi in dois:
            client = SelfLinkClient(entity=doi, mode="paper")
            client.extract()
            result = client.get_result()
            assert isinstance(result, PaperResult)
            assert isinstance(result.ssid, str)
            assert isinstance(result.title, str)
            assert isinstance(result.citation_score, float)
            assert isinstance(result.reference_score, float)
            assert result.citation_score >= 0
            assert result.reference_score >= 0
            assert isinstance(result.self_references, dict)
            assert isinstance(result.self_citations, dict)

    def test_paper_ssid(self, ssids):
        for ssid in ssids:
            client = SelfLinkClient(entity=ssid, mode="paper")
            client.extract()
            result = client.get_result()
            if result is None:
                return
            assert isinstance(result, PaperResult)
            assert isinstance(result.ssid, str)
            assert isinstance(result.title, str)
            assert isinstance(result.citation_score, float)
            assert isinstance(result.reference_score, float)
            assert result.citation_score >= 0
            assert result.reference_score >= 0
            assert isinstance(result.self_references, dict)
            assert isinstance(result.self_citations, dict)

```

##### File: `paperscraper-main/paperscraper/citations/tests/test_self_citations.py`

```python
import logging
import time

import pytest

from paperscraper.citations import self_citations_paper
from paperscraper.citations.self_citations import CitationResult

logging.disable(logging.INFO)


class TestSelfCitations:
    @pytest.fixture
    def dois(self):
        return [
            "10.1038/s41586-023-06600-9",
            "ed69978f1594a4e2b9dccfc950490fa1df817ae8",
        ]

    def test_single_doi(self, dois):
        result = self_citations_paper(dois[0])
        assert isinstance(result, CitationResult)
        assert isinstance(result.ssid, str)
        assert isinstance(result.num_citations, int)
        assert result.num_citations > 10
        assert isinstance(result.citation_score, float)
        assert result.citation_score > 0
        for author, self_cites in result.self_citations.items():
            assert isinstance(author, str)
            assert isinstance(self_cites, float)
            assert self_cites >= 0 and self_cites <= 100
        time.sleep(5)

    def test_multiple_dois(self, dois):
        start_time = time.perf_counter()
        result = self_citations_paper(dois)
        async_duration = time.perf_counter() - start_time
        assert isinstance(result, list)
        assert len(result) == len(dois)
        for cit_result in result:
            assert isinstance(cit_result, CitationResult)
            assert isinstance(cit_result.ssid, str)
            assert isinstance(cit_result.num_citations, int)
            assert cit_result.num_citations > 0
            assert cit_result.citation_score > 0
            assert isinstance(cit_result.citation_score, float)
            for author, self_cites in cit_result.self_citations.items():
                assert isinstance(author, str)
                assert isinstance(self_cites, float)
                assert self_cites >= 0 and self_cites <= 100
        time.sleep(5)

        # compare async and sync performance

        # Measure synchronous execution time (three independent calls)
        start_time = time.perf_counter()
        sync_result = [self_citations_paper(doi) for doi in dois]
        sync_duration = time.perf_counter() - start_time

        print(f"Asynchronous execution time (batch): {async_duration:.2f} seconds")
        print(
            f"Synchronous execution time (independent calls): {sync_duration:.2f} seconds"
        )

        assert 0.1 * async_duration <= sync_duration, (
            f"Async execution ({async_duration:.2f}s) is slower than sync execution "
            f"({sync_duration:.2f}s)"
        )

        for a, s in zip(result, sync_result):
            assert a == s, f"{a} vs {s}"

```

##### File: `paperscraper-main/paperscraper/citations/tests/test_self_references.py`

```python
import logging
import time
from typing import Dict

import pytest

from paperscraper.citations import self_references_paper
from paperscraper.citations.self_references import ReferenceResult

logging.disable(logging.INFO)


class TestSelfReferences:
    @pytest.fixture
    def dois(self):
        return [
            "10.1038/s41586-023-06600-9",
            "10.1016/j.neunet.2014.09.003",
        ]

    def test_single_doi(self, dois):
        result = self_references_paper(dois[0])
        assert isinstance(result, ReferenceResult)
        assert isinstance(result.num_references, int)
        assert result.num_references > 0
        assert isinstance(result.ssid, str)
        assert isinstance(result.reference_score, float)
        assert result.reference_score > 0
        assert isinstance(result.self_references, Dict)
        for author, self_cites in result.self_references.items():
            assert isinstance(author, str)
            assert isinstance(self_cites, float)
            assert self_cites >= 0 and self_cites <= 100

    def test_multiple_dois(self, dois):
        results = self_references_paper(dois[1:])
        assert isinstance(results, list)
        assert len(results) == len(dois[1:])
        for ref_result in results:
            assert isinstance(ref_result, ReferenceResult)
            assert isinstance(ref_result.ssid, str)
            assert isinstance(ref_result.num_references, int)
            assert ref_result.num_references > 0
            assert ref_result.reference_score > 0
            assert isinstance(ref_result.reference_score, float)
            for author, self_cites in ref_result.self_references.items():
                assert isinstance(author, str)
                assert isinstance(self_cites, float)
                assert self_cites >= 0 and self_cites <= 100

    def test_compare_async_and_sync_performance(self, dois):
        """
        Compares the execution time of asynchronous and synchronous `self_references`
        for a list of DOIs.
        """

        start_time = time.perf_counter()
        async_results = self_references_paper(dois)
        async_duration = time.perf_counter() - start_time

        # Measure synchronous execution time (three independent calls)
        start_time = time.perf_counter()
        sync_results = [self_references_paper(doi) for doi in dois]

        sync_duration = time.perf_counter() - start_time

        print(f"Asynchronous execution time (batch): {async_duration:.2f} seconds")
        print(
            f"Synchronous execution time (independent calls): {sync_duration:.2f} seconds"
        )
        for a, s in zip(async_results, sync_results):
            assert a == s, f"{a} vs {s}"

        # Assert that async execution (batch) is faster or at least not slower
        assert 0.9 * async_duration <= sync_duration, (
            f"Async execution ({async_duration:.2f}s) is slower than sync execution "
            f"({sync_duration:.2f}s)"
        )

```

#### Directory: `paperscraper-main/paperscraper/get_dumps`

##### File: `paperscraper-main/paperscraper/get_dumps/__init__.py`

```python
from .arxiv import arxiv  # noqa
from .biorxiv import biorxiv  # noqa
from .chemrxiv import chemrxiv  # noqa
from .medrxiv import medrxiv  # noqa

```

##### File: `paperscraper-main/paperscraper/get_dumps/arxiv.py`

```python
"""Dump arxiv data in JSONL format."""

import json
import os
from datetime import datetime, timedelta
from typing import Optional

import pkg_resources
from tqdm import tqdm

from ..arxiv import get_arxiv_papers_api

today = datetime.today().strftime("%Y-%m-%d")
save_folder = pkg_resources.resource_filename("paperscraper", "server_dumps")
save_path = os.path.join(save_folder, f"arxiv_{today}.jsonl")


def arxiv(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    save_path: str = save_path,
):
    """
    Fetches papers from arXiv based on time range, i.e., start_date and end_date.
    If the start_date and end_date are not provided, fetches papers from the earliest
    possible date to the current date. The fetched papers are stored in JSONL format.

    Args:
        start_date (str, optional): Start date in format YYYY-MM-DD. Defaults to None.
        end_date (str, optional): End date in format YYYY-MM-DD. Defaults to None.
        save_path (str, optional): Path to save the JSONL dump. Defaults to save_path.
    """
    # Set default dates
    EARLIEST_START = "1991-01-01"
    if start_date is None:
        start_date = EARLIEST_START
    if end_date is None:
        end_date = datetime.today().strftime("%Y-%m-%d")

    # Convert dates to datetime objects
    start_date = datetime.strptime(start_date, "%Y-%m-%d")
    end_date = datetime.strptime(end_date, "%Y-%m-%d")

    if start_date > end_date:
        raise ValueError(
            f"start_date {start_date} cannot be later than end_date {end_date}"
        )

    # Open file for writing results
    with open(save_path, "w") as fp:
        progress_bar = tqdm(total=(end_date - start_date).days + 1)

        current_date = start_date
        while current_date <= end_date:
            next_date = current_date + timedelta(days=1)
            progress_bar.set_description(
                f"Fetching {current_date.strftime('%Y-%m-%d')}"
            )

            # Format dates for query
            query = f"submittedDate:[{current_date.strftime('%Y%m%d0000')} TO {next_date.strftime('%Y%m%d0000')}]"
            try:
                papers = get_arxiv_papers_api(
                    query=query,
                    fields=["title", "authors", "date", "abstract", "journal", "doi"],
                    verbose=False,
                )
                if not papers.empty:
                    for paper in papers.to_dict(orient="records"):
                        fp.write(json.dumps(paper) + "\n")
            except Exception as e:
                print(f"Arxiv scraping error: {current_date.strftime('%Y-%m-%d')}: {e}")
            current_date = next_date
            progress_bar.update(1)

```

##### File: `paperscraper-main/paperscraper/get_dumps/biorxiv.py`

```python
"""Dump bioRxiv data in JSONL format."""

import json
import os
from datetime import datetime
from typing import Optional

import pkg_resources
from tqdm import tqdm

from ..xrxiv.xrxiv_api import BioRxivApi

today = datetime.today().strftime("%Y-%m-%d")
save_path = os.path.join(
    pkg_resources.resource_filename("paperscraper", "server_dumps"),
    f"biorxiv_{today}.jsonl",
)


def biorxiv(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    save_path: str = save_path,
    max_retries: int = 10,
):
    """Fetches papers from biorxiv based on time range, i.e., start_date and end_date.
    If the start_date and end_date are not provided, papers will be fetched from biorxiv
    from the launch date of biorxiv until the current date. The fetched papers will be
    stored in jsonl format in save_path.

    Args:
        start_date (str, optional): begin date expressed as YYYY-MM-DD.
            Defaults to None, i.e., earliest possible.
        end_date (str, optional): end date expressed as YYYY-MM-DD.
            Defaults to None, i.e., today.
        save_path (str, optional): Path where the dump is stored.
            Defaults to save_path.
        max_retries (int, optional): Number of retries when API shows connection issues.
            Defaults to 10.
    """
    # create API client
    api = BioRxivApi(max_retries=max_retries)

    # dump all papers
    with open(save_path, "w") as fp:
        for index, paper in enumerate(
            tqdm(api.get_papers(start_date=start_date, end_date=end_date))
        ):
            if index > 0:
                fp.write(os.linesep)
            fp.write(json.dumps(paper))

```

##### File: `paperscraper-main/paperscraper/get_dumps/chemrxiv.py`

```python
"""Dump chemRxiv data in JSONL format."""

import logging
import os
import sys
from datetime import datetime
from typing import Optional

import pkg_resources

from .utils.chemrxiv import ChemrxivAPI, download_full, parse_dump

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

today = datetime.today().strftime("%Y-%m-%d")
save_folder = pkg_resources.resource_filename("paperscraper", "server_dumps")
save_path = os.path.join(save_folder, f"chemrxiv_{today}.jsonl")


def chemrxiv(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    save_path: str = save_path,
) -> None:
    """Fetches papers from bichemrxiv based on time range, i.e., start_date and end_date.
    If the start_date and end_date are not provided, papers will be fetched from chemrxiv
    from the launch date of chemrxiv until the current date. The fetched papers will be
    stored in jsonl format in save_path.

    Args:
        start_date (str, optional): begin date expressed as YYYY-MM-DD.
            Defaults to None, i.e., earliest possible.
        end_date (str, optional): end date expressed as YYYY-MM-DD.
            Defaults to None, i.e., today.
        save_path (str, optional): Path where the dump is stored.
            Defaults to save_path.
    """

    # create API client
    api = ChemrxivAPI(start_date, end_date)
    # Download the data
    download_full(save_folder, api)
    # Convert to JSONL format.
    parse_dump(save_folder, save_path)

```

##### File: `paperscraper-main/paperscraper/get_dumps/medrxiv.py`

```python
"""Dump medrxiv data in JSONL format."""

import json
import os
from datetime import datetime
from typing import Optional

import pkg_resources
from tqdm import tqdm

from ..xrxiv.xrxiv_api import MedRxivApi

today = datetime.today().strftime("%Y-%m-%d")
save_folder = pkg_resources.resource_filename("paperscraper", "server_dumps")
save_path = os.path.join(save_folder, f"medrxiv_{today}.jsonl")


def medrxiv(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    save_path: str = save_path,
    max_retries: int = 10,
):
    """Fetches papers from medrxiv based on time range, i.e., start_date and end_date.
    If the start_date and end_date are not provided, then papers will be fetched from
    medrxiv starting from the launch date of medrxiv until current date. The fetched
    papers will be stored in jsonl format in save_path.

    Args:
        start_date (str, optional): begin date expressed as YYYY-MM-DD.
            Defaults to None, i.e., earliest possible.
        end_date (str, optional): end date expressed as YYYY-MM-DD.
            Defaults to None, i.e., today.
        save_path (str, optional): Path where the dump is stored.
            Defaults to save_path.
        max_retries (int, optional): Number of retries when API shows connection issues.
            Defaults to 10.
    """
    # create API client
    api = MedRxivApi(max_retries=max_retries)
    # dump all papers
    with open(save_path, "w") as fp:
        for index, paper in enumerate(
            tqdm(api.get_papers(start_date=start_date, end_date=end_date))
        ):
            if index > 0:
                fp.write(os.linesep)
            fp.write(json.dumps(paper))

```

#### Directory: `paperscraper-main/paperscraper/get_dumps/utils`

##### File: `paperscraper-main/paperscraper/get_dumps/utils/__init__.py`

```python

```

#### Directory: `paperscraper-main/paperscraper/get_dumps/utils/chemrxiv`

##### File: `paperscraper-main/paperscraper/get_dumps/utils/chemrxiv/__init__.py`

```python
from .chemrxiv_api import ChemrxivAPI  # noqa
from .utils import *  # noqa

```

##### File: `paperscraper-main/paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py`

```python
import logging
import os
import sys
from datetime import datetime
from time import time
from typing import Dict, Optional
from urllib.parse import urljoin

import requests
from requests.exceptions import ChunkedEncodingError

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

now_datetime = datetime.now()
launch_dates = {"chemrxiv": "2017-01-01"}


class ChemrxivAPI:
    """Handle OpenEngage API requests, using access.
    Adapted from https://github.com/fxcoudert/tools/blob/master/chemRxiv/chemRxiv.py.
    """

    base = "https://chemrxiv.org/engage/chemrxiv/public-api/v1/"

    def __init__(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        page_size: Optional[int] = None,
        max_retries: int = 10,
    ):
        """
        Initialize API class.

        Args:
            start_date (Optional[str], optional): begin date expressed as YYYY-MM-DD.
                Defaults to None.
            end_date (Optional[str], optional): end date expressed as YYYY-MM-DD.
                Defaults to None.
            page_size (int, optional): The batch size used to fetch the records from chemrxiv.
            max_retries (int): Number of retries in case of error
        """

        self.page_size = page_size or 50
        self.max_retries = max_retries

        # Begin Date and End Date of the search
        launch_date = launch_dates["chemrxiv"]
        launch_datetime = datetime.fromisoformat(launch_date)

        if start_date:
            start_datetime = datetime.fromisoformat(start_date)
            if start_datetime < launch_datetime:
                self.start_date = launch_date
                logger.warning(
                    f"Begin date {start_date} is before chemrxiv launch date. Will use {launch_date} instead."
                )
            else:
                self.start_date = start_date
        else:
            self.start_date = launch_date
        if end_date:
            end_datetime = datetime.fromisoformat(end_date)
            if end_datetime > now_datetime:
                logger.warning(
                    f"End date {end_date} is in the future. Will use {now_datetime} instead."
                )
                self.end_date = now_datetime.strftime("%Y-%m-%d")
            else:
                self.end_date = end_date
        else:
            self.end_date = now_datetime.strftime("%Y-%m-%d")

    def request(self, url, method, params=None):
        """Send an API request to open Engage."""

        for attempt in range(self.max_retries):
            try:
                if method.casefold() == "get":
                    return requests.get(url, params=params, timeout=10)
                elif method.casefold() == "post":
                    return requests.post(url, json=params, timeout=10)
                else:
                    raise ConnectionError(f"Unknown method for query: {method}")
            except ChunkedEncodingError as e:
                logger.warning(f"ChunkedEncodingError occurred for {url}: {e}")
                if attempt + 1 == self.max_retries:
                    raise e
                time.sleep(3)

    def query(self, query, method="get", params=None):
        """Perform a direct query."""

        r = self.request(urljoin(self.base, query), method, params=params)
        r.raise_for_status()
        return r.json()

    def query_generator(self, query, method: str = "get", params: Dict = {}):
        """Query for a list of items, with paging. Returns a generator."""

        try:
            total = self.number_of_preprints()
        except Exception:
            total = float("inf")   # fallback if that call fails

        page = 0
        while True:
            params.update(
                {
                    "limit": self.page_size,
                    "skip": page * self.page_size,
                    "searchDateFrom": self.start_date,
                    "searchDateTo": self.end_date,
                }
            )
            if page * self.page_size > total:
                break
            r = self.request(urljoin(self.base, query), method, params=params)
            if r.status_code == 400:
                raise ValueError(r.json()["message"])
            r.raise_for_status()
            r = r.json()
            r = r["itemHits"]

            # If we have no more results, bail out
            if len(r) == 0:
                return

            yield from r
            page += 1

    def all_preprints(self):
        """Return a generator to all the chemRxiv articles."""
        return self.query_generator("items")

    def preprint(self, article_id):
        """Information on a given preprint.
        .. seealso:: https://docs.figshare.com/#public_article
        """
        return self.query(os.path.join("items", article_id))

    def number_of_preprints(self):
        return self.query("items")["totalCount"]

```

##### File: `paperscraper-main/paperscraper/get_dumps/utils/chemrxiv/utils.py`

```python
"""Misc utils to download chemRxiv dump"""

import json
import logging
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional

from requests.exceptions import SSLError
from requests.models import HTTPError
from tqdm import tqdm

from .chemrxiv_api import ChemrxivAPI

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

today = datetime.today().strftime("%Y-%m-%d")


def get_author(author_list: List[Dict]) -> str:
    """Parse ChemRxiv dump entry to extract author list

    Args:
        author_list (list): List of dicts, one per author.

    Returns:
        str: ;-concatenated author list.
    """

    return "; ".join([" ".join([a["firstName"], a["lastName"]]) for a in author_list])


def get_categories(category_list: List[Dict]) -> str:
    """Parse ChemRxiv dump entry to extract the categories of the paper

    Args:
        category_list (list): List of dicts, one per category.

    Returns:
        str: ;-concatenated category list.
    """

    return "; ".join([a["name"] for a in category_list])


def get_date(datestring: str) -> str:
    """Get the date of a chemrxiv dump enry.

    Args:
        datestring: String in the format: 2021-10-15T05:12:32.356Z

    Returns:
        str: Date in the format: YYYY-MM-DD.
    """
    return datestring.split("T")[0]


def get_metrics(metrics_list: List[Dict]) -> Dict:
    """
    Parse ChemRxiv dump entry to extract the access metrics of the paper.

    Args:
        metrics_list (List[Dict]): A list of single-keyed, dictionaries each
            containing key and value for exactly one metric.

    Returns:
        Dict: A flattened dictionary with all metrics and a timestamp
    """
    metric_dict = {m["description"]: m["value"] for m in metrics_list}

    # This assumes that the .jsonl is constructed at roughly the same date
    # where this entry was obtained from the API
    metric_dict.update({"timestamp": today})


def parse_dump(source_path: str, target_path: str) -> None:
    """
    Parses the dump as generated by the chemrXiv API and this repo:
    https://github.com/cthoyt/chemrxiv-summarize
    into a format that is equal to that of biorXiv and medRxiv.

    NOTE: This is a lazy parser trying to store all data in memory.

    Args:
        source_path: Path to the source dump
    """

    dump = []
    # Read source dump
    for file_name in tqdm(os.listdir(source_path)):
        if not file_name.endswith(".json"):
            continue
        filepath = os.path.join(source_path, file_name)
        with open(filepath, "r") as f:
            source_paper = json.load(f)

        target_paper = {
            "title": source_paper["title"],
            "doi": source_paper["doi"],
            "published_doi": (
                source_paper["vor"]["vorDoi"] if source_paper["vor"] else "N.A."
            ),
            "published_url": (
                source_paper["vor"]["url"] if source_paper["vor"] else "N.A."
            ),
            "authors": get_author(source_paper["authors"]),
            "abstract": source_paper["abstract"],
            "date": get_date(source_paper["statusDate"]),
            "journal": "chemRxiv",
            "categories": get_categories(source_paper["categories"]),
            "metrics": get_metrics(source_paper["metrics"]),
            "license": source_paper["license"]["name"],
        }
        dump.append(target_paper)
        os.remove(filepath)
    # Write dump
    with open(target_path, "w") as f:
        for idx, target_paper in enumerate(dump):
            if idx > 0:
                f.write(os.linesep)
            f.write(json.dumps(target_paper))
    logger.info("Done, shutting down")


def download_full(save_dir: str, api: Optional[ChemrxivAPI] = None) -> None:
    if api is None:
        api = ChemrxivAPI()

    os.makedirs(save_dir, exist_ok=True)
    for preprint in tqdm(api.all_preprints()):
        path = os.path.join(save_dir, f"{preprint['item']['id']}.json")
        if os.path.exists(path):
            continue
        preprint = preprint["item"]
        preprint_id = preprint["id"]
        try:
            preprint = api.preprint(preprint_id)
        except HTTPError:
            logger.warning(f"HTTP API Client error for ID: {preprint_id}")
        except SSLError:
            logger.warning(f"SSLError for ID: {preprint_id}")

        with open(path, "w") as file:
            json.dump(preprint, file, indent=2)

```

#### Directory: `paperscraper-main/paperscraper/pdf`

##### File: `paperscraper-main/paperscraper/pdf/__init__.py`

```python
from .pdf import load_api_keys, save_pdf, save_pdf_from_dump  # noqa

```

##### File: `paperscraper-main/paperscraper/pdf/fallbacks.py`

```python
"""Functionalities to scrape PDF files of publications."""

import calendar
import datetime
import io
import logging
import re
import sys
import time
import zipfile
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Callable, Dict, Union

import boto3
import requests
from botocore.client import BaseClient
from lxml import etree
from tqdm import tqdm

ELIFE_XML_INDEX = None  # global variable to cache the eLife XML index from GitHub

logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)


def fallback_wiley_api(
    paper_metadata: Dict[str, Any],
    output_path: Path,
    api_keys: Dict[str, str],
    max_attempts: int = 2,
) -> bool:
    """
    Attempt to download the PDF via the Wiley TDM API (popular publisher which blocks standard scraping attempts; API access free for academic users).

    This function uses the WILEY_TDM_API_TOKEN environment variable to authenticate
    with the Wiley TDM API and attempts to download the PDF for the given paper.
    See https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining for a description on how to get your WILEY_TDM_API_TOKEN.

    Args:
        paper_metadata (dict): Dictionary containing paper metadata. Must include the 'doi' key.
        output_path (Path): A pathlib.Path object representing the path where the PDF will be saved.
        api_keys (dict): Preloaded API keys.
        max_attempts (int): The maximum number of attempts to retry API call.

    Returns:
        bool: True if the PDF file was successfully downloaded, False otherwise.
    """

    WILEY_TDM_API_TOKEN = api_keys.get("WILEY_TDM_API_TOKEN")
    encoded_doi = paper_metadata["doi"].replace("/", "%2F")
    api_url = f"https://api.wiley.com/onlinelibrary/tdm/v1/articles/{encoded_doi}"
    headers = {"Wiley-TDM-Client-Token": WILEY_TDM_API_TOKEN}

    attempt = 0
    success = False

    while attempt < max_attempts:
        try:
            api_response = requests.get(
                api_url, headers=headers, allow_redirects=True, timeout=60
            )
            api_response.raise_for_status()
            if api_response.content[:4] != b"%PDF":
                logger.warning(
                    f"API returned content that is not a valid PDF for {paper_metadata['doi']}."
                )
            else:
                with open(output_path.with_suffix(".pdf"), "wb+") as f:
                    f.write(api_response.content)
                logger.info(
                    f"Successfully downloaded PDF via Wiley API for {paper_metadata['doi']}."
                )
                success = True
                break
        except Exception as e2:
            if attempt < max_attempts - 1:
                logger.info("Waiting 20 seconds before retrying...")
                time.sleep(20)
            logger.error(
                f"Could not download via Wiley API (attempt {attempt + 1}/{max_attempts}): {e2}"
            )

        attempt += 1

    # **Mandatory delay of 10 seconds to comply with Wiley API rate limits**
    logger.info(
        "Waiting 10 seconds before next request to comply with Wiley API rate limits..."
    )
    time.sleep(10)
    return success


def fallback_bioc_pmc(doi: str, output_path: Path) -> bool:
    """
    Attempt to download the XML via the BioC-PMC fallback.

    This function first converts a given DOI to a PMCID using the NCBI ID Converter API.
    If a PMCID is found, it constructs the corresponding PMC XML URL and attempts to
    download the full-text XML.

    PubMed Central® (PMC) is a free full-text archive of biomedical and life sciences
    journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM).

    Args:
        doi (str): The DOI of the paper to retrieve.
        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.

    Returns:
        bool: True if the XML file was successfully downloaded, False otherwise.
    """
    ncbi_tool = "paperscraper"
    ncbi_email = "your_email@example.com"

    converter_url = "https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/"
    params = {
        "tool": ncbi_tool,
        "email": ncbi_email,
        "ids": doi,
        "idtype": "doi",
        "format": "json",
    }
    try:
        conv_response = requests.get(converter_url, params=params, timeout=60)
        conv_response.raise_for_status()
        data = conv_response.json()
        records = data.get("records", [])
        if not records or "pmcid" not in records[0]:
            logger.warning(
                f"No PMCID available for DOI {doi}. Fallback via PMC therefore not possible."
            )
            return False
        pmcid = records[0]["pmcid"]
        logger.info(f"Converted DOI {doi} to PMCID {pmcid}.")
    except Exception as conv_err:
        logger.error(f"Error during DOI to PMCID conversion: {conv_err}")
        return False

    # Construct PMC XML URL
    xml_url = f"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_xml/{pmcid}/unicode"
    logger.info(f"Attempting to download XML from BioC-PMC URL: {xml_url}")
    try:
        xml_response = requests.get(xml_url, timeout=60)
        xml_response.raise_for_status()
        xml_path = output_path.with_suffix(".xml")
        # check for xml error:
        if xml_response.content.startswith(
            b"[Error] : No result can be found. <BR><HR><B> - https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/"
        ):
            logger.warning(f"No XML found for DOI {doi} at BioC-PMC URL {xml_url}.")
            return False
        with open(xml_path, "wb+") as f:
            f.write(xml_response.content)
        logger.info(f"Successfully downloaded XML for DOI {doi} to {xml_path}.")
        return True
    except Exception as xml_err:
        logger.error(f"Failed to download XML from BioC-PMC URL {xml_url}: {xml_err}")
        return False


def fallback_elsevier_api(
    paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str]
) -> bool:
    """
    Attempt to download the full text via the Elsevier TDM API.
    For more information, see:
    https://www.elsevier.com/about/policies-and-standards/text-and-data-mining
    (Requires an institutional subscription and an API key provided in the api_keys dictionary under the key "ELSEVIER_TDM_API_KEY".)

    Args:
        paper_metadata (Dict[str, Any]): Dictionary containing paper metadata. Must include the 'doi' key.
        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.
        api_keys (Dict[str, str]): A dictionary containing API keys. Must include the key "ELSEVIER_TDM_API_KEY".

    Returns:
        bool: True if the XML file was successfully downloaded, False otherwise.
    """
    elsevier_api_key = api_keys.get("ELSEVIER_TDM_API_KEY")
    doi = paper_metadata["doi"]
    api_url = f"https://api.elsevier.com/content/article/doi/{doi}?apiKey={elsevier_api_key}&httpAccept=text%2Fxml"
    logger.info(f"Attempting download via Elsevier API (XML) for {doi}: {api_url}")
    headers = {"Accept": "application/xml"}
    try:
        response = requests.get(api_url, headers=headers, timeout=60)

        # Check for 401 error and look for APIKEY_INVALID in the response
        if response.status_code == 401:
            error_text = response.text
            if "APIKEY_INVALID" in error_text:
                logger.error("Invalid API key. Couldn't download via Elsevier XML API")
            else:
                logger.error("401 Unauthorized. Couldn't download via Elsevier XML API")
            return False

        response.raise_for_status()

        # Attempt to parse it with lxml to confirm it's valid XML
        try:
            etree.fromstring(response.content)
        except etree.XMLSyntaxError as e:
            logger.warning(f"Elsevier API returned invalid XML for {doi}: {e}")
            return False

        xml_path = output_path.with_suffix(".xml")
        with open(xml_path, "wb") as f:
            f.write(response.content)
        logger.info(
            f"Successfully used Elsevier API to downloaded XML for {doi} to {xml_path}"
        )
        return True
    except Exception as e:
        logger.error(f"Could not download via Elsevier XML API: {e}")
        return False


def fallback_elife_xml(doi: str, output_path: Path) -> bool:
    """
    Attempt to download the XML via the eLife XML repository on GitHub.

    eLife provides open access to their XML files on GitHub, which can be used as a fallback.
    When multiple versions exist (revised papers), it takes the latest version (e.g., v3 instead of v1).

    Args:
        doi (str): The DOI of the eLife paper to download.
        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.

    Returns:
        bool: True if the XML file was successfully downloaded, False otherwise.
    """
    parts = doi.split("eLife.")
    if len(parts) < 2:
        logger.error(f"Unable to parse eLife DOI: {doi}")
        return False
    article_num = parts[1].strip()

    index = get_elife_xml_index()
    if article_num not in index:
        logger.warning(f"No eLife XML found for DOI {doi}.")
        return False
    candidate_files = index[article_num]
    latest_version, latest_download_url = max(candidate_files, key=lambda x: x[0])
    try:
        r = requests.get(latest_download_url, timeout=60)
        r.raise_for_status()
        latest_xml = r.content
    except Exception as e:
        logger.error(f"Error downloading file from {latest_download_url}: {e}")
        return False

    xml_path = output_path.with_suffix(".xml")
    with open(xml_path, "wb") as f:
        f.write(latest_xml)
    logger.info(
        f"Successfully downloaded XML via eLife API ({latest_version}) for DOI {doi} to {xml_path}."
    )
    return True


def get_elife_xml_index() -> dict:
    """
    Fetch the eLife XML index from GitHub and return it as a dictionary.

    This function retrieves and caches the list of available eLife articles in XML format
    from the eLife GitHub repository. It ensures that the latest version of each article
    is accessible for downloading. The index is cached in memory to avoid repeated
    network requests when processing multiple eLife papers.

    Returns:
        dict: A dictionary where keys are article numbers (as strings) and values are
              lists of tuples (version, download_url). Each list is sorted by version number.
    """
    global ELIFE_XML_INDEX
    if ELIFE_XML_INDEX is None:
        logger.info("Fetching eLife XML index from GitHub using git tree API")
        ELIFE_XML_INDEX = {}
        # Use the git tree API to get the full repository tree.
        base_tree_url = "https://api.github.com/repos/elifesciences/elife-article-xml/git/trees/master?recursive=1"
        r = requests.get(base_tree_url, timeout=60)
        r.raise_for_status()
        tree_data = r.json()
        items = tree_data.get("tree", [])
        # Look for files in the 'articles' directory matching the pattern.
        pattern = r"articles/elife-(\d+)-v(\d+)\.xml"
        for item in items:
            path = item.get("path", "")
            match = re.match(pattern, path)
            if match:
                article_num_padded = match.group(1)
                version = int(match.group(2))
                # Construct the raw download URL.
                download_url = f"https://raw.githubusercontent.com/elifesciences/elife-article-xml/master/{path}"
                ELIFE_XML_INDEX.setdefault(article_num_padded, []).append(
                    (version, download_url)
                )
        # Sort each article's file list by version.
        for key in ELIFE_XML_INDEX:
            ELIFE_XML_INDEX[key].sort(key=lambda x: x[0])
    return ELIFE_XML_INDEX


def month_folder(doi: str) -> str:
    """
    Query bioRxiv API to get the posting date of a given DOI.
    Convert a date to the BioRxiv S3 folder name, rolling over if it's the month's last day.
    E.g., if date is the last day of April, treat as May_YYYY.

    Args:
        doi: The DOI for which to retrieve the date.

    Returns:
        Month and year in format `October_2019`
    """
    url = f"https://api.biorxiv.org/details/biorxiv/{doi}/na/json"
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    date_str = resp.json()["collection"][0]["date"]
    date = datetime.date.fromisoformat(date_str)

    # NOTE: bioRxiv papers posted on the last day of the month are archived the next day
    last_day = calendar.monthrange(date.year, date.month)[1]
    if date.day == last_day:
        date = date + datetime.timedelta(days=1)
    return date.strftime("%B_%Y")


def list_meca_keys(s3_client: BaseClient, bucket: str, prefix: str) -> list:
    """
    List all .meca object keys under a given prefix in a requester-pays bucket.

    Args:
        s3_client: S3 client to get the data from.
        bucket: bucket to get data from.
        prefix: prefix to get data from.

    Returns:
        List of keys, one per existing .meca in the bucket.
    """
    keys = []
    paginator = s3_client.get_paginator("list_objects_v2")
    for page in paginator.paginate(
        Bucket=bucket, Prefix=prefix, RequestPayer="requester"
    ):
        for obj in page.get("Contents", []):
            if obj["Key"].endswith(".meca"):
                keys.append(obj["Key"])
    return keys


def find_meca_for_doi(
    s3_client: BaseClient, bucket: str, key: str, doi_token: str
) -> bool:
    """
    Efficiently inspect manifest.xml within a .meca zip by fetching only necessary bytes.
    Parse via ZipFile to read manifest.xml and match DOI token.

    Args:
        s3_client: S3 client to get the data from.
        bucket: bucket to get data from.
        key: prefix to get data from.
        doi_token: the DOI that should be matched

    Returns:
        Whether or not the DOI could be matched
    """
    try:
        head = s3_client.get_object(
            Bucket=bucket, Key=key, Range="bytes=0-4095", RequestPayer="requester"
        )["Body"].read()
        tail = s3_client.get_object(
            Bucket=bucket, Key=key, Range="bytes=-4096", RequestPayer="requester"
        )["Body"].read()
    except Exception:
        return False

    data = head + tail
    with zipfile.ZipFile(io.BytesIO(data)) as z:
        manifest = z.read("manifest.xml")

    # Extract the last part of the DOI (newer DOIs that contain date fail otherwise)
    doi_token = doi_token.split(".")[-1]
    return doi_token.encode("utf-8") in manifest.lower()


def fallback_s3(
    doi: str, output_path: Union[str, Path], api_keys: dict, workers: int = 32
) -> bool:
    """
    Download a BioRxiv PDF via the requester-pays S3 bucket using range requests.

    Args:
        doi: The DOI for which to retrieve the PDF (e.g. '10.1101/798496').
        output_path: Path where the PDF will be saved (with .pdf suffix added).
        api_keys: Dict containing 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.

    Returns:
        True if download succeeded, False otherwise.
    """

    s3 = boto3.client(
        "s3",
        aws_access_key_id=api_keys.get("AWS_ACCESS_KEY_ID"),
        aws_secret_access_key=api_keys.get("AWS_SECRET_ACCESS_KEY"),
        region_name="us-east-1",
    )
    bucket = "biorxiv-src-monthly"

    # Derive prefix from DOI date
    prefix = f"Current_Content/{month_folder(doi)}/"

    # List MECA archives in that month
    meca_keys = list_meca_keys(s3, bucket, prefix)
    if not meca_keys:
        return False

    token = doi.split("/")[-1].lower()
    target = None
    executor = ThreadPoolExecutor(max_workers=32)
    futures = {
        executor.submit(find_meca_for_doi, s3, bucket, key, token): key
        for key in meca_keys
    }
    target = None
    pbar = tqdm(
        total=len(futures),
        desc=f"Scanning in biorxiv with {workers} workers for {doi}…",
    )
    for future in as_completed(futures):
        key = futures[future]
        try:
            if future.result():
                target = key
                pbar.set_description(f"Success! Found target {doi} in {key}")
                # cancel pending futures to speed shutdown
                for fut in futures:
                    fut.cancel()
                break
        except Exception:
            pass
        finally:
            pbar.update(1)
    # shutdown without waiting for remaining threads
    executor.shutdown(wait=False)
    if target is None:
        logger.error(f"Could not find {doi} on biorxiv")
        return False

    # Download full MECA and extract PDF
    data = s3.get_object(Bucket=bucket, Key=target, RequestPayer="requester")[
        "Body"
    ].read()
    output_path = Path(output_path)
    with zipfile.ZipFile(io.BytesIO(data)) as z:
        for name in z.namelist():
            if name.lower().endswith(".pdf"):
                z.extract(name, path=output_path.parent)
                # Move file to desired location
                (output_path.parent / name).rename(output_path.with_suffix(".pdf"))
                return True
    return False


FALLBACKS: Dict[str, Callable] = {
    "bioc_pmc": fallback_bioc_pmc,
    "elife": fallback_elife_xml,
    "elsevier": fallback_elsevier_api,
    "s3": fallback_s3,
    "wiley": fallback_wiley_api,
}

```

##### File: `paperscraper-main/paperscraper/pdf/pdf.py`

```python
"""Functionalities to scrape PDF files of publications."""

import json
import logging
import os
import sys
from pathlib import Path
from typing import Any, Dict, Optional, Union

import requests
import tldextract
from bs4 import BeautifulSoup
from tqdm import tqdm

from ..utils import load_jsonl
from .fallbacks import FALLBACKS
from .utils import load_api_keys

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)

ABSTRACT_ATTRIBUTE = {
    "biorxiv": ["DC.Description"],
    "arxiv": ["citation_abstract"],
    "chemrxiv": ["citation_abstract"],
}
DEFAULT_ATTRIBUTES = ["citation_abstract", "description"]


def save_pdf(
    paper_metadata: Dict[str, Any],
    filepath: Union[str, Path],
    save_metadata: bool = False,
    api_keys: Optional[Union[str, Dict[str, str]]] = None,
) -> None:
    """
    Save a PDF file of a paper.

    Args:
        paper_metadata: A dictionary with the paper metadata. Must contain the `doi` key.
        filepath: Path to the PDF file to be saved (with or without suffix).
        save_metadata: A boolean indicating whether to save paper metadata as a separate json.
        api_keys: Either a dictionary containing API keys (if already loaded) or a string (path to API keys file).
                  If None, will try to load from `.env` file and if unsuccessful, skip API-based fallbacks.
    """
    if not isinstance(paper_metadata, Dict):
        raise TypeError(f"paper_metadata must be a dict, not {type(paper_metadata)}.")
    if "doi" not in paper_metadata.keys():
        raise KeyError("paper_metadata must contain the key 'doi'.")
    if not isinstance(filepath, str):
        raise TypeError(f"filepath must be a string, not {type(filepath)}.")

    output_path = Path(filepath)

    if not Path(output_path).parent.exists():
        raise ValueError(f"The folder: {output_path} seems to not exist.")

    # load API keys from file if not already loaded via in save_pdf_from_dump (dict)
    if not isinstance(api_keys, dict):
        api_keys = load_api_keys(api_keys)

    doi = paper_metadata["doi"]
    url = f"https://doi.org/{doi}"
    success = False
    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        success = True
    except Exception as e:
        error = str(e)
        logger.warning(f"Could not download from: {url} - {e}. ")

    if not success and "biorxiv" in error:
        if (
            api_keys.get("AWS_ACCESS_KEY_ID") is None
            or api_keys.get("AWS_SECRET_ACCESS_KEY") is None
        ):
            logger.info(
                "BiorXiv PDFs can be downloaded from a S3 bucket with a requester-pay option. "
                "Consider setting `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to use this option. "
                "Pricing is a few cent per GB, thus each request costs < 0.1 cents. "
                "For details see: https://www.biorxiv.org/tdm"
            )
        else:
            success = FALLBACKS["s3"](doi, output_path, api_keys)
            if success:
                return

    if not success:
        # always first try fallback to BioC-PMC (open access papers on PubMed Central)
        success = FALLBACKS["bioc_pmc"](doi, output_path)

        # if BioC-PMC fails, try other fallbacks
        if not success:
            # check for specific publishers
            if "elife" in error.lower():  # elife has an open XML repository on GitHub
                FALLBACKS["elife"](doi, output_path)
            elif (
                ("wiley" in error.lower())
                and api_keys
                and ("WILEY_TDM_API_TOKEN" in api_keys)
            ):
                FALLBACKS["wiley"](paper_metadata, output_path, api_keys)
        return

    soup = BeautifulSoup(response.text, features="lxml")
    meta_pdf = soup.find("meta", {"name": "citation_pdf_url"})
    if meta_pdf and meta_pdf.get("content"):
        pdf_url = meta_pdf.get("content")
        try:
            response = requests.get(pdf_url, timeout=60)
            response.raise_for_status()

            if response.content[:4] != b"%PDF":
                logger.warning(
                    f"The file from {url} does not appear to be a valid PDF."
                )
                success = FALLBACKS["bioc_pmc"](doi, output_path)
                if not success:
                    # Check for specific publishers
                    if "elife" in doi.lower():
                        logger.info("Attempting fallback to eLife XML repository")
                        FALLBACKS["elife"](doi, output_path)
                    elif api_keys and "WILEY_TDM_API_TOKEN" in api_keys:
                        FALLBACKS["wiley"](paper_metadata, output_path, api_keys)
                    elif api_keys and "ELSEVIER_TDM_API_KEY" in api_keys:
                        FALLBACKS["elsevier"](paper_metadata, output_path, api_keys)
            else:
                with open(output_path.with_suffix(".pdf"), "wb+") as f:
                    f.write(response.content)
        except Exception as e:
            logger.warning(f"Could not download {pdf_url}: {e}")
    else:  # if no citation_pdf_url meta tag found, try other fallbacks
        if "elife" in doi.lower():
            logger.info(
                "DOI contains eLife, attempting fallback to eLife XML repository on GitHub."
            )
            if not FALLBACKS["elife"](doi, output_path):
                logger.warning(
                    f"eLife XML fallback failed for {paper_metadata['doi']}."
                )
        elif (
            api_keys and "ELSEVIER_TDM_API_KEY" in api_keys
        ):  # elsevier journals can be accessed via the Elsevier TDM API (requires API key)
            FALLBACKS["elsevier"](paper_metadata, output_path, api_keys)
        else:
            logger.warning(
                f"Retrieval failed. No citation_pdf_url meta tag found for {url} and no applicable fallback mechanism available."
            )

    if not save_metadata:
        return

    metadata = {}
    # Extract title
    title_tag = soup.find("meta", {"name": "citation_title"})
    metadata["title"] = title_tag.get("content") if title_tag else "Title not found"

    # Extract authors
    authors = []
    for author_tag in soup.find_all("meta", {"name": "citation_author"}):
        if author_tag.get("content"):
            authors.append(author_tag["content"])
    metadata["authors"] = authors if authors else ["Author information not found"]

    # Extract abstract
    domain = tldextract.extract(url).domain
    abstract_keys = ABSTRACT_ATTRIBUTE.get(domain, DEFAULT_ATTRIBUTES)

    for key in abstract_keys:
        abstract_tag = soup.find("meta", {"name": key})
        if abstract_tag:
            raw_abstract = BeautifulSoup(
                abstract_tag.get("content", "None"), "html.parser"
            ).get_text(separator="\n")
            if raw_abstract.strip().startswith("Abstract"):
                raw_abstract = raw_abstract.strip()[8:]
            metadata["abstract"] = raw_abstract.strip()
            break

    if "abstract" not in metadata.keys():
        metadata["abstract"] = "Abstract not found"
        logger.warning(f"Could not find abstract for {url}")
    elif metadata["abstract"].endswith("..."):
        logger.warning(f"Abstract truncated from {url}")

    # Save metadata to JSON
    try:
        with open(output_path.with_suffix(".json"), "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"Failed to save metadata to {str(output_path)}: {e}")


def save_pdf_from_dump(
    dump_path: str,
    pdf_path: str,
    key_to_save: str = "doi",
    save_metadata: bool = False,
    api_keys: Optional[str] = None,
) -> None:
    """
    Receives a path to a `.jsonl` dump with paper metadata and saves the PDF files of
    each paper.

    Args:
        dump_path: Path to a `.jsonl` file with paper metadata, one paper per line.
        pdf_path: Path to a folder where the files will be stored.
        key_to_save: Key in the paper metadata to use as filename.
            Has to be `doi` or `title`. Defaults to `doi`.
        save_metadata: A boolean indicating whether to save paper metadata as a separate json.
        api_keys: Path to a file with API keys. If None, API-based fallbacks will be skipped.
    """

    if not isinstance(dump_path, str):
        raise TypeError(f"dump_path must be a string, not {type(dump_path)}.")
    if not dump_path.endswith(".jsonl"):
        raise ValueError("Please provide a dump_path with .jsonl extension.")

    if not isinstance(pdf_path, str):
        raise TypeError(f"pdf_path must be a string, not {type(pdf_path)}.")

    if not isinstance(key_to_save, str):
        raise TypeError(f"key_to_save must be a string, not {type(key_to_save)}.")
    if key_to_save not in ["doi", "title", "date"]:
        raise ValueError("key_to_save must be one of 'doi' or 'title'.")

    papers = load_jsonl(dump_path)

    if not isinstance(api_keys, dict):
        api_keys = load_api_keys(api_keys)

    pbar = tqdm(papers, total=len(papers), desc="Processing")
    for i, paper in enumerate(pbar):
        pbar.set_description(f"Processing paper {i + 1}/{len(papers)}")

        if "doi" not in paper.keys() or paper["doi"] is None:
            logger.warning(f"Skipping {paper['title']} since no DOI available.")
            continue
        filename = paper[key_to_save].replace("/", "_")
        pdf_file = Path(os.path.join(pdf_path, f"{filename}.pdf"))
        xml_file = pdf_file.with_suffix(".xml")
        if pdf_file.exists():
            logger.info(f"File {pdf_file} already exists. Skipping download.")
            continue
        if xml_file.exists():
            logger.info(f"File {xml_file} already exists. Skipping download.")
            continue
        output_path = str(pdf_file)
        save_pdf(paper, output_path, save_metadata=save_metadata, api_keys=api_keys)

```

##### File: `paperscraper-main/paperscraper/pdf/utils.py`

```python
import os
from typing import Dict, Optional

from dotenv import find_dotenv, load_dotenv


def load_api_keys(filepath: Optional[str] = None) -> Dict[str, str]:
    """
    Reads API keys from a file and returns them as a dictionary.
    The file should have each API key on a separate line in the format:
        KEY_NAME=API_KEY_VALUE

    Example:
        WILEY_TDM_API_TOKEN=your_wiley_token_here
        ELSEVIER_TDM_API_KEY=your_elsevier_key_here

    Args:
        filepath: Optional path to the file containing API keys.

    Returns:
        Dict[str, str]: A dictionary where keys are API key names and values are their respective API keys.
    """
    if filepath:
        load_dotenv(dotenv_path=filepath)
    else:
        load_dotenv(find_dotenv())

    return {
        "WILEY_TDM_API_TOKEN": os.getenv("WILEY_TDM_API_TOKEN"),
        "ELSEVIER_TDM_API_KEY": os.getenv("ELSEVIER_TDM_API_KEY"),
        "AWS_ACCESS_KEY_ID": os.getenv("AWS_ACCESS_KEY_ID"),
        "AWS_SECRET_ACCESS_KEY": os.getenv("AWS_SECRET_ACCESS_KEY"),
    }

```

#### Directory: `paperscraper-main/paperscraper/pubmed`

##### File: `paperscraper-main/paperscraper/pubmed/__init__.py`

```python
from .pubmed import *  # noqa

```

##### File: `paperscraper-main/paperscraper/pubmed/pubmed.py`

```python
import datetime
import logging
from typing import List, Union

import pandas as pd
from pymed_paperscraper import PubMed

from ..utils import dump_papers
from .utils import get_emails, get_query_from_keywords_and_date

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

PUBMED = PubMed(tool="MyTool", email="abc@def.gh")

pubmed_field_mapper = {"publication_date": "date"}

# Authors fields needs specific processing
process_fields = {
    "authors": lambda authors: list(
        map(
            lambda a: str(a.get("firstname", "")) + "" + str(a.get("lastname", "")),
            authors,
        )
    ),
    "date": lambda date: (
        date.strftime("%Y-%m-%d") if isinstance(date, datetime.date) else date
    ),
    "doi": lambda doi: doi.split("\n")[0] if isinstance(doi, str) else doi,
}


def get_pubmed_papers(
    query: str,
    fields: List = ["title", "authors", "date", "abstract", "journal", "doi"],
    max_results: int = 9998,
    *args,
    **kwargs,
) -> pd.DataFrame:
    """
    Performs PubMed API request of a query and returns list of papers with
    fields as desired.

    Args:
        query: Query to PubMed API. Needs to match PubMed API notation.
        fields: List of strings with fields to keep in output.
            NOTE: If 'emails' is passed, an attempt is made to extract author mail
            addresses.
        max_results: Maximal number of results retrieved from DB. Defaults
            to 9998, higher values likely raise problems due to PubMedAPI, see:
            https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit
        args: additional arguments for pubmed.query
        kwargs: additional arguments for pubmed.query

    Returns:
        pd.DataFrame. One paper per row.

    """
    if max_results > 9998:
        logger.warning(
            f"\nmax_results cannot be larger than 9998, received {max_results}."
            "This will likely result in a JSONDecodeError. Considering lowering `max_results`.\n"
            "For PubMed, ESearch can only retrieve the first 9,999 records matching the query. "
            "To obtain more than 9,999 PubMed records, consider using EDirect that contains additional"
            "logic to batch PubMed search results automatically so that an arbitrary number can be retrieved"
        )
    raw = list(PUBMED.query(query, max_results=max_results, *args, **kwargs))

    get_mails = "emails" in fields
    if get_mails:
        fields.pop(fields.index("emails"))

    processed = [
        {
            pubmed_field_mapper.get(key, key): process_fields.get(
                pubmed_field_mapper.get(key, key), lambda x: x
            )(value)
            for key, value in paper.toDict().items()
            if pubmed_field_mapper.get(key, key) in fields
        }
        for paper in raw
    ]
    if get_mails:
        for idx, paper in enumerate(raw):
            processed[idx].update({"emails": get_emails(paper)})

    return pd.DataFrame(processed)


def get_and_dump_pubmed_papers(
    keywords: List[Union[str, List[str]]],
    output_filepath: str,
    fields: List = ["title", "authors", "date", "abstract", "journal", "doi"],
    start_date: str = "None",
    end_date: str = "None",
    *args,
    **kwargs,
) -> None:
    """
    Combines get_pubmed_papers and dump_papers.

    Args:
        keywords: List of keywords to request pubmed API.
            The outer list level will be considered as AND separated keys.
            The inner level as OR separated.
        output_filepath: Path where the dump will be saved.
        fields: List of strings with fields to keep in output.
            Defaults to ['title', 'authors', 'date', 'abstract',
            'journal', 'doi'].
            NOTE: If 'emails' is passed, an attempt is made to extract author mail
            addresses.
        start_date: Start date for the search. Needs to be in format:
            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific
            dates are used.
        end_date: End date for the search. Same notation as start_date.
    """
    # Translate keywords into query.
    query = get_query_from_keywords_and_date(
        keywords, start_date=start_date, end_date=end_date
    )
    papers = get_pubmed_papers(query, fields, *args, **kwargs)
    dump_papers(papers, output_filepath)

```

##### File: `paperscraper-main/paperscraper/pubmed/utils.py`

```python
import warnings
from typing import List, Union

from pymed_paperscraper.article import PubMedArticle

finalize_disjunction = lambda x: "(" + x[:-4] + ") AND "
finalize_conjunction = lambda x: x[:-5]
date_root = '("{0}"[Date - Create] : "{1}"[Date - Create])'


def get_query_from_keywords(keywords: List[Union[str, List]]) -> str:
    """Receives a list of keywords and returns the query for the pubmed API.

    Args:
        keywords (List[str, List[str]]): Items will be AND separated. If items
            are lists themselves, they will be OR separated.

    Returns:
        str: query to enter to pubmed API.
    """

    query = ""
    for i, key in enumerate(keywords):
        if isinstance(key, str):
            query += f"({key}) AND "
        elif isinstance(key, list):
            inter = "".join([f"({syn}) OR " for syn in key])
            query += finalize_disjunction(inter)

    query = finalize_conjunction(query)
    return query


def get_query_from_keywords_and_date(
    keywords: List[Union[str, List]], start_date: str = "None", end_date: str = "None"
) -> str:
    """Receives a list of keywords and returns the query for the pubmed API.

    Args:
        keywords (List[str, List[str]]): Items will be AND separated. If items
            are lists themselves, they will be OR separated.
        start_date (str): Start date for the search. Needs to be in format:
            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific
            dates are used.
        end_date (str): End date for the search. Same notation as start_date.

    Note: If start_date and end_date are left as default, the function is
        identical to get_query_from_keywords.

    Returns:
        str: query to enter to pubmed API.
    """

    query = get_query_from_keywords(keywords)

    if start_date != "None" and end_date != "None":
        date = date_root.format(start_date, end_date)
    elif start_date != "None" and end_date == "None":
        date = date_root.format(start_date, "3000")
    elif start_date == "None" and end_date != "None":
        date = date_root.format("1000", end_date)
    else:
        return query

    return query + " AND " + date


def get_emails(paper: PubMedArticle) -> List:
    """
    Extracts author email addresses from PubMedArticle.

    Args:
        paper (PubMedArticle): An object of type PubMedArticle. Requires to have
            an 'author' field.

    Returns:
        List: A possibly empty list of emails associated to authors of the paper.
    """

    emails = []
    for author in paper.authors:
        for v in author.values():
            if v is not None and "@" in v:
                parts = v.split("@")
                if len(parts) == 2:
                    # Found one email address
                    prefix = parts[0].split(" ")[-1]
                    postfix = parts[1]
                    mail = prefix + "@" + postfix
                    if not (postfix.endswith(".") or postfix.endswith(" ")):
                        emails.append(mail)
                    else:
                        emails.append(mail[:-1])
                else:
                    # Found multiple addresses
                    for idx, part in enumerate(parts):
                        try:
                            if idx == 0:
                                prefix = part.split(" ")[-1]
                            else:
                                postfix = part.split("\n")[0]

                                if postfix.endswith("."):
                                    postfix = postfix[:-1]
                                    mail = prefix + "@" + postfix
                                else:
                                    current_postfix = postfix.split(" ")[0]
                                    mail = prefix + "@" + current_postfix
                                    prefix = postfix.split(" ")[1]
                                emails.append(mail)
                        except IndexError:
                            warnings.warn(f"Mail could not be inferred from {part}.")

    return list(set(emails))

```

#### Directory: `paperscraper-main/paperscraper/pubmed/tests`

##### File: `paperscraper-main/paperscraper/pubmed/tests/__init__.py`

```python

```

##### File: `paperscraper-main/paperscraper/pubmed/tests/test_pubmed.py`

```python
import os
import tempfile

from paperscraper.pubmed import get_and_dump_pubmed_papers, get_pubmed_papers
from paperscraper.pubmed.utils import get_query_from_keywords_and_date

KEYWORDS = [["machine learning", "deep learning"], ["zoology"]]


class TestPubMed:
    def test_get_and_dump_pubmed(self):
        with tempfile.TemporaryDirectory() as temp_dir:
            output_filepath = os.path.join(temp_dir, "tmp.jsonl")
            get_and_dump_pubmed_papers(KEYWORDS, output_filepath=output_filepath)
            assert os.path.exists(output_filepath), "File was not created"

    def test_email(self):
        query = get_query_from_keywords_and_date(KEYWORDS, start_date="2020/07/20")
        df = get_pubmed_papers(query, fields=["emails", "title", "authors"])
        assert "emails" in df.columns

        query = get_query_from_keywords_and_date(KEYWORDS, end_date="2020/07/20")
        df = get_pubmed_papers(query, fields=["emails", "title", "authors"])
        assert "emails" in df.columns

        query = get_query_from_keywords_and_date(
            KEYWORDS, start_date="2020/07/10", end_date="2020/07/20"
        )
        df = get_pubmed_papers(query, fields=["emails", "title", "authors"])
        assert "emails" in df.columns

    def test_doi(self):
        query = "CRISPR-based gene editing in plants: Focus on reagents and their delivery tools"
        df = get_pubmed_papers(query, fields=["doi", "title", "authors"])
        for i, r in df.iterrows():
            assert "\n" not in r.doi

```

#### Directory: `paperscraper-main/paperscraper/scholar`

##### File: `paperscraper-main/paperscraper/scholar/__init__.py`

```python
from .scholar import *  # noqa

```

##### File: `paperscraper-main/paperscraper/scholar/core.py`

```python

```

##### File: `paperscraper-main/paperscraper/scholar/scholar.py`

```python
import logging
import sys
from typing import List

import pandas as pd
from scholarly import scholarly

from ..citations.citations import get_citations_from_title  # noqa
from ..utils import dump_papers

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)


scholar_field_mapper = {
    "venue": "journal",
    "author": "authors",
    "cites": "citations",
    "pub_year": "year",
}
process_fields = {"year": lambda x: int(x) if x.isdigit() else -1, "citations": int}


def get_scholar_papers(
    title: str,
    fields: List = ["title", "authors", "year", "abstract", "journal", "citations"],
    *args,
    **kwargs,
) -> pd.DataFrame:
    """
    Performs Google Scholar API request of a given title and returns list of papers with
    fields as desired.

    Args:
        title: Query to arxiv API. Needs to match the arxiv API notation.
        fields: List of strings with fields to keep in output.

    Returns:
        pd.DataFrame. One paper per row.

    """
    logger.info(
        "NOTE: Scholar API cannot be used with Boolean logic in keywords."
        "Query should be a single string to be entered in the Scholar search field."
    )
    if not isinstance(title, str):
        raise TypeError(f"Pass str not {type(title)}")

    matches = scholarly.search_pubs(title)

    processed = []
    for paper in matches:
        # Extracts title, author, year, journal, abstract
        entry = {
            scholar_field_mapper.get(key, key): process_fields.get(
                scholar_field_mapper.get(key, key), lambda x: x
            )(value)
            for key, value in paper["bib"].items()
            if scholar_field_mapper.get(key, key) in fields
        }

        entry["citations"] = paper["num_citations"]
        processed.append(entry)

    return pd.DataFrame(processed)


def get_and_dump_scholar_papers(
    title: str,
    output_filepath: str,
    fields: List = ["title", "authors", "year", "abstract", "journal", "citations"],
) -> None:
    """
    Combines get_scholar_papers and dump_papers.

    Args:
        title: Paper to search for on Google Scholar.
        output_filepath: Path where the dump will be saved.
        fields: List of strings with fields to keep in output.
    """
    papers = get_scholar_papers(title, fields)
    dump_papers(papers, output_filepath)

```

#### Directory: `paperscraper-main/paperscraper/scholar/tests`

##### File: `paperscraper-main/paperscraper/scholar/tests/__init__.py`

```python

```

##### File: `paperscraper-main/paperscraper/scholar/tests/test_scholar.py`

```python
import functools
import logging

import pandas as pd
import pytest
from scholarly._proxy_generator import MaxTriesExceededException

from paperscraper.citations import get_citations_from_title
from paperscraper.scholar import get_and_dump_scholar_papers, get_scholar_papers

logging.disable(logging.INFO)


def handle_scholar_exception(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except MaxTriesExceededException as e:
            logging.info(f"MaxTriesExceededException caught: {e}")
            pytest.skip("Skipping test due to MaxTriesExceededException")

    return wrapper


class TestScholar:
    @handle_scholar_exception
    def test_citations(self):
        num = get_citations_from_title("GT4SD")
        assert isinstance(num, int) and num > 0

    @handle_scholar_exception
    def test_dump_search(self, tmpdir):
        temp_dir = tmpdir.mkdir("scholar_papers")
        output_filepath = temp_dir.join("results.jsonl")
        get_and_dump_scholar_papers("GT4SD", str(output_filepath))
        assert output_filepath.check(file=1)

    @handle_scholar_exception
    def test_basic_search(self):
        results = get_scholar_papers("GT4SD")
        assert len(results) > 0 and isinstance(results, pd.DataFrame)
        assert all(
            [
                x in results.columns
                for x in [
                    "title",
                    "abstract",
                    "citations",
                    "year",
                    "authors",
                    "journal",
                ]
            ]
        )

    @handle_scholar_exception
    def test_bad_search(self):
        results = get_scholar_papers("GT4SDfsdhfiobfpsdfbsdp")
        assert len(results) == 0

```

#### Directory: `paperscraper-main/paperscraper/server_dumps`

##### File: `paperscraper-main/paperscraper/server_dumps/__init__.py`

```python
"""
Folder for the metadata dumps from biorxiv, medrxiv and chemrxiv API.
No code here but will be populated with your local `.jsonl` files.
"""

```

#### Directory: `paperscraper-main/paperscraper/tests`

##### File: `paperscraper-main/paperscraper/tests/__init__.py`

```python

```

##### File: `paperscraper-main/paperscraper/tests/test_dump.py`

```python
import importlib
import logging
import multiprocessing
import os
import time
from datetime import datetime, timedelta
from functools import partial

import pytest

import paperscraper.load_dumps as load_dumps_module
from paperscraper import dump_queries
from paperscraper.arxiv import get_and_dump_arxiv_papers
from paperscraper.get_dumps import arxiv, biorxiv, chemrxiv, medrxiv
from paperscraper.load_dumps import QUERY_FN_DICT

logging.disable(logging.INFO)

covid19 = ["COVID-19", "SARS-CoV-2"]
ai = ["Artificial intelligence", "Deep learning", "Machine learning"]
mi = ["Medical imaging"]


def target_func(queue, func):
    try:
        func()
        queue.put(True)  # Function completed (this should never happen)
    except Exception as e:
        queue.put(e)


class TestDumper:
    def test_dump_existence_initial(self):
        # This test checks the initial state, should be run first if order matters
        assert len(QUERY_FN_DICT) == 2, "Initial length of QUERY_FN_DICT should be 2"

    @pytest.fixture
    def setup_medrxiv(self):
        return partial(medrxiv, max_retries=2)

    @pytest.fixture
    def setup_biorxiv(self):
        return partial(biorxiv, max_retries=2)

    @pytest.fixture
    def setup_chemrxiv(self):
        return chemrxiv

    @pytest.fixture
    def setup_arxiv(self):
        return arxiv

    def run_function_with_timeout(self, func, timeout):
        queue = multiprocessing.Queue()
        process = multiprocessing.Process(target=target_func, args=(queue, func))
        process.start()
        time.sleep(timeout)

        was_alive = process.is_alive()
        process.terminate()
        process.join()

        if not was_alive and not queue.empty():
            raise queue.get()
        elif not was_alive:
            return False
        else:
            return True

    @pytest.mark.timeout(30)
    def test_medrxiv(self, setup_medrxiv):
        # Check that the function runs for at least 15 seconds
        assert self.run_function_with_timeout(setup_medrxiv, 15), (
            "medrxiv should still be running after 15 seconds"
        )

    @pytest.mark.timeout(30)
    def test_biorxiv(self, setup_biorxiv):
        # Check that the function runs for at least 15 seconds
        assert self.run_function_with_timeout(setup_biorxiv, 15), (
            "biorxiv should still be running after 15 seconds"
        )

    @pytest.mark.timeout(30)
    def test_chemrxiv(self, setup_chemrxiv):
        # Check that the function runs for at least 15 seconds
        assert self.run_function_with_timeout(setup_chemrxiv, 15), (
            "chemrxiv should still be running after 15 seconds"
        )

    @pytest.mark.timeout(30)
    def test_arxiv(self, setup_arxiv):
        # Check that the function runs for at least 15 seconds
        assert self.run_function_with_timeout(setup_arxiv, 15), (
            "arxiv should still be running after 90 seconds"
        )

    def test_chemrxiv_date(self):
        chemrxiv(start_date="2024-06-01", end_date="2024-06-01")

    def test_biorxiv_date(self):
        biorxiv(start_date="2024-06-01", end_date="2024-06-01")

    def test_arxiv_date(self):
        # Result of this may be empty because arxiv updates not daily.
        # With days=4 it should never be empty.
        arxiv(start_date=(datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d"))

        arxiv(end_date="1991-01-01")
        arxiv(start_date="1993-04-03", end_date="1993-04-03")

    def test_arxiv_wrong_date(self):
        with pytest.raises(
            ValueError, match=r"start_date .* cannot be later than end_date .*"
        ):
            arxiv(start_date="2024-06-02", end_date="2024-06-01")

    def test_dumping(self):
        queries = [[covid19, ai, mi]]
        dump_queries(queries, "tmpdir")
        assert os.path.exists("tmpdir/pubmed")

    def test_arxiv_dumping(self):
        query = [covid19, ai, mi]
        get_and_dump_arxiv_papers(query, output_filepath="covid19_ai_imaging.jsonl")
        assert os.path.exists("covid19_ai_imaging.jsonl")

    def test_get_arxiv_date(self):
        get_and_dump_arxiv_papers(
            [["MPEGO"]],
            output_filepath="mpego.jsonl",
            start_date="2020-06-01",
            end_date="2024-06-02",
            backend="api",
        )
        get_and_dump_arxiv_papers(
            [["PaccMann"]],
            output_filepath="paccmann.jsonl",
            end_date="2023-06-02",
            backend="infer",
        )
        get_and_dump_arxiv_papers(
            [["QontOT"]],
            output_filepath="qontot.jsonl",
            start_date="2023-01-02",
            backend="local",
        )

    def test_dump_existence(self):
        importlib.reload(load_dumps_module)
        from paperscraper.load_dumps import QUERY_FN_DICT

        assert len(QUERY_FN_DICT) == 5, (
            f"Expected QUERY_FN_DICT to also contain med/bio/chemrxiv, {QUERY_FN_DICT}"
        )

```

##### File: `paperscraper-main/paperscraper/tests/test_impactor.py`

```python
import logging

import pytest

from paperscraper.impact import Impactor

logging.disable(logging.INFO)


class TestImpactor:
    @pytest.fixture
    def impactor(self):
        return Impactor()

    def test_basic_search(self, impactor: Impactor):
        results = impactor.search("Nat Comm", threshold=99, sort_by="score")
        assert len(results) > 0  # Ensure we get some results
        assert all(
            "journal" in r and "factor" in r and "score" in r for r in results
        )  # Basic fields are present

    def test_fuzzy_search(self, impactor: Impactor):
        results = impactor.search("Nat Comm", threshold=99)
        assert any(
            r["journal"] == "Nature Communications" for r in results
        )  # Check for a specific journal

    def test_sort_by_score(self, impactor: Impactor):
        results = impactor.search("nature chem", threshold=80, sort_by="score")
        scores = [r["score"] for r in results]
        assert scores == sorted(
            scores, reverse=True
        )  # Ensure results are sorted by score

    def test_impact_factor_filtering(self, impactor: Impactor):
        results = impactor.search("Quantum information", threshold=70, min_impact=8)
        assert all(
            8 <= r["factor"] for r in results
        )  # Check if all results have a factor >= 8

    def test_return_all_fields(self, impactor: Impactor):
        results = impactor.search("nature chem", return_all=True)
        for sorting in ["impact", "journal", "score"]:
            assert all(
                len(r) > 3 for r in results
            )  # Check if more than the basic fields are returned

    def test_quantum_information_search(self, impactor):
        expected_results = [
            {"journal": "Innovation", "factor": 33.2, "score": 70},
            {"journal": "InfoMat", "factor": 22.7, "score": 71},
            {"journal": "Information Fusion", "factor": 14.7, "score": 71},
            {"journal": "PRX Quantum", "factor": 9.3, "score": 78},
        ]

        results = impactor.search(
            "Quantum information", threshold=70, sort_by="factor", min_impact=8
        )

        # Ensure that the results match the expected results
        assert len(results) == len(expected_results), "Number of results does not match"
        for expected, actual in zip(expected_results, results):
            assert (
                expected["journal"] == actual["journal"]
            ), f"Journal name does not match for {expected['journal']}"
            assert (
                abs(expected["factor"] - actual["factor"]) < 0.001
            ), f"Impact factor does not match for {expected['journal']}"
            assert (
                expected["score"] == actual["score"]
            ), f"Score does not match for {expected['journal']}"

    def test_type_error(self, impactor: Impactor):
        with pytest.raises(TypeError):
            impactor.search(123, threshold=99)  # query is not a str
        with pytest.raises(TypeError):
            impactor.search("Nature", threshold="99")  # threshold is not an int

    def test_value_error(self, impactor: Impactor):
        with pytest.raises(ValueError):
            impactor.search("Nature", threshold=-1)

    def test_nlm_id(self, impactor: Impactor):
        results = impactor.search("101528555", return_all=True)
        assert len(results) > 0

```

##### File: `paperscraper-main/paperscraper/tests/test_pdf.py`

```python
import logging
import os
import shutil
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from paperscraper.pdf import load_api_keys, save_pdf, save_pdf_from_dump
from paperscraper.pdf.fallbacks import FALLBACKS

logging.disable(logging.INFO)


TEST_FILE_PATH = str(Path(__file__).parent / "test_dump.jsonl")
SAVE_PATH = "tmp_pdf_storage"


class TestPDF:
    @pytest.fixture
    def paper_data(self):
        return {"doi": "10.48550/arXiv.2207.03928"}

    def test_basic_search(self):
        paper_data = {"doi": "10.48550/arXiv.2207.03928"}
        save_pdf(paper_data, filepath="gt4sd.pdf", save_metadata=True)
        assert os.path.exists("gt4sd.pdf")
        assert os.path.exists("gt4sd.json")
        os.remove("gt4sd.pdf")
        os.remove("gt4sd.json")

        # chemrxiv
        paper_data = {"doi": "10.26434/chemrxiv-2021-np7xj-v4"}
        save_pdf(paper_data, filepath="kinases.pdf", save_metadata=True)
        assert os.path.exists("kinases.pdf")
        assert os.path.exists("kinases.json")
        os.remove("kinases.pdf")
        os.remove("kinases.json")

        # biorxiv
        if os.path.exists("taskload.pdf"):
            os.remove("taskload.pdf")
        paper_data = {"doi": "10.1101/798496"}
        # NOTE: biorxiv is cloudflare controlled so standard scraping fails

        # Now try with S3 routine
        keys = load_api_keys("api_keys.txt")
        save_pdf(
            {"doi": "10.1101/786871"},
            filepath="taskload.pdf",
            save_metadata=False,
            api_keys=keys,
        )
        assert os.path.exists("taskload.pdf")
        os.remove("taskload.pdf")

        # Test S3 fallback explicitly
        FALLBACKS["s3"](doi="10.1101/786871", output_path="taskload.pdf", api_keys=keys)
        assert os.path.exists("taskload.pdf")
        os.remove("taskload.pdf")

        # Test S3 fallback with newer DOIs (including year/month/day)
        FALLBACKS["s3"](doi="10.1101/2023.10.09.561414", output_path="taskload.pdf", api_keys=keys)
        assert os.path.exists("taskload.pdf")
        os.remove("taskload.pdf")

        # medrxiv now also seems cloudflare-controlled. skipping test
        # paper_data = {"doi": "10.1101/2020.09.02.20187096"}
        # save_pdf(paper_data, filepath="covid_review.pdf", save_metadata=True)
        # assert os.path.exists("covid_review.pdf")
        # assert os.path.exists("covid_review.json")
        # os.remove("covid_review.pdf")
        # os.remove("covid_review.json")

        # journal with OA paper
        paper_data = {"doi": "10.1038/s42256-023-00639-z"}
        save_pdf(paper_data, filepath="regression_transformer", save_metadata=True)
        assert os.path.exists("regression_transformer.pdf")
        assert os.path.exists("regression_transformer.json")
        os.remove("regression_transformer.pdf")
        os.remove("regression_transformer.json")

        # book chapter with paywall
        paper_data = {"doi": "10.1007/978-981-97-4828-0_7"}
        save_pdf(paper_data, filepath="clm_chapter", save_metadata=True)
        assert not os.path.exists("clm_chapter.pdf")
        assert os.path.exists("clm_chapter.json")
        os.remove("clm_chapter.json")

        # journal without OA paper
        paper_data = {"doi": "10.1126/science.adk9587"}
        save_pdf(paper_data, filepath="color", save_metadata=True)
        assert not os.path.exists("color.pdf")
        assert not os.path.exists("color.json")

    def test_missing_doi(self):
        with pytest.raises(KeyError):
            paper_data = {"title": "Sample Paper"}
            save_pdf(paper_data, "sample_paper.pdf")

    def test_invalid_metadata_type(self):
        with pytest.raises(TypeError):
            save_pdf(paper_metadata="not_a_dict", filepath="output.pdf")

    def test_missing_doi_key(self):
        with pytest.raises(KeyError):
            save_pdf(paper_metadata={}, filepath="output.pdf")

    def test_invalid_filepath_type(self):
        with pytest.raises(TypeError):
            save_pdf(paper_metadata=self.paper_data, filepath=123)

    def test_incorrect_filepath_extension(self):
        with pytest.raises(TypeError):
            save_pdf(paper_metadata=self.paper_data, filepath="output.txt")

    def test_incorrect_filepath_type(self):
        with pytest.raises(TypeError):
            save_pdf(paper_metadata=list(self.paper_data), filepath="output.txt")

    def test_nonexistent_directory_in_filepath(self, paper_data):
        with pytest.raises(ValueError):
            save_pdf(paper_metadata=paper_data, filepath="/nonexistent/output.pdf")

    @patch("requests.get")
    def test_network_issues_on_doi_url_request(self, mock_get, paper_data):
        mock_get.side_effect = Exception("Network error")
        save_pdf(paper_metadata=paper_data, filepath="output.pdf")
        assert not os.path.exists("output.pdf")

    @patch("requests.get")
    def test_missing_pdf_url_in_meta_tags(self, mock_get, paper_data):
        response = MagicMock()
        response.text = "<html></html>"
        mock_get.return_value = response
        save_pdf(paper_metadata=paper_data, filepath="output.pdf")
        assert not os.path.exists("output.pdf")

    @patch("requests.get")
    def test_network_issues_on_pdf_url_request(self, mock_get, paper_data):
        response_doi = MagicMock()
        response_doi.text = (
            '<meta name="citation_pdf_url" content="http://valid.url/document.pdf">'
        )
        mock_get.side_effect = [response_doi, Exception("Network error")]
        save_pdf(paper_metadata=paper_data, filepath="output.pdf")
        assert not os.path.exists("output.pdf")

    def test_save_pdf_from_dump_wrong_type(self):
        with pytest.raises(TypeError):
            save_pdf_from_dump(-1, pdf_path=SAVE_PATH, key_to_save="doi")

    def test_save_pdf_from_dump_wrong_output_type(self):
        with pytest.raises(TypeError):
            save_pdf_from_dump(TEST_FILE_PATH, pdf_path=1, key_to_save="doi")

    def test_save_pdf_from_dump_wrong_suffix(self):
        with pytest.raises(ValueError):
            save_pdf_from_dump(
                TEST_FILE_PATH.replace("jsonl", "json"),
                pdf_path=SAVE_PATH,
                key_to_save="doi",
            )

    def test_save_pdf_from_dump_wrong_key(self):
        with pytest.raises(ValueError):
            save_pdf_from_dump(TEST_FILE_PATH, pdf_path=SAVE_PATH, key_to_save="doix")

    def test_save_pdf_from_dump_wrong_key_type(self):
        with pytest.raises(TypeError):
            save_pdf_from_dump(TEST_FILE_PATH, pdf_path=SAVE_PATH, key_to_save=["doix"])

    def test_save_pdf_from_dump(self):
        os.makedirs(SAVE_PATH, exist_ok=True)
        save_pdf_from_dump(TEST_FILE_PATH, pdf_path=SAVE_PATH, key_to_save="doi")
        shutil.rmtree(SAVE_PATH)

    def test_api_keys_none_pmc(self):
        """Test that save_pdf works properly even when no API keys are provided. Paper in PMC."""
        test_doi = {"doi": "10.1038/s41587-022-01613-7"}  # DOI known to be in PMC
        filename = SAVE_PATH + "_pmc"
        # Call function with no API keys
        save_pdf(test_doi, filepath=filename, api_keys=None)

        # Verify file was created - with .xml extension from PMC fallback
        assert os.path.exists(filename + ".xml"), (
            "XML file was not created via PMC fallback"
        )
        os.remove(filename + ".xml")

    def test_api_keys_none_oa(self):
        """Test that save_pdf works properly even when no API keys are provided. Paper available open-access."""
        test_doi = {"doi": "10.1038/s42256-023-00639-z"}  # DOI known to be OA
        filename = SAVE_PATH + "_oa"
        # Call function with no API keys
        save_pdf(test_doi, filepath=filename, api_keys=None)

        # Verify file was created - with .pdf extension for direct PDF download
        assert os.path.exists(filename + ".pdf"), (
            "PDF file was not created for OA content"
        )
        os.remove(filename + ".pdf")

    def test_api_key_file(self):
        test_doi = {"doi": "10.1002/smll.202309431"}  # Use a DOI from Wiley
        with open("tmp_keyfile.txt", "w") as f:
            f.write("WILEY_TDM_API_TOKEN=INVALID_TEST_KEY_123")
        save_pdf(test_doi, filepath=SAVE_PATH, api_keys="tmp_keyfile.txt")
        os.remove("tmp_keyfile.txt")

    def test_api_key_env(self):
        test_doi = {"doi": "10.1002/smll.202309431"}  # Use a DOI known to be in PMC
        with patch.dict(
            os.environ, {"WILEY_TDM_API_TOKEN": "ANOTHER_INVALID_TEST_KEY"}
        ):
            save_pdf(test_doi, filepath=SAVE_PATH, api_keys=None)

    @pytest.mark.skipif(
        os.getenv("INSTITUTIONAL_NETWORK") != "1",
        reason="Not in an institutional network",
    )
    def test_api_key_file_academic_network(self):
        test_doi = {"doi": "10.1002/smll.202309431"}  # Use a DOI from Wiley
        filename = SAVE_PATH + "_wiley"
        wiley_key_path = SAVE_PATH + "_wiley_key1"
        success = False
        try:
            with open(wiley_key_path, "w") as f:
                f.write("WILEY_TDM_API_TOKEN=INVALID_TEST_KEY_123")
            save_pdf(test_doi, filepath=filename, api_keys=wiley_key_path)
            # Verify file was created - with .pdf extension for Wiley content
            assert os.path.exists(filename + ".pdf"), (
                "PDF file was not created for Wiley content"
            )
            success = True
        finally:
            for file in [filename + ".pdf", wiley_key_path]:
                if os.path.exists(file):
                    os.remove(file)
            if not success:
                raise ValueError("PDF file was not created for Wiley content")

    @pytest.mark.skipif(
        os.getenv("INSTITUTIONAL_NETWORK") != "1",
        reason="Not in an institutional network",
    )
    def test_api_key_file_env_academic_network(self):
        test_doi = {"doi": "10.1002/smll.202309431"}  # Use a DOI from Wiley
        filename = SAVE_PATH + "_wiley"
        line = "WILEY_TDM_API_TOKEN=INVALID_TEST_KEY_123\n"
        # Append to .env file in the current directory
        with open(".env", "a") as f:
            f.write(line)

        try:
            save_pdf(test_doi, filepath=filename, api_keys=None)

            # Verify file was created - with .pdf extension for Wiley content
            assert os.path.exists(filename + ".pdf"), (
                "PDF file was not created for Wiley content"
            )
        finally:
            # Clean up
            if os.path.exists(filename + ".pdf"):
                os.remove(filename + ".pdf")
            with open(".env", "r") as f:
                lines = f.readlines()
            if lines and lines[-1] == line:
                with open(".env", "w") as f:
                    f.writelines(lines[:-1])

    def test_fallback_bioc_pmc_real_api(self):
        """Test the BioC-PMC fallback with a real API call."""
        test_doi = "10.1038/s41587-022-01613-7"  # Use a DOI known to be in PMC
        output_path = Path("test_bioc_pmc_output")
        try:
            result = FALLBACKS["bioc_pmc"](test_doi, output_path)
            assert result is True
            assert (output_path.with_suffix(".xml")).exists()
            with open(
                output_path.with_suffix(".xml"), "r"
            ) as f:  # Check if the file contains XML data
                content = f.read()
                assert "<" in content and ">" in content  # Basic XML check
                assert len(content) > 100  # Should have substantial content
        finally:
            if (output_path.with_suffix(".xml")).exists():
                os.remove(output_path.with_suffix(".xml"))

    def test_fallback_bioc_pmc_no_pmcid(self):
        """Test BioC-PMC fallback when no PMCID is available."""
        test_doi = "10.1002/smll.202309431"  # This DOI should not have a PMCID
        output_path = Path("test_bioc_pmc_no_pmcid")
        result = FALLBACKS["bioc_pmc"](test_doi, output_path)
        assert result is False
        assert not os.path.exists(output_path.with_suffix(".xml"))

    def test_fallback_elife_xml_real_api(self):
        """Test the eLife XML fallback with a real API call."""
        test_doi = "10.7554/eLife.100173"  # Use a DOI known to be in eLife
        output_path = Path("test_elife_xml_output")
        try:
            result = FALLBACKS["elife"](test_doi, output_path)
            assert result is True
            assert (output_path.with_suffix(".xml")).exists()
            with open(
                output_path.with_suffix(".xml"), "r"
            ) as f:  # Check if the file contains XML data
                content = f.read()
                assert "<" in content and ">" in content  # Basic XML check
                assert len(content) > 100  # Should have substantial content
        finally:
            if (output_path.with_suffix(".xml")).exists():
                os.remove(output_path.with_suffix(".xml"))

    def test_fallback_elife_nonexistent_article(self):
        """Test eLife XML fallback with a DOI that looks like eLife but doesn't exist."""
        test_doi = (
            "10.7554/eLife.00001"  # Article that doesn't exist in eLife repository
        )
        output_path = Path("test_elife_nonexistent")
        result = FALLBACKS["elife"](test_doi, output_path)
        # Assertions - should return False and not create a file
        assert result is False
        assert not os.path.exists(output_path.with_suffix(".xml"))

    @patch("requests.get")
    def test_fallback_wiley_api_mock(self, mock_get):
        """Test Wiley API fallback with mocked response."""
        mock_response = MagicMock()
        mock_response.content = b"%PDF-1.5 test content"
        mock_response.raise_for_status = MagicMock()
        mock_get.return_value = mock_response
        paper_metadata = {"doi": "10.1002/smll.202309431"}
        output_path = Path("test_wiley_output")
        api_keys = {"WILEY_TDM_API_TOKEN": "test_token"}
        try:
            FALLBACKS["wiley"](paper_metadata, output_path, api_keys)
            assert mock_get.called
            mock_get.assert_called_with(
                "https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fsmll.202309431",
                headers={"Wiley-TDM-Client-Token": "test_token"},
                allow_redirects=True,
                timeout=60,
            )
            pdf_path = output_path.with_suffix(".pdf")
            assert os.path.exists(pdf_path)
            with open(pdf_path, "rb") as f:
                content = f.read()
                assert content == b"%PDF-1.5 test content"
        finally:
            if os.path.exists(output_path.with_suffix(".pdf")):
                os.remove(output_path.with_suffix(".pdf"))

    def test_fallback_wiley_api_returns_boolean(self):
        """Test that fallback_wiley_api properly returns a boolean value."""
        paper_metadata = {"doi": "10.1002/smll.202309431"}
        output_path = Path("test_wiley_output")
        api_keys = {"WILEY_TDM_API_TOKEN": "INVALID_TEST_KEY_123"}
        result = FALLBACKS["wiley"](paper_metadata, output_path, api_keys)
        # Check the result is a boolean
        # will be True if on university network and False otherwise
        assert isinstance(result, bool)
        if result and output_path.with_suffix(".pdf").exists():
            os.remove(output_path.with_suffix(".pdf"))

    @patch("requests.get")
    def test_fallback_elsevier_api_mock(self, mock_get):
        """Test Elsevier API fallback with mocked response."""
        mock_response = MagicMock()
        mock_response.content = b"<xml>Test content</xml>"
        mock_response.raise_for_status = MagicMock()
        mock_get.return_value = mock_response
        paper_metadata = {"doi": "10.1016/j.xops.2024.100504"}
        output_path = Path("test_elsevier_output")
        api_keys = {"ELSEVIER_TDM_API_KEY": "test_key"}
        try:
            FALLBACKS["elsevier"](paper_metadata, output_path, api_keys)
            assert mock_get.called
            mock_get.assert_called_with(
                "https://api.elsevier.com/content/article/doi/10.1016/j.xops.2024.100504?apiKey=test_key&httpAccept=text%2Fxml",
                headers={"Accept": "application/xml"},
                timeout=60,
            )
            xml_path = output_path.with_suffix(".xml")
            assert os.path.exists(xml_path)
            with open(xml_path, "rb") as f:
                content = f.read()
                assert content == b"<xml>Test content</xml>"
        finally:
            if os.path.exists(output_path.with_suffix(".xml")):
                os.remove(output_path.with_suffix(".xml"))

    def test_fallback_elsevier_api_invalid_key(self, caplog):
        """Test real Elsevier API connectivity by verifying invalid key response pattern."""
        caplog.set_level(logging.ERROR)
        paper_metadata = {"doi": "10.1016/j.xops.2024.100504"}
        output_path = Path("test_elsevier_invalid")
        api_keys = {"ELSEVIER_TDM_API_KEY": "INVALID_TEST_KEY_123"}
        result = FALLBACKS["elsevier"](paper_metadata, output_path, api_keys)
        # Should return False for invalid key
        assert result is False
        assert not output_path.with_suffix(".xml").exists()
        # Check for the specific APIKEY_INVALID error in the logs
        assert "invalid" in caplog.text.lower()

```

#### Directory: `paperscraper-main/paperscraper/xrxiv`

##### File: `paperscraper-main/paperscraper/xrxiv/__init__.py`

```python
"""bioRxiv and medRxiv utilities."""

```

##### File: `paperscraper-main/paperscraper/xrxiv/xrxiv_api.py`

```python
"""API for bioRxiv and medRXiv."""

import logging
import time
from datetime import datetime
from functools import wraps
from time import sleep
from typing import Generator, List, Optional
from urllib.error import HTTPError

import requests
from requests.exceptions import ConnectionError, Timeout

launch_dates = {"biorxiv": "2013-01-01", "medrxiv": "2019-06-01"}
logger = logging.getLogger(__name__)


def retry_multi():
    """Retry a function several times"""

    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            num_retries = 0
            max_retries = getattr(self, "max_retries", 10)
            while num_retries <= max_retries:
                try:
                    ret = func(self, *args, **kwargs)
                    if ret is None:
                        time.sleep(5)
                        continue
                    break
                except HTTPError:
                    if num_retries == max_retries:
                        raise
                    num_retries += 1
                    time.sleep(5)
            return ret

        return wrapper

    return decorator


class XRXivApi:
    """API class."""

    def __init__(
        self,
        server: str,
        launch_date: str,
        api_base_url: str = "https://api.biorxiv.org",
        max_retries: int = 10,
    ):
        """
        Initialize API class.

        Args:
            server (str): name of the preprint server to access.
            launch_date (str): launch date expressed as YYYY-MM-DD.
            api_base_url (str, optional): Base url for the API. Defaults to 'api.biorxiv.org'.
            max_retries (int, optional): Maximal number of retries for a request before an
                error is raised. Defaults to 10.
        """
        self.server = server
        self.api_base_url = api_base_url
        self.launch_date = launch_date
        self.launch_datetime = datetime.fromisoformat(self.launch_date)
        self.get_papers_url = (
            "{}/details/{}".format(self.api_base_url, self.server)
            + "/{start_date}/{end_date}/{cursor}"
        )
        self.max_retries = max_retries

    @retry_multi()
    def call_api(self, start_date, end_date, cursor):
        try:
            json_response = requests.get(
                self.get_papers_url.format(
                    start_date=start_date, end_date=end_date, cursor=cursor
                ),
                timeout=10,
            ).json()
        except requests.exceptions.Timeout:
            logger.info("Timed out, will retry")
            return None

        return json_response

    def get_papers(
        self,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        fields: List[str] = ["title", "doi", "authors", "abstract", "date", "journal"],
        max_retries: int = 10,
    ) -> Generator:
        """
        Get paper metadata.

        Args:
            start_date (Optional[str]): begin date. Defaults to None, a.k.a. launch date.
            end_date (Optional[str]): end date. Defaults to None, a.k.a. today.
            fields (List[str], optional): fields to return per paper.
                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].
            max_retries (int): Number of retries on connection failure. Defaults to 10.

        Yields:
            Generator: a generator of paper metadata (dict) with the desired fields.
        """
        try:
            now_datetime = datetime.now()
            if start_date:
                start_datetime = datetime.fromisoformat(start_date)
                if start_datetime < self.launch_datetime:
                    start_date = self.launch_date
            else:
                start_date = self.launch_date
            if end_date:
                end_datetime = datetime.fromisoformat(end_date)
                if end_datetime > now_datetime:
                    end_date = now_datetime.strftime("%Y-%m-%d")
            else:
                end_date = now_datetime.strftime("%Y-%m-%d")
            do_loop = True
            cursor = 0
            while do_loop:
                papers = []
                for attempt in range(max_retries):
                    try:
                        json_response = self.call_api(start_date, end_date, cursor)
                        do_loop = json_response["messages"][0]["status"] == "ok"
                        if do_loop:
                            cursor += json_response["messages"][0]["count"]
                            for paper in json_response["collection"]:
                                processed_paper = {
                                    field: paper.get(field, "") for field in fields
                                }
                                papers.append(processed_paper)

                        if do_loop:
                            yield from papers
                            break
                    except (ConnectionError, Timeout) as e:
                        logger.error(
                            f"Connection error: {e}. Retrying ({attempt + 1}/{max_retries})"
                        )
                        sleep(5)
                        continue
                    except Exception as exc:
                        logger.exception(f"Failed getting papers: {exc}")
        except Exception as exc:
            logger.exception(f"Failed getting papers: {exc}")


class BioRxivApi(XRXivApi):
    """bioRxiv API."""

    def __init__(self, max_retries: int = 10):
        super().__init__(
            server="biorxiv",
            launch_date=launch_dates["biorxiv"],
            max_retries=max_retries,
        )


class MedRxivApi(XRXivApi):
    """medRxiv API."""

    def __init__(self, max_retries: int = 10):
        super().__init__(
            server="medrxiv",
            launch_date=launch_dates["medrxiv"],
            max_retries=max_retries,
        )

```

##### File: `paperscraper-main/paperscraper/xrxiv/xrxiv_query.py`

```python
"""Query dumps from bioRxiv and medRXiv."""

import logging
import sys
from typing import List, Union

import pandas as pd

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logger = logging.getLogger(__name__)


class XRXivQuery:
    """Query class."""

    def __init__(
        self,
        dump_filepath: str,
        fields: List[str] = ["title", "doi", "authors", "abstract", "date", "journal"],
    ):
        """
        Initialize the query class.

        Args:
            dump_filepath (str): filepath to the dump to be queried.
            fields (List[str], optional): fields to contained in the dump per paper.
                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].
        """
        self.dump_filepath = dump_filepath
        self.fields = fields
        self.errored = False

        try:
            self.df = pd.read_json(self.dump_filepath, lines=True)
            self.df["date"] = [date.strftime("%Y-%m-%d") for date in self.df["date"]]
        except ValueError as e:
            logger.warning(f"Problem in reading file {dump_filepath}: {e} - Skipping!")
            self.errored = True
        except KeyError as e:
            logger.warning(f"Key {e} missing in file from {dump_filepath} - Skipping!")
            self.errored = True

    def search_keywords(
        self,
        keywords: List[Union[str, List[str]]],
        fields: List[str] = None,
        output_filepath: str = None,
    ) -> pd.DataFrame:
        """
        Search for papers in the dump using keywords.

        Args:
            keywords (List[str, List[str]]): Items will be AND separated. If items
                are lists themselves, they will be OR separated.
            fields (List[str], optional): fields to be used in the query search.
                Defaults to None, a.k.a. search in all fields excluding date.
            output_filepath (str, optional): optional output filepath where to store
                the hits in JSONL format. Defaults to None, a.k.a., no export to a file.

        Returns:
            pd.DataFrame: A dataframe with one paper per row.
        """
        if fields is None:
            fields = self.fields
        fields = [field for field in fields if field != "date"]
        hits_per_field = []
        for field in fields:
            field_data = self.df[field].str.lower()
            hits_per_keyword = []
            for keyword in keywords:
                if isinstance(keyword, list):
                    query = "|".join([_.lower() for _ in keyword])
                else:
                    query = keyword.lower()
                hits_per_keyword.append(field_data.str.contains(query))
            if len(hits_per_keyword):
                keyword_hits = hits_per_keyword[0]
                for single_keyword_hits in hits_per_keyword[1:]:
                    keyword_hits &= single_keyword_hits
                hits_per_field.append(keyword_hits)
        if len(hits_per_field):
            hits = hits_per_field[0]
            for single_hits in hits_per_field[1:]:
                hits |= single_hits
        if output_filepath is not None:
            self.df[hits].to_json(output_filepath, orient="records", lines=True)
        return self.df[hits]

```

#### Directory: `paperscraper-main/paperscraper/xrxiv/tests`

##### File: `paperscraper-main/paperscraper/xrxiv/tests/__init__.py`

```python

```

##### File: `paperscraper-main/paperscraper/xrxiv/tests/test_xrxiv.py`

```python
import os

from paperscraper.get_dumps import medrxiv
from paperscraper.xrxiv.xrxiv_query import XRXivQuery

covid19 = ["COVID-19", "SARS-CoV-2"]
ai = ["Artificial intelligence", "Deep learning", "Machine learning"]
mi = ["Medical imaging"]


class TestXRXiv:
    def test_get_medrxiv(self):
        medrxiv(
            start_date="2020-05-01",
            end_date="2020-05-02",
            save_path="medriv_tmp_dump.jsonl",
        )

    def test_xriv_querier(self):
        querier = XRXivQuery("medriv_tmp_dump.jsonl")
        query = [covid19, ai, mi]
        querier.search_keywords(query, output_filepath="covid19_ai_imaging.jsonl")
        assert os.path.exists("covid19_ai_imaging.jsonl")

```

## Documentation

### Documentation Directory: docs

#### README.md

../README.md

#### api_reference.md

# API Reference

::: paperscraper
    options:
        show_if_no_docstring: false
        show_submodules: true
        filters:
            - "!^_[^_]"




---

*Documentation generated by LlamaPackageService for jannisborn/paperscraper (github)*
